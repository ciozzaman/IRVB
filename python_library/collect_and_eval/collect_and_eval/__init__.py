import numpy as np
from scipy.optimize import curve_fit
# import matplotlib
# matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import math
import statistics as s
import csv
import os,sys
from astropy.io import fits
import matplotlib.animation as animation
import pandas
import scipy.stats
import peakutils
import collections
from scipy.ndimage import rotate
from skimage.transform import resize
from uncertainties import correlated_values,ufloat
from uncertainties.unumpy import nominal_values,std_devs,uarray
import time as tm
import concurrent.futures as cf
import copy as cp

import sys, traceback, logging
logging.basicConfig(level=logging.ERROR)

import pyuda
# client=pyuda.Client()
from pyexcel_ods import get_data

#
# This is a collection of script that can be usefull in the interpretation of IR camera files
#



# SOMETHING USEFULL FOR PROFILING
#
#
# import cProfile
# import re
# cProfile.run(' SCRIPT OR SINGLE INSTRUCTION THAT I WANT TO PROFILE ')
#
#


def is_number(s):
	""" Returns True is string is a number. """
	try:
		float(s)
		return True
	except ValueError:
		return False

def rsquared(x, y):
	""" Return R^2 where x and y are array-like."""

	# slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)

	x = np.array(x)
	y = np.array(y)
	SSres = np.nansum((x-y)**2)
	SStot = np.nansum((y - np.nanmean(y))**2)
	return 1- SSres/SStot#r_value**2


#This function will generate a polinomial of n order GOOD FOR WHOLE FRAMES
def polygen(n):
	def polyadd(x, *params):
		params=np.array(params)
		shape=np.shape(x)
		temp=np.zeros(shape)
		# if (len(shape)>1) & (len(np.shape(params))==1):
		# 	params=np.reshape(params,(shape[-2],shape[-1],n))
		for i in range(n):
			# print('np.shape(temp),np.shape(params),np.shape(x)',temp,params,x)
			x2=np.power(x,i)
			# print('params[:][:][i]',params[:][:][i])
			# print('params',params)
			para=np.multiply(params[:,:,i],x2)
			temp=np.add(temp,para)
		return temp
	return polyadd

# #This function will generate a polinomial of n order GOOD FOR background fitting BUT NOT WORKING!! 2018/03/30
# def polygen2(n,shape):
# 	def polyadd(x, *params):
# 		params=np.array(params)
# 		x=np.reshape(x,shape)
# 		temp=np.zeros(shape)
# 		# if (len(shape)>1) & (len(np.shape(params))==1):
# 		params=np.reshape(params,(shape[-2],shape[-1],n))
# 		for i in range(n):
# 			# print('np.shape(temp),np.shape(params),np.shape(x)',temp,params,x)
# 			x2=np.power(x,i)
# 			# print('params[:][:][i]',params[:][:][i])
# 			# print('params',params)
# 			para=np.multiply(params[:,:,i],x2)
# 			temp=np.add(temp,para)
# 		return temp
# 	return polyadd

#This function will generate a polinomial of n order
def polygen3(n):
	def polyadd(x, *params):
		temp=0
		for i in range(n):
			temp+=params[i]*x**i
		return temp
	return polyadd


#############################################################################################

def order_filenames(filenames):

	# 13/05/2018 THIS FUNCTION IS INTODUCED TO FIX A BUG IN THE CASE filenames CONTAINS .npy FILES AND NOT .csv

	extention=filenames[0][-4:]
	if ((extention=='.csv') or (extention=='.CSV')):
		return order_filenames_csv(filenames)
	else:
		numbers = []
		for filename in filenames:
			temp = '0'
			for digit in filename:
				if digit.isdigit() == True:
					temp = temp+digit
			numbers.append(int(temp))
		filenames = [__ for _, __ in sorted(zip(numbers, filenames))]
		# return sorted(filenames, key=str.lower)
		return filenames

######################################################################################################

def order_filenames_csv(filenames):

	# section to be sure that the files are ordered in the proper numeric order
	# CAUTION!! THE FOLDER MUST CONTAIN MAX 9999999 FILES!

	reference=[]
	filenamescorr=[]
	for i in range(len(filenames)):
		# print(filenames[i])
		start=0
		end=len(filenames[i])
		for j in range(len(filenames[i])):
			if ((filenames[i][j]=='-') or (filenames[i][j]=='_')): #modified 12/08/2018 from "_" to "-"
				start=j
			elif filenames[i][j]=='.':
				end=j
		index=filenames[i][start+1:end]
		#print('index',index)

		# Commented 21/08/2018
		# if int(index)<10:
		# 	temp=filenames[i][:start+1]+'000000'+filenames[i][end-1:]
		# 	filenamescorr.append(temp)
		# elif int(index)<100:
		# 	temp=filenames[i][:start+1]+'00000'+filenames[i][end-2:]
		# 	filenamescorr.append(temp)
		# 	# print(filenames[i],temp)
		# elif int(index)<1000:
		# 	temp=filenames[i][:start+1]+'0000'+filenames[i][end-3:]
		# 	filenamescorr.append(temp)
		# elif int(index)<10000:
		# 	temp=filenames[i][:start+1]+'000'+filenames[i][end-4:]
		# 	filenamescorr.append(temp)
		# elif int(index)<100000:
		# 	temp=filenames[i][:start+1]+'00'+filenames[i][end-5:]
		# 	filenamescorr.append(temp)
		# elif int(index)<1000000:
		# 	temp=filenames[i][:start+1]+'0'+filenames[i][end-6:]
		# 	filenamescorr.append(temp)
		# else:
		# 	filenamescorr.append(filenames[i])
		reference.append(index)
	reference=np.array(reference)
	filenamesnew=np.array([filenames for _,filenames in sorted(zip(reference,filenames))])

	# Commented 21/08/2018
	# filenamescorr=sorted(filenamescorr, key=str.lower)
	# filenamesnew=[]
	# for i in range(len(filenamescorr)):
	# 	# print(filenamescorr[i])
	# 	for j in range(len(filenamescorr[i])):
	# 		if ((filenames[i][j]=='-') or (filenames[i][j]=='_')): #modified 12/08/2018 from "_" to "-"
	# 			start=j
	# 		elif filenamescorr[i][j]=='.':
	# 			end=j
	# 	index=filenamescorr[i][start+1:end]
	# 	# if int(index)<10:
	# 	# 	temp=filenamescorr[i][:start+1]+str(int(index))+filenamescorr[i][end:]
	# 	# 	filenamesnew.append(temp)
	# 	# elif int(index)<100:
	# 	# 	temp=filenamescorr[i][:start+1]+str(int(index))+filenamescorr[i][end:]
	# 	# 	filenamesnew.append(temp)
	# 	# 	# print(filenames[i],temp)
	# 	# if int(index)<1000000:
	# 	temp=filenamescorr[i][:start+1]+str(int(index))+filenamescorr[i][end:]
	# 	filenamesnew.append(temp)
	# 	# 	# print(filenamescorr[i],temp)
	# 	# else:
	# 	# 	filenamesnew.append(filenamescorr[i])
	filenames=filenamesnew

	return(filenames)

#############################################################################################


def collect_subfolder(extpath):

	# THIS FUNCTION GENERATE THE .npy FILE CORRESPONDING TO A SINGLE FOLDER STARTING FROM ALL THE .csv FILES IN IT

	##print('sys.argv[0] =', sys.argv[0])
	#pathname = os.path.dirname(sys.argv[0])
	##print('path =', pathname)
	#print('full path =', os.path.abspath(pathname))
	#path=os.path.abspath(pathname)


	# path=os.getcwd()
	path=extpath
	print('path =', path)

	position=[]
	for i in range(len(path)):
		if path[i]=='/':
			position.append(i)
	position=max(position)
	lastpath=path[position+1:]
	# print('lastpath',lastpath)

	f = []
	for (dirpath, dirnames, filenames) in os.walk(path):
		f.extend(filenames)
	#	break

	filenames=f
	# print('filenames',filenames)


	temp=[]
	print('len(filenames)',len(filenames))
	for index in range(len(filenames)):
		# print(filenames[index])
		if filenames[index][-3:]=='csv':
			temp.append(filenames[index])
			# filenames=np.delete(filenames,index)
			# print('suca')
	filenames=temp

	filenames=order_filenames(filenames)

	numfiles=len(filenames)


	filename=filenames[0]

	# print('os.path.join(path,filename)',os.path.join(path,filename))

	firstrow=-1
	with open(os.path.join(path,filename),'r') as csvfile:
		reader = csv.reader(csvfile)
		# print('reader',reader)
		pointer=0
		for row in reader:
	#		print('shape',np.shape(row))
			# print('row',row)
			if not row:
				temp='empty'
			else:
				temp=row[0]
			if (is_number(temp)) & (firstrow==-1):
				firstrow=pointer
				rowlen=np.shape(row)[0]
				pointer+=1
			elif is_number(temp) & firstrow>-1:
				pointer+=1
			else:
				ponter=0
	lastrow=pointer
	sizey=lastrow-firstrow
	sizex=rowlen

	data=np.zeros((1,numfiles,sizey,sizex))

	print('firstrow,lastrow,sizey,sizex',firstrow,lastrow,sizey,sizex)

	file=0
	for filename in filenames:
		with open(os.path.join(path,filename),'r') as csvfile:
			reader = csv.reader(csvfile)
			# print('reader',reader)
			tempdata=[]
			pointer=sizey-1
			for row in reader:
		#		print('shape',np.shape(row))
				# print('row',row)
				if not row:
					temp='empty'
				else:
					temp=row[0]
				if is_number(temp):
					for k in range(len(row)):
						# print('j,i,pointer,k',j,i,pointer,k)
						data[0,file,pointer,k]=(float(row[k]))
						# print('float(k)',float(row[k]))
					pointer-=1
					# print('row',row)
				else:
					ponter=0
			file+=1

	np.save(os.path.join(extpath,lastpath),data)

	# plt.pcolor(data[0,20,:,:])
	# plt.show()


##############################################################################################

def collect_subfolderfits(extpath,start='auto',stop='auto'):

	# 2018/01/16 Upgraded to handle, as optional, a limited range of frames
	# extpath: folder the containing the file .fts

	##print('sys.argv[0] =', sys.argv[0])
	#pathname = os.path.dirname(sys.argv[0])
	##print('path =', pathname)
	#print('full path =', os.path.abspath(pathname))
	#path=os.path.abspath(pathname)


	# path=os.getcwd()
	path=extpath
	print('path =', path)

	position=[]
	for i in range(len(path)):
		if path[i]=='/':
			position.append(i)
	position=max(position)
	lastpath=path[position+1:]
	# print('lastpath',lastpath)

	f = []
	for (dirpath, dirnames, filenames) in os.walk(path):
		f.extend(filenames)
	#	break

	filenames=f
	# print('filenames',filenames)

	filefits=[]
	temp=[]
	print('len(filenames)',len(filenames))
	for index in range(len(filenames)):
		# print(filenames[index])
		if filenames[index][-3:]=='csv':
			temp.append(filenames[index])
			# filenames=np.delete(filenames,index)
			# print('suca')
		elif filenames[index][-3:]=='fts':
			filefits.append(filenames[index])
	filenames=temp
	filenames=sorted(filenames, key=str.lower)

	numfiles=len(filenames)

	if numfiles>0:
		filename=filenames[0]
		# print('os.path.join(path,filename)',os.path.join(path,filename))
		firstrow=-1
		with open(os.path.join(path,filename),'r') as csvfile:
			reader = csv.reader(csvfile)
			# print('reader',reader)
			pointer=0
			for row in reader:
		#		print('shape',np.shape(row))
				# print('row',row)
				if not row:
					temp='empty'
				else:
					temp=row[0]
				if (is_number(temp)) & (firstrow==-1):
					firstrow=pointer
					rowlen=np.shape(row)[0]
					pointer+=1
				elif is_number(temp) & firstrow>-1:
					pointer+=1
				else:
					ponter=0
		lastrow=pointer
		sizey=lastrow-firstrow
		sizex=rowlen


	datafit=fits.open(os.path.join(path,filefits[0]))
	if numfiles==0:
		sizey = datafit[0].shape[1]
		sizex = datafit[0].shape[2]
	lenfits=datafit[0].shape[0]
	zero_level = 2**(datafit[0].header['BITPIX']-1)
	datafit=datafit[0].data

	datafit=datafit+zero_level

	datatest=np.zeros((1,numfiles,sizey,sizex))
	# data=np.zeros((1,lenfits,sizey,sizex))
	# index=0
	# for frame in datafit:
	# 	# plt.pcolor(frame)
	# 	# plt.show()
	# 	frame=np.flip(frame,0)
	# 	# plt.pcolor(frame)
	# 	# plt.show()
	# 	data[0,index,:,:]=frame
	# 	index+=1
	data = np.array([np.flip(datafit,axis=1)])

	if numfiles>0:
		print('firstrow,lastrow,sizey,sizex',firstrow,lastrow,sizey,sizex)
	else:
		print('sizey,sizex',sizey,sizex)

	if numfiles>0:
		file=0
		for filename in filenames:
			with open(os.path.join(path,filename),'r') as csvfile:
				reader = csv.reader(csvfile)
				# print('reader',reader)
				tempdata=[]
				pointer=sizey-1
				for row in reader:
			#		print('shape',np.shape(row))
					# print('row',row)
					if not row:
						temp='empty'
					else:
						temp=row[0]
					if is_number(temp):
						for k in range(len(row)):
							# print('j,i,pointer,k',j,i,pointer,k)
							datatest[0,file,pointer,k]=(float(row[k]))
							# print('float(k)',float(row[k]))
						pointer-=1
						# print('row',row)
					else:
						ponter=0
				file+=1


	# 2018/01/16 Section added to avoid the need of a separate "collect_subfolderfits_limited" function
	if start=='auto':
		force_start=0
	elif (start<0 or start>lenfits):
		print('The initial limit is out of range (a number of frame)')
		print('0 will be used instead of '+str(start))
		force_start=0
	else:
		force_start=start
	if stop=='auto':
		force_end=lenfits
	elif ((stop<-lenfits+1) or stop>lenfits or stop<=force_start):
		print('The final limit to search for the oscillation ad erase it is out of range (a number of frame)')
		print(str(lenfits)+'s will be used instead of '+str(stop))
		force_end=lenfits
	else:
		force_end=stop

	# if start<0:
	# 	start=0
	# if start>lenfits:
	# 	start=lenfits
	# if stop>lenfits:
	# 	stop=lenfits
	# if stop<start:
	# 	print('there must be something wrong, you are giving start frame higher than stop one')
	# 	exit()
	datacropped=data[:,force_start:force_end,:,:]


	# if (not (np.array_equal(datatest[0,0],data[0,0]))):
	# 	print('there must be something wrong, the first frame of the FITS file do not match with the first csv files')
	# 	exit()
	if (numfiles>0 and (not (np.array_equal(datatest[0,-1],data[0,-1])))):
		print('there must be something wrong, the last frame of the FITS file do not match with the last csv files')
		# exit()
	else:
		# np.save(os.path.join(extpath,lastpath),datacropped)
		np.savez_compressed(os.path.join(extpath,lastpath),datacropped=datacropped)

	# plt.pcolor(data[0,20,:,:])
	# plt.show()

##############################################################################################

def collect_subfolderfits_limited(extpath,start,stop):

	# 10/05/2018 THIS FUNCTION IS EXACLT LIKE collect_subfolderfits BUT LIMIT THE FILE CREATED FROM THE start FRAME TO THE  finish ONE
	print('Use collect_subfolderfits instead of collect_subfolderfits_limited')
	exit()


	##print('sys.argv[0] =', sys.argv[0])
	#pathname = os.path.dirname(sys.argv[0])
	##print('path =', pathname)
	#print('full path =', os.path.abspath(pathname))
	#path=os.path.abspath(pathname)


	# path=os.getcwd()
	path=extpath
	print('path =', path)

	position=[]
	for i in range(len(path)):
		if path[i]=='/':
			position.append(i)
	position=max(position)
	lastpath=path[position+1:]
	# print('lastpath',lastpath)

	f = []
	for (dirpath, dirnames, filenames) in os.walk(path):
		f.extend(filenames)
	#	break

	filenames=f
	# print('filenames',filenames)

	filefits=[]
	temp=[]
	print('len(filenames)',len(filenames))
	for index in range(len(filenames)):
		# print(filenames[index])
		if filenames[index][-3:]=='csv':
			temp.append(filenames[index])
			# filenames=np.delete(filenames,index)
			# print('suca')
		elif filenames[index][-3:]=='fts':
			filefits.append(filenames[index])
	filenames=temp
	filenames=sorted(filenames, key=str.lower)

	numfiles=len(filenames)


	filename=filenames[0]

	# print('os.path.join(path,filename)',os.path.join(path,filename))

	firstrow=-1
	with open(os.path.join(path,filename),'r') as csvfile:
		reader = csv.reader(csvfile)
		# print('reader',reader)
		pointer=0
		for row in reader:
	#		print('shape',np.shape(row))
			# print('row',row)
			if not row:
				temp='empty'
			else:
				temp=row[0]
			if (is_number(temp)) & (firstrow==-1):
				firstrow=pointer
				rowlen=np.shape(row)[0]
				pointer+=1
			elif is_number(temp) & firstrow>-1:
				pointer+=1
			else:
				ponter=0
	lastrow=pointer
	sizey=lastrow-firstrow
	sizex=rowlen


	datafit=fits.open(os.path.join(path,filefits[0]))
	datafit=datafit[0].data

	datafit=datafit+32767
	lenfits=len(datafit)

	data=np.zeros((1,lenfits,sizey,sizex))
	datatest=np.zeros((1,numfiles,sizey,sizex))
	index=0
	for frame in datafit:
		# plt.pcolor(frame)
		# plt.show()
		frame=np.flip(frame,0)
		# plt.pcolor(frame)
		# plt.show()
		data[0,index,:,:]=frame
		index+=1


	print('firstrow,lastrow,sizey,sizex',firstrow,lastrow,sizey,sizex)

	file=0
	for filename in filenames:
		with open(os.path.join(path,filename),'r') as csvfile:
			reader = csv.reader(csvfile)
			# print('reader',reader)
			tempdata=[]
			pointer=sizey-1
			for row in reader:
		#		print('shape',np.shape(row))
				# print('row',row)
				if not row:
					temp='empty'
				else:
					temp=row[0]
				if is_number(temp):
					for k in range(len(row)):
						# print('j,i,pointer,k',j,i,pointer,k)
						datatest[0,file,pointer,k]=(float(row[k]))
						# print('float(k)',float(row[k]))
					pointer-=1
					# print('row',row)
				else:
					ponter=0
			file+=1

	if (np.array_equal(datatest[0,0],data[0,0]))&(np.array_equal(datatest[0,-1],data[0,-1])):
		print('there must be something wrong, the first or last frame of the TITS file do not match with the two csv files')
		exit()


	if start<0:
		start=0
	if start>lenfits:
		start=lenfits
	if stop>lenfits:
		stop=lenfits
	if stop<start:
		print('there must be something wrong, you are giving start frame higher than stop one')
		exit()
	datacropped=data[:,start:stop,:,:]

	np.save(os.path.join(extpath,lastpath),datacropped)

	# plt.pcolor(data[0,20,:,:])
	# plt.show()

##############################################################################################

# make movie

def movie(extpath,framerate,integration,xlabel=(),ylabel=(),barlabel=(),cmap='rainbow',timesteps='auto',extvmin='auto',extvmax='auto'):

	path=extpath
	print('path =', path)

	position=[]
	for i in range(len(path)):
		if path[i]=='/':
			position.append(i)
	position=max(position)
	lastpath=path[position+1:]
	# print('lastpath',lastpath)

	f = []
	for (dirpath, dirnames, filenames) in os.walk(path):
		f.extend(filenames)
	#	break

	filenames=f
	# print('filenames',filenames)

	filefits=[]
	temp=[]
	filemovie=[]
	print('len(filenames)',len(filenames))
	for index in range(len(filenames)):
		# print(filenames[index])
		if filenames[index][-3:]=='npy':
			temp.append(filenames[index])

	filenames=temp
	filenames=sorted(filenames, key=str.lower)

	numfiles=len(filenames)


	filename=filenames[0]

	data=np.load(os.path.join(path,filename))

	fig = plt.figure()
	ax = fig.add_subplot(111)

	# I like to position my colorbars this way, but you don't have to
	# div = make_axes_locatable(ax)
	# cax = div.append_axes('right', '5%', '5%')

	# def f(x, y):
	#	 return np.exp(x) + np.sin(y)

	# x = np.linspace(0, 1, 120)
	# y = np.linspace(0, 2 * np.pi, 100).reshape(-1, 1)

	# This is now a list of arrays rather than a list of artists
	frames = [None]*len(data[0])
	frames[0]=data[0,0]

	for i in range(len(data[0])):
		# x	   += 1
		# curVals  = f(x, y)
		frames[i]=(data[0,i])

	cv0 = frames[0]
	im = ax.imshow(cv0,cmap, origin='lower') # Here make an AxesImage rather than contour
	cb = fig.colorbar(im).set_label(barlabel)
	cb = ax.set_xlabel(xlabel)
	cb = ax.set_ylabel(ylabel)
	tx = ax.set_title('Frame 0')



	if timesteps=='auto':
		def animate(i):
			arr = frames[i]
			if extvmax=='auto':
				vmax = np.max(arr)
			elif extvmax=='allmax':
				vmax = np.max(data)
			else:
				vmax = extvmax

			if extvmin=='auto':
				vmin = np.min(arr)
			elif extvmin=='allmin':
				vmin = np.min(data)
			else:
				vmin = extvmin

			im.set_data(arr)
			im.set_clim(vmin, vmax)
			tx.set_text('Frame {0}'.format(i)+', FR '+str(framerate)+'Hz, t '+str(np.around(0+i/framerate,decimals=3))+'s int '+str(integration)+'ms')
			# In this version you don't have to do anything to the colorbar,
			# it updates itself when the mappable it watches (im) changes
	else:
		def animate(i):
			arr = frames[i]
			if extvmax=='auto':
				vmax = np.max(arr)
			elif extvmax=='allmax':
				vmax = np.max(data)
			else:
				vmax = extvmax

			if extvmin=='auto':
				vmin = np.min(arr)
			elif extvmin=='allmin':
				vmin = np.min(data)
			else:
				vmin = extvmin

			im.set_data(arr)
			im.set_clim(vmin, vmax)
			tx.set_text('Frame {0}'.format(i)+', t '+str(timesteps[i])+'s int '+str(integration)+'ms')

	ani = animation.FuncAnimation(fig, animate, frames=len(data[0]))

	ani.save(os.path.join(extpath,lastpath)+'.mp4', fps=30, extra_args=['-vcodec', 'libx264'])


######################################################################################################
EFIT_path_default = '/common/uda-scratch/lkogan/efitpp_eshed'
foil_size = [0.07,0.09]

# done a bit later so that calculate_tangency_angle_for_poloidal_section is calculated
# exec(open("/home/ffederic/work/analysis_scripts/scripts/python_library/collect_and_eval/collect_and_eval/MASTU_structure.py").read())

# functions to draw the x-point on images or movies
def point_toroidal_to_cartesian(coords):	# r,z,teta deg	to	x,y,z
	out = np.zeros_like(coords).astype(float)
	out.T[0]=coords.T[0] * np.cos(coords.T[2]*2*np.pi/360)
	out.T[1]=coords.T[0] * np.sin(coords.T[2]*2*np.pi/360)
	out.T[2]=coords.T[1]
	return out

def point_cartesian_to_toroidal(coords):	# x,y,z	to	r,z,teta deg
	out = np.zeros_like(coords).astype(float)
	out.T[0]=((coords.T[0]**2) + (coords.T[1]**2))**0.5
	out.T[1]=coords.T[2]
	out.T[2]=np.arctan(coords.T[1]/coords.T[0])/np.pi*180
	out.T[2][coords.T[0]<0] += 180
	return out

stand_off_length = 0.045	# m
# Rf=1.54967	# m	radius of the centre of the foil
pinhole_to_foil_vertical = 0.01 + 0.003 + 0.002 + stand_off_length
Rf=1.48967 + pinhole_to_foil_vertical	# m	radius of the centre of the foil
plane_equation = np.array([1,-1,0,2**0.5 * Rf])	# plane of the foil
centre_of_front_plate = np.array([1.48967+0.002,-0.7,135])	# R,z,teta deg
pinhole_offset = np.array([-0.0198,-0.0198])	# toroidal direction parallel to the place surface, z

def locate_pinhole(centre_of_front_plate=centre_of_front_plate,pinhole_offset=pinhole_offset):
	# this is wrong. the movement has to go along the plane of the foil (assumed same as front plate)
	# as there is no way to verify the rotation of the flange, I assume it is vertical
	pinhole_location_toroidal = np.array([(centre_of_front_plate[0]**2 + pinhole_offset[0]**2)**0.5,centre_of_front_plate[1]+pinhole_offset[1],centre_of_front_plate[2]+360/(2*np.pi)*np.arctan(pinhole_offset[0]/centre_of_front_plate[0])])	# R,z,teta deg
	return point_toroidal_to_cartesian(pinhole_location_toroidal)	# x,y,z

# pinhole_location = np.array([-1.04087,1.068856,-0.7198])	# x,y,z
pinhole_location = locate_pinhole(pinhole_offset=pinhole_offset)	# x,y,z
pinhole_location_toroidal=point_cartesian_to_toroidal(np.array([pinhole_location]))[0]	# R,z,teta deg
centre_of_foil = np.array([-Rf/(2**0.5), Rf/(2**0.5), -0.7])	# x,y,z
foil_size = [0.07,0.09]	# m
R_centre_column = 0.2608	# m
pinhole_relative_location = np.array(foil_size)/2 - pinhole_offset #+ 0.0198
pinhole_radious = 0.004/2	# m

def return_MU01_sxd_region_delimiter():
	from scipy.interpolate import interp1d
	MU01_sxd_region_delimiter = interp1d([0.7,0.76,0.95,1.6],[-2.5,-1.83,-1.55,-1],bounds_error=None, fill_value='extrapolate')
	return MU01_sxd_region_delimiter

def return_DMS_LOS19_V2_higher_delimiter():
	# return the path of the LOS 19 violet as shown in Figure 2b of Verhaegh2022
	from scipy.interpolate import interp1d
	DMS_LOS19_V2_higher_delimiter = interp1d([0.71,1.625],[-1.675,-1.58],bounds_error=None, fill_value='extrapolate')
	return DMS_LOS19_V2_higher_delimiter

def return_MWI_delimiter():
	# return the path of the LOS 19 violet as shown in Figure 2b of Verhaegh2022
	from scipy.interpolate import interp1d
	MWI_delimiter = interp1d([0.75,1.545],[-1.723,-1.59],bounds_error=None, fill_value='extrapolate')
	return MWI_delimiter

def return_inner_outer_leg_delimiter(efit_reconstruction,i_efit_time):
	# return the division of inner and outer leg as per Figure 3.19 in my thesis Federici2023b
	from scipy.interpolate import interp1d
	delimiter = interp1d([efit_reconstruction.lower_xpoint_z[i_efit_time],efit_reconstruction.mag_axis_z[i_efit_time],efit_reconstruction.upper_xpoint_z[i_efit_time]],[efit_reconstruction.lower_xpoint_r[i_efit_time],efit_reconstruction.mag_axis_r[i_efit_time],efit_reconstruction.upper_xpoint_r[i_efit_time]],bounds_error=None, fill_value='extrapolate')
	return delimiter

def calculate_tangency_angle_for_poloidal_section(radius_to_trace,pinhole_location_toroidal=pinhole_location_toroidal,side='left'):
	if side=='left':
		reference_radius = pinhole_location_toroidal[2]-(90-(np.arcsin(radius_to_trace/pinhole_location_toroidal[0])/np.pi)*180)
	elif side=='right':
		reference_radius = pinhole_location_toroidal[2]+(90-(np.arcsin(radius_to_trace/pinhole_location_toroidal[0])/np.pi)*180)
	return reference_radius

# moved here so that calculate_tangency_angle_for_poloidal_section is calculated
client=pyuda.Client()
exec(open("/home/ffederic/work/analysis_scripts/scripts/python_library/collect_and_eval/collect_and_eval/MASTU_structure.py").read())
# reset_connection(client)
del client

def find_location_on_foil(point_coord,plane_equation=plane_equation,pinhole_location=pinhole_location):
	# t = (-plane_equation[-1] -np.sum(plane_equation[:-1]*point_coord,axis=-1)) / np.sum(plane_equation[:-1]*(pinhole_location-point_coord),axis=-1)
	# out = point_coord + ((pinhole_location-point_coord).T*t).T
	out = pinhole_location-((pinhole_location-point_coord).T * ( np.sum(plane_equation[:-1]*pinhole_location,axis=-1) + plane_equation[-1] )/ ( np.sum(plane_equation[:-1]*(pinhole_location-point_coord),axis=-1) )).T
	return out

def absolute_position_on_foil_to_foil_coord(coords,centre_of_foil=centre_of_foil,foil_size=foil_size):	# out in [x,z]
	out = np.zeros((np.shape(coords)[0],np.shape(coords)[1]-1))
	out.T[1] = foil_size[1]/2 -(coords.T[2] - centre_of_foil[2])
	out.T[1][np.logical_or(out.T[1]>foil_size[1],out.T[1]<0)] = np.nan
	out.T[0] = np.sign((coords.T[0]-centre_of_foil[0]))*((coords.T[0]-centre_of_foil[0])**2 + (coords.T[1]-centre_of_foil[1])**2)**0.5 + foil_size[0]/2
	out.T[0][np.logical_or(out.T[0]>foil_size[0],out.T[0]<0)] = np.nan
	return out

# structure_point_location_on_foil = []
# for time in range(len(stucture_r)):
# 	point_location = np.array([stucture_r[time],stucture_z[time],stucture_t[time]]).T
# 	point_location = point_toroidal_to_cartesian(point_location)
# 	point_location = find_location_on_foil(point_location)
# 	structure_point_location_on_foil.append(absolute_position_on_foil_to_foil_coord(point_location))
# structure_point_location_on_foil.append(np.array([pinhole_relative_location[0] + np.arange(-pinhole_radious,+pinhole_radious+pinhole_radious/10/2,pinhole_radious/10),pinhole_relative_location[1] + np.abs(pinhole_radious**2-np.arange(-pinhole_radious,+pinhole_radious+pinhole_radious/10/2,pinhole_radious/10)**2)**0.5]).T)
# structure_point_location_on_foil.append(np.array([pinhole_relative_location[0] + np.arange(-pinhole_radious,+pinhole_radious+pinhole_radious/10/2,pinhole_radious/10),pinhole_relative_location[1] - np.abs(pinhole_radious**2-np.arange(-pinhole_radious,+pinhole_radious+pinhole_radious/10/2,pinhole_radious/10)**2)**0.5]).T)

def return_structure_point_location_on_foil(plane_equation=plane_equation,pinhole_offset=pinhole_offset,centre_of_foil=centre_of_foil,foil_size=foil_size,centre_of_front_plate=centre_of_front_plate,pinhole_radious=pinhole_radious):
	pinhole_location = locate_pinhole(centre_of_front_plate=centre_of_front_plate,pinhole_offset=pinhole_offset)	# x,y,z
	pinhole_relative_location = np.array(foil_size)/2 - pinhole_offset #+ 0.0198
	structure_point_location_on_foil = []
	for time in range(len(stucture_r)):
		point_location = np.array([stucture_r[time],stucture_z[time],stucture_t[time]]).T
		point_location = point_toroidal_to_cartesian(point_location)
		point_location = find_location_on_foil(point_location,plane_equation=plane_equation,pinhole_location=pinhole_location)
		structure_point_location_on_foil.append(absolute_position_on_foil_to_foil_coord(point_location,centre_of_foil=centre_of_foil,foil_size=foil_size))
	structure_point_location_on_foil.append(np.array([pinhole_relative_location[0] + np.arange(-pinhole_radious,+pinhole_radious+pinhole_radious/10/2,pinhole_radious/10),pinhole_relative_location[1] + np.abs(pinhole_radious**2-np.arange(-pinhole_radious,+pinhole_radious+pinhole_radious/10/2,pinhole_radious/10)**2)**0.5]).T)
	structure_point_location_on_foil.append(np.array([pinhole_relative_location[0] + np.arange(-pinhole_radious,+pinhole_radious+pinhole_radious/10/2,pinhole_radious/10),pinhole_relative_location[1] - np.abs(pinhole_radious**2-np.arange(-pinhole_radious,+pinhole_radious+pinhole_radious/10/2,pinhole_radious/10)**2)**0.5]).T)
	for time in range(len(stucture_r_t_to_recalculate)):
		for value in ['left','right']:
			stucture_t_t_to_recalculate =calculate_tangency_angle_for_poloidal_section(stucture_r_t_to_recalculate[time],pinhole_location_toroidal=point_cartesian_to_toroidal(np.array([pinhole_location]))[0],side=value)
			point_location = np.array([stucture_r_t_to_recalculate[time],stucture_z_t_to_recalculate[time],stucture_t_t_to_recalculate]).T
			point_location = point_toroidal_to_cartesian(point_location)
			point_location = find_location_on_foil(point_location,plane_equation=plane_equation,pinhole_location=pinhole_location)
			structure_point_location_on_foil.append(absolute_position_on_foil_to_foil_coord(point_location,centre_of_foil=centre_of_foil,foil_size=foil_size))
	return structure_point_location_on_foil

structure_radial_profile = [_MASTU_CORE_GRID_POLYGON]
structure_radial_profile.append(FULL_MASTU_CORE_GRID_POLYGON)
structure_radial_profile.append(np.array([[1.554,1.554,1.746,1.746,1.554],[-0.270,-0.452,-0.452,-0.270,-0.270]]).T)	# P5
structure_radial_profile.append(np.array([[1.516,1.516,1.788,1.788,1.516],[-0.227,-0.495,-0.495,-0.227,-0.227]]).T)	# P5 support
structure_radial_profile.append(np.array([[1.554,1.554,1.746,1.746,1.554],[0.270,0.452,0.452,0.270,0.270]]).T)	# P5
structure_radial_profile.append(np.array([[1.516,1.516,1.788,1.788,1.516],[0.227,0.495,0.495,0.227,0.227]]).T)	# P5 support
structure_radial_profile.append(np.array([[1.309,1.268,1.272,1.369,1.370,1.309],[-0.879,-0.95,-1.017,-1.017,-0.879,-0.879]]).T)	# P6
structure_radial_profile.append(np.array([[1.406,1.306,1.194,1.208,1.309,1.413,1.406],[-0.822,-0.822,-1.002,-1.002,-0.842,-0.842,-0.842]]).T)	# P6 cover
structure_radial_profile.append(np.array([[1.309,1.268,1.272,1.369,1.370,1.309],[0.879,0.95,1.017,1.017,0.879,0.879]]).T)	# P6
structure_radial_profile.append(np.array([[1.406,1.306,1.194,1.208,1.309,1.413,1.406],[0.822,0.822,1.002,1.002,0.842,0.842,0.842]]).T)	# P6 cover
structure_radial_profile.append(np.array([[1.468,1.395,1.446,1.518,1.468],[-0.495,-0.776,-0.788,-0.508,-0.495]]).T)	# ELM coil
structure_radial_profile.append(np.array([[1.468,1.395,1.446,1.518,1.468],[0.495,0.776,0.788,0.508,0.495]]).T)	# ELM coil
structure_radial_profile.append(np.array([[2,1.490,1.490,2],[-0.616,-0.616,-0.784,-0.784]]).T)	# IRVB tube
def return_structure_radial_profile():
	return structure_radial_profile
res_bolo_radial_profile = [np.array([[R_centre_column,(core_tangential_common_point[0]**2+core_tangential_common_point[1]**2)**0.5],[0,0]]).T]
for core_poloidal_arrival_ in core_poloidal_arrival:
	res_bolo_radial_profile.append(np.array([[core_poloidal_arrival_[0],core_poloidal_common_point[0]],[core_poloidal_arrival_[1],core_poloidal_common_point[1]]]).T)
for divertor_poloidal_arrival_ in divertor_poloidal_arrival:
	res_bolo_radial_profile.append(np.array([[divertor_poloidal_arrival_[0],divertor_poloidal_common_point[0]],[divertor_poloidal_arrival_[1],divertor_poloidal_common_point[1]]]).T)

def return_fueling_point_location_on_foil(plane_equation=plane_equation,pinhole_offset=pinhole_offset,centre_of_foil=centre_of_foil,foil_size=foil_size):
	pinhole_location = locate_pinhole(pinhole_offset=pinhole_offset)	# x,y,z
	fueling_point_location_on_foil = []
	for time in range(len(fueling_r)):
		point_location = np.array([fueling_r[time],fueling_z[time],fueling_t[time]]).T
		point_location = point_toroidal_to_cartesian(point_location)
		point_location = find_location_on_foil(point_location,plane_equation=plane_equation,pinhole_location=pinhole_location)
		fueling_point_location_on_foil.append(absolute_position_on_foil_to_foil_coord(point_location,centre_of_foil=centre_of_foil,foil_size=foil_size))
	return fueling_point_location_on_foil
# fueling_point_location_on_foil = return_fueling_point_location_on_foil()

def return_all_time_x_point_location(efit_reconstruction,resolution = 1000,plane_equation=plane_equation,pinhole_location=pinhole_location,centre_of_foil=centre_of_foil,foil_size=foil_size):
	all_time_x_point_location = []
	for time in range(len(efit_reconstruction.time)):
		x_point_location = np.array([[efit_reconstruction.lower_xpoint_r[time]]*resolution,[efit_reconstruction.lower_xpoint_z[time]]*resolution,np.linspace(0,360,resolution)]).T
		x_point_location = point_toroidal_to_cartesian(x_point_location)
		x_point_location = find_location_on_foil(x_point_location,plane_equation=plane_equation,pinhole_location=pinhole_location)
		all_time_x_point_location.append(absolute_position_on_foil_to_foil_coord(x_point_location,centre_of_foil=centre_of_foil,foil_size=foil_size))
	all_time_x_point_location = np.array(all_time_x_point_location)
	return all_time_x_point_location

def return_all_time_mag_axis_location(efit_reconstruction,resolution = 1000,plane_equation=plane_equation,pinhole_location=pinhole_location,centre_of_foil=centre_of_foil,foil_size=foil_size):
	all_time_mag_axis_location = []
	for time in range(len(efit_reconstruction.time)):
		mag_axis_location = np.array([[efit_reconstruction.mag_axis_r[time]]*resolution,[efit_reconstruction.mag_axis_z[time]]*resolution,np.linspace(0,360,resolution)]).T
		mag_axis_location = point_toroidal_to_cartesian(mag_axis_location)
		mag_axis_location = find_location_on_foil(mag_axis_location,plane_equation=plane_equation,pinhole_location=pinhole_location)
		all_time_mag_axis_location.append(absolute_position_on_foil_to_foil_coord(mag_axis_location,centre_of_foil=centre_of_foil,foil_size=foil_size))
	all_time_mag_axis_location = np.array(all_time_mag_axis_location)
	return all_time_mag_axis_location

def return_all_time_strike_points_location(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine,resolution = 1000,plane_equation=plane_equation,pinhole_location=pinhole_location,centre_of_foil=centre_of_foil,ref_angle=[],foil_size=foil_size):
	from scipy.signal import find_peaks, peak_prominences as get_proms
	temp_R = np.ones((len(efit_reconstruction.time),20))*np.nan
	temp_Z = np.ones((len(efit_reconstruction.time),20))*np.nan
	for time in range(len(efit_reconstruction.time)):
		temp = np.array([efit_reconstruction.strikepointR[time],efit_reconstruction.strikepointZ[time]]).T
		if temp.max()<1e-1 and len(flatten(all_time_sep_r[time][0]))>0 and len(flatten(all_time_sep_r[time][2]))>0:
			try:
				a=np.concatenate([r_fine[all_time_sep_r[time][0]],r_fine[all_time_sep_r[time][2]]])
				b=np.concatenate([z_fine[all_time_sep_z[time][0]],z_fine[all_time_sep_z[time][2]]])
			except:
				try:
					a=r_fine[all_time_sep_r[time][0]]
					b=z_fine[all_time_sep_z[time][0]]
				except:
					a=r_fine[all_time_sep_r[time][2]]
					b=z_fine[all_time_sep_z[time][2]]
			# c=np.abs(a-R_centre_column)
			c=np.abs(a-R_centre_column_interpolator(b))
			peaks = find_peaks(-c)[0]
			peaks = peaks[c[peaks]<4e-3]
			# efit_reconstruction.strikepointR[time][:min(len(efit_reconstruction.strikepointR[time]),len(peaks))] = a[peaks][:min(len(efit_reconstruction.strikepointR[time]),len(peaks))]
			# efit_reconstruction.strikepointZ[time][:min(len(efit_reconstruction.strikepointZ[time]),len(peaks))] = b[peaks][:min(len(efit_reconstruction.strikepointZ[time]),len(peaks))]
			temp_R[time][:min(20,len(peaks))] = a[peaks][:min(20,len(peaks))]
			temp_Z[time][:min(20,len(peaks))] = b[peaks][:min(20,len(peaks))]
		else:
			temp_R[time][:min(20,len(efit_reconstruction.strikepointR[time]))] = efit_reconstruction.strikepointR[time][:min(20,len(efit_reconstruction.strikepointR[time]))]
			temp_Z[time][:min(20,len(efit_reconstruction.strikepointZ[time]))] = -np.abs(efit_reconstruction.strikepointZ[time][:min(20,len(efit_reconstruction.strikepointZ[time]))])
	all_time_strike_points_location = []
	for time in range(len(efit_reconstruction.time)):
		# strike_point_location = np.array([efit_reconstruction.strikepointR[time],-np.abs(efit_reconstruction.strikepointZ[time]),[60]*len(efit_reconstruction.strikepointZ[time])]).T
		if ref_angle==[]:
			strike_point_location = np.array([temp_R[time].tolist()*2,temp_Z[time].tolist()*2,calculate_tangency_angle_for_poloidal_section(temp_R[time],side='left').tolist()+calculate_tangency_angle_for_poloidal_section(temp_R[time],side='right').tolist()]).T
		else:
			strike_point_location = np.array([temp_R[time].tolist()*2,temp_Z[time].tolist()*2,[ref_angle[0]]*len(temp_Z[time])+[ref_angle[1]]*len(temp_Z[time])]).T
		strike_point_location = point_toroidal_to_cartesian(strike_point_location)
		strike_point_location = find_location_on_foil(strike_point_location,plane_equation=plane_equation,pinhole_location=pinhole_location)
		all_time_strike_points_location.append(absolute_position_on_foil_to_foil_coord(strike_point_location,centre_of_foil=centre_of_foil,foil_size=foil_size))
	all_time_strike_points_location_rot = []
	for time in range(len(efit_reconstruction.time)):
		temp = []
		for __i in range(len(temp_R[time])):
			# strike_point_location = np.array([[efit_reconstruction.strikepointR[time][__i]]*resolution,[-np.abs(efit_reconstruction.strikepointZ[time][__i])]*resolution,np.linspace(0,360,resolution)]).T
			strike_point_location = np.array([[temp_R[time][__i]]*resolution,[temp_Z[time][__i]]*resolution,np.linspace(0,360,resolution)]).T
			strike_point_location = point_toroidal_to_cartesian(strike_point_location)
			strike_point_location = find_location_on_foil(strike_point_location,plane_equation=plane_equation,pinhole_location=pinhole_location)
			temp.append(absolute_position_on_foil_to_foil_coord(strike_point_location,centre_of_foil=centre_of_foil,foil_size=foil_size))
		all_time_strike_points_location_rot.append(temp)
	return all_time_strike_points_location,all_time_strike_points_location_rot

def return_all_time_strike_points_location_radial(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine,resolution = 1000):
	from scipy.signal import find_peaks, peak_prominences as get_proms
	temp_R = np.ones((len(efit_reconstruction.time),20))*np.nan
	temp_Z = np.ones((len(efit_reconstruction.time),20))*np.nan
	for time in range(len(efit_reconstruction.time)):
		temp = np.array([efit_reconstruction.strikepointR[time],efit_reconstruction.strikepointZ[time]]).T
		if temp.max()<1e-1 and len(flatten(all_time_sep_r[time][0]))>0 and len(flatten(all_time_sep_r[time][2]))>0:
			try:
				a=np.concatenate([r_fine[all_time_sep_r[time][0]],r_fine[all_time_sep_r[time][2]]])
				b=np.concatenate([z_fine[all_time_sep_z[time][0]],z_fine[all_time_sep_z[time][2]]])
			except:
				try:
					a=r_fine[all_time_sep_r[time][0]]
					b=z_fine[all_time_sep_z[time][0]]
				except:
					a=r_fine[all_time_sep_r[time][2]]
					b=z_fine[all_time_sep_z[time][2]]
			# c=np.abs(a-R_centre_column)
			c=np.abs(a-R_centre_column_interpolator(b))
			peaks = find_peaks(-c)[0]
			peaks = peaks[c[peaks]<4e-3]
			# efit_reconstruction.strikepointR[time][:min(len(efit_reconstruction.strikepointR[time]),len(peaks))] = a[peaks][:min(len(efit_reconstruction.strikepointR[time]),len(peaks))]
			# efit_reconstruction.strikepointZ[time][:min(len(efit_reconstruction.strikepointZ[time]),len(peaks))] = b[peaks][:min(len(efit_reconstruction.strikepointZ[time]),len(peaks))]
			temp_R[time][:min(20,len(peaks))] = a[peaks][:min(20,len(peaks))]
			temp_Z[time][:min(20,len(peaks))] = b[peaks][:min(20,len(peaks))]
		else:
			temp_R[time][:min(20,len(efit_reconstruction.strikepointR[time]))] = efit_reconstruction.strikepointR[time][:min(20,len(efit_reconstruction.strikepointR[time]))]
			temp_Z[time][:min(20,len(efit_reconstruction.strikepointZ[time]))] = -np.abs(efit_reconstruction.strikepointZ[time][:min(20,len(efit_reconstruction.strikepointZ[time]))])
	return np.transpose([temp_R,temp_Z], axes=(1,0,2))

def return_all_time_separatrix(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine,ref_angle=[],plane_equation=plane_equation,pinhole_location=pinhole_location,centre_of_foil=centre_of_foil,foil_size=foil_size):
	all_time_separatrix = []
	for time in range(len(efit_reconstruction.time)):
		separatrix = []
		for i in range(len(all_time_sep_r[time])*2):
			side = 'left'
			if ref_angle!=[]:
				i_ref_angle = ref_angle[0]
			if i>=len(all_time_sep_r[time]):
				i-=len(all_time_sep_r[time])
				side = 'right'
				if ref_angle!=[]:
					i_ref_angle = ref_angle[1]
			try:
				if ref_angle==[]:
					# the other method consider a flat angle to look at tangency of separatrix and LOS through the pinhole. the next calculates the right angle
					point_location = np.array([r_fine[all_time_sep_r[time][i]],z_fine[all_time_sep_z[time][i]],calculate_tangency_angle_for_poloidal_section(r_fine[all_time_sep_r[time][i]],side=side)]).T
				else:
					point_location = np.array([r_fine[all_time_sep_r[time][i]],z_fine[all_time_sep_z[time][i]],[i_ref_angle]*len(all_time_sep_z[time][i])]).T
			except:
				if ref_angle==[]:
					point_location = np.array([[0],[0],[calculate_tangency_angle_for_poloidal_section(0)]]).T
				else:
					point_location = np.array([[0],[0],[0]]).T
			point_location = point_toroidal_to_cartesian(point_location)
			point_location = find_location_on_foil(point_location,plane_equation=plane_equation,pinhole_location=pinhole_location)
			separatrix.append(absolute_position_on_foil_to_foil_coord(point_location,centre_of_foil=centre_of_foil,foil_size=foil_size))
		all_time_separatrix.append(separatrix)
	return all_time_separatrix

def return_all_time_separatrix_radial(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine):
	all_time_separatrix = []
	for time in range(len(efit_reconstruction.time)):
		separatrix = []
		for i in range(len(all_time_sep_r[time])):
			try:
				separatrix.append([r_fine[all_time_sep_r[time][i]],z_fine[all_time_sep_z[time][i]]])
			except:
				separatrix.append([[0],[0]])
		all_time_separatrix.append(separatrix)
	return all_time_separatrix

def return_core_tangential_location_on_foil(resolution = 10000,plane_equation=plane_equation,pinhole_location=pinhole_location,centre_of_foil=centre_of_foil,foil_size=foil_size):
	core_tangential_location_on_foil = []
	for i in range(len(core_tangential_arrival)):
		point_location = np.array([np.linspace(core_tangential_arrival[i][0],core_tangential_common_point[0],resolution),np.linspace(core_tangential_arrival[i][1],core_tangential_common_point[1],resolution),[core_tangential_arrival[i][2]]*resolution]).T
		point_location = find_location_on_foil(point_location,plane_equation=plane_equation,pinhole_location=pinhole_location)
		core_tangential_location_on_foil.append(absolute_position_on_foil_to_foil_coord(point_location,centre_of_foil=centre_of_foil,foil_size=foil_size))
	return core_tangential_location_on_foil
# core_tangential_location_on_foil = return_core_tangential_location_on_foil()

def return_core_poloidal_location_on_foil(angle=60,resolution = 10000,plane_equation=plane_equation,pinhole_location=pinhole_location,centre_of_foil=centre_of_foil,foil_size=foil_size):
	core_poloidal_location_on_foil = []
	for i in range(len(core_poloidal_arrival)):
		point_location = np.array([np.linspace(core_poloidal_arrival[i][0],core_poloidal_common_point[0],resolution),np.linspace(core_poloidal_arrival[i][1],core_poloidal_common_point[1],resolution),[angle]*resolution]).T
		point_location = point_toroidal_to_cartesian(point_location)
		point_location = find_location_on_foil(point_location,plane_equation=plane_equation,pinhole_location=pinhole_location)
		core_poloidal_location_on_foil.append(absolute_position_on_foil_to_foil_coord(point_location,centre_of_foil=centre_of_foil,foil_size=foil_size))
	return core_poloidal_location_on_foil
# core_poloidal_location_on_foil = return_core_poloidal_location_on_foil()

def return_core_poloidal(resolution = 100):
	core_poloidal = []
	for arrival in core_poloidal_arrival:
		if np.sum(np.isnan(arrival))>0:
			core_poloidal.append([])
		else:
			interp1 = interp1d([arrival[0],core_poloidal_common_point[0]],[arrival[1],core_poloidal_common_point[1]],fill_value="extrapolate",bounds_error=False)
			core_poloidal.append(np.array([np.linspace(arrival[0],core_poloidal_common_point[0],resolution),interp1(np.linspace(arrival[0],core_poloidal_common_point[0],resolution))]).T)
	return core_poloidal

def return_divertor_poloidal_location_on_foil(angle=60,resolution = 10000,plane_equation=plane_equation,pinhole_location=pinhole_location,centre_of_foil=centre_of_foil):
	divertor_poloidal_location_on_foil = []
	for i in range(len(divertor_poloidal_arrival)):
		point_location = np.array([np.linspace(divertor_poloidal_arrival[i][0],divertor_poloidal_common_point[0],resolution),np.linspace(divertor_poloidal_arrival[i][1],divertor_poloidal_common_point[1],resolution),[angle]*resolution]).T
		point_location = point_toroidal_to_cartesian(point_location)
		point_location = find_location_on_foil(point_location,plane_equation=plane_equation,pinhole_location=pinhole_location)
		divertor_poloidal_location_on_foil.append(absolute_position_on_foil_to_foil_coord(point_location,centre_of_foil=centre_of_foil))
	return divertor_poloidal_location_on_foil

def return_divertor_poloidal(resolution = 100):
	divertor_poloidal = []
	for arrival in divertor_poloidal_arrival:
		if np.sum(np.isnan(arrival))>0:
			divertor_poloidal.append([])
		else:
			interp1 = interp1d([arrival[0],divertor_poloidal_common_point[0]],[arrival[1],divertor_poloidal_common_point[1]],fill_value="extrapolate",bounds_error=False)
			divertor_poloidal.append(np.array([np.linspace(arrival[0],divertor_poloidal_common_point[0],resolution),interp1(np.linspace(arrival[0],divertor_poloidal_common_point[0],resolution))]).T)
	return divertor_poloidal

def movie_from_data(data,framerate,integration=1,xlabel=(),ylabel=(),barlabel=(),cmap='rainbow',form_factor_size=15,timesteps='auto',extvmin='auto',extvmax='auto',image_extent=[],mask=[0],mask_alpha=0.2,time_offset=0,prelude='',vline=None,hline=None,EFIT_path=EFIT_path_default,include_EFIT=False,efit_reconstruction=None,EFIT_output_requested = False,pulse_ID=None,overlay_x_point=False,overlay_mag_axis=False,overlay_structure=False,overlay_strike_points=False,overlay_separatrix=False,overlay_res_bolo=False,structure_alpha=0.5,foil_size=foil_size,additional_polygons_dict = dict([])):
	import matplotlib.animation as animation
	import numpy as np

	if len(image_extent)==4:
		form_factor = (image_extent[1]-image_extent[0])/(image_extent[3]-image_extent[2])
	else:
		form_factor = (np.shape(data[0][0])[1])/(np.shape(data[0][0])[0])
	fig = plt.figure(figsize=(form_factor_size*form_factor, form_factor_size))
	ax = fig.add_subplot(111)

	# I like to position my colorbars this way, but you don't have to
	# div = make_axes_locatable(ax)
	# cax = div.append_axes('right', '5%', '5%')

	# def f(x, y):
	#	 return np.exp(x) + np.sin(y)

	# x = np.linspace(0, 1, 120)
	# y = np.linspace(0, 2 * np.pi, 100).reshape(-1, 1)

	if len(image_extent)==4:
		ver = np.arange(0,np.shape(data[0][0])[0])
		up = np.abs(image_extent[3]-ver).argmin()
		down = np.abs(image_extent[2]-ver).argmin()
		hor = np.arange(0,np.shape(data[0][0])[1])
		left = np.abs(image_extent[0]-hor).argmin()
		right = np.abs(image_extent[1]-hor).argmin()
		data[0][:,:,:left] = np.nan
		data[0][:,:,right+1:] = np.nan
		data[0][:,:down] = np.nan
		data[0][:,up+1:] = np.nan

	# This is now a list of arrays rather than a list of artists
	frames = [None]*len(data[0])
	frames[0]=data[0,0]

	for i in range(len(data[0])):
		# x	   += 1
		# curVals  = f(x, y)
		frames[i]=(data[0,i])

	cv0 = frames[0]
	im = ax.imshow(cv0,cmap, origin='lower', interpolation='none') # Here make an AxesImage rather than contour
	if len(image_extent)==4:
		ax.set_ylim(top=image_extent[3],bottom=image_extent[2])
		ax.set_xlim(right=image_extent[1],left=image_extent[0])


	if len(np.shape(mask))==2:
		if np.shape(mask)!=np.shape(data[0][0]):
			print('The shape of the mask '+str(np.shape(mask))+' does not match with the shape of the record '+str(np.shape(data[0][0])))
		masked = np.ma.masked_where(mask == 0, mask)
		im2 = ax.imshow(masked, 'gray', interpolation='none', alpha=mask_alpha,origin='lower',extent = [0,np.shape(cv0)[1]-1,0,np.shape(cv0)[0]-1])

	if pulse_ID!=None:
		if int(pulse_ID) > 45517:	# restricted to MU02
			stand_off_length = 0.06	# m
		else:
			stand_off_length = 0.045	# m
	else:
		stand_off_length = 0.045	# m
	Rf=1.48967 + 0.01 + 0.003 + 0.002 + stand_off_length	# m	radius of the centre of the foil
	plane_equation = np.array([1,-1,0,2**0.5 * Rf])	# plane of the foil
	centre_of_foil = np.array([-Rf/(2**0.5), Rf/(2**0.5), -0.7])	# x,y,z

	if include_EFIT:
		try:
			if efit_reconstruction == None:
				print('reading '+EFIT_path+'/epm0'+str(pulse_ID)+'.nc')
				efit_reconstruction = mclass(EFIT_path+'/epm0'+str(pulse_ID)+'.nc',pulse_ID=pulse_ID)
			else:
				print('EFIT reconstruction externally supplied')
			EFIT_dt = np.median(np.diff(efit_reconstruction.time))
		except Exception as e:
			print('reading '+EFIT_path+'/epm0'+str(pulse_ID)+'.nc failed '+'with error: ' + str(e))
			logging.exception('with error: ' + str(e))
			include_EFIT=False
			overlay_x_point=False
			overlay_mag_axis=False
			overlay_separatrix=False
			overlay_strike_points=False
			overlay_separatrix=False
			efit_reconstruction = None
		if overlay_x_point:
			all_time_x_point_location = return_all_time_x_point_location(efit_reconstruction,plane_equation=plane_equation,centre_of_foil=centre_of_foil)
			plot1 = ax.plot(0,0,'-r', alpha=1)[0]
		if overlay_mag_axis:
			all_time_mag_axis_location = return_all_time_mag_axis_location(efit_reconstruction,plane_equation=plane_equation,centre_of_foil=centre_of_foil)
			plot2 = ax.plot(0,0,'--r', alpha=1)[0]
		if overlay_separatrix or overlay_strike_points:
			all_time_sep_r,all_time_sep_z,r_fine,z_fine = efit_reconstruction_to_separatrix_on_foil(efit_reconstruction)
		if overlay_strike_points:
			all_time_strike_points_location,all_time_strike_points_location_rot = return_all_time_strike_points_location(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine,plane_equation=plane_equation,centre_of_foil=centre_of_foil)
			plot3 = ax.plot(0,0,'xr',markersize=20, alpha=1)[0]
			plot4 = []
			for __i in range(len(all_time_strike_points_location_rot[0])):
				plot4.append(ax.plot(0,0,'-r', alpha=1)[0])
		if overlay_separatrix:
			all_time_separatrix = return_all_time_separatrix(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine,plane_equation=plane_equation,centre_of_foil=centre_of_foil)
			plot5 = []
			for __i in range(len(all_time_separatrix[0])):
				plot5.append(ax.plot(0,0,'--b', alpha=1)[0])
	if overlay_structure:
		structure_point_location_on_foil = return_structure_point_location_on_foil(plane_equation=plane_equation,centre_of_foil=centre_of_foil)
		fueling_point_location_on_foil = return_fueling_point_location_on_foil(plane_equation=plane_equation,centre_of_foil=centre_of_foil)
		for i in range(len(fueling_point_location_on_foil)):
			ax.plot(np.array(fueling_point_location_on_foil[i][:,0])*(np.shape(cv0)[1]-1)/foil_size[0],np.array(fueling_point_location_on_foil[i][:,1])*(np.shape(cv0)[0]-1)/foil_size[1],'+k',markersize=40,alpha=structure_alpha)
			ax.plot(np.array(fueling_point_location_on_foil[i][:,0])*(np.shape(cv0)[1]-1)/foil_size[0],np.array(fueling_point_location_on_foil[i][:,1])*(np.shape(cv0)[0]-1)/foil_size[1],'ok',markersize=5,alpha=structure_alpha)
		for i in range(len(structure_point_location_on_foil)):
			ax.plot(np.array(structure_point_location_on_foil[i][:,0])*(np.shape(cv0)[1]-1)/foil_size[0],np.array(structure_point_location_on_foil[i][:,1])*(np.shape(cv0)[0]-1)/foil_size[1],'--k',alpha=structure_alpha)
	if overlay_res_bolo:
		core_tangential_location_on_foil = return_core_tangential_location_on_foil(plane_equation=plane_equation,centre_of_foil=centre_of_foil)
		core_poloidal_location_on_foil = return_core_poloidal_location_on_foil(plane_equation=plane_equation,centre_of_foil=centre_of_foil)
		for i in range(len(core_tangential_location_on_foil)):
			ax.plot(np.array(core_tangential_location_on_foil[i][:,0])*(np.shape(cv0)[1]-1)/foil_size[0],np.array(core_tangential_location_on_foil[i][:,1])*(np.shape(cv0)[0]-1)/foil_size[1],'--r',alpha=1)
		for i in range(len(core_poloidal_location_on_foil)):
			ax.plot(np.array(core_poloidal_location_on_foil[i][:,0])*(np.shape(cv0)[1]-1)/foil_size[0],np.array(core_poloidal_location_on_foil[i][:,1])*(np.shape(cv0)[0]-1)/foil_size[1],'--r',alpha=1)
	if len(list(additional_polygons_dict.keys()))!=0:
		plot7 = []
		for __i in range(additional_polygons_dict['number_of_polygons']):
			plot7.append(ax.plot(0,0,additional_polygons_dict['marker'][__i], alpha=1)[0])

	# if len(np.shape(mask)) == 2:
	# im = ax.imshow(mask,'gray',interpolation='none',alpha=1)
	if np.sum(vline == None)==0:
		if np.shape(vline)==():
			vline = max(0,min(vline,np.shape(cv0)[1]-1))
			axvline = ax.axvline(x=vline,linestyle='--',color='k')
		else:
			for i in range(len(vline)):
				vline[i] = max(0,min(vline[i],np.shape(cv0)[1]-1))
				axvline = ax.axvline(x=vline[i],linestyle='--',color='k')
	if np.sum(hline == None)==0:
		if np.shape(hline)==():
			hline = max(0,min(hline,np.shape(cv0)[0]-1))
			axhline = ax.axhline(y=hline,linestyle='--',color='k')
		else:
			for i in range(len(hline)):
				hline[i] = max(0,min(hline[i],np.shape(cv0)[0]-1))
				axhline = ax.axhline(y=hline[i],linestyle='--',color='k')

	cb = fig.colorbar(im,fraction=0.0577, pad=0.04).set_label(barlabel)
	cb = ax.set_xlabel(xlabel)
	cb = ax.set_ylabel(ylabel)
	tx = ax.set_title('Frame 0')


	# if timesteps=='auto':
	# 	timesteps_int = time_offset+np.arange(len(data[0])+1)/framerate
	# else:
	# 	timesteps_int = cp.deepcopy(timesteps)
	def animate(i):
		arr = frames[i]
		if extvmax=='auto':
			vmax = np.nanmax(arr)
		elif extvmax=='allmax':
			vmax = np.nanmax(data)
		elif hasattr(extvmax, "__len__"):
			vmax = extvmax[i]
		else:
			vmax = extvmax

		if extvmin=='auto':
			vmin = np.nanmin(arr)
		elif extvmin=='allmin':
			vmin = np.nanmin(data)
		elif hasattr(extvmin, "__len__"):
			vmin = extvmin[i]
		else:
			vmin = extvmin
		im.set_data(arr)
		im.set_clim(vmin, vmax)
		if timesteps=='auto':
			time_int = time_offset+i/framerate
			tx.set_text(prelude + 'Frame {0}'.format(i)+', FR %.3gHz, t %.3gs, int %.3gms' %(framerate,time_int,integration))
		else:
			time_int = timesteps[i]
			tx.set_text(prelude + 'Frame {0}'.format(i)+', t %.3gs, int %.3gms' %(time_int,integration))
		if include_EFIT:
			if np.min(np.abs(time_int-efit_reconstruction.time))>EFIT_dt:	# means that the reconstruction is not available for that time
				if overlay_x_point:
					plot1.set_data(([],[]))
				if overlay_mag_axis:
					plot2.set_data(([],[]))
				if overlay_strike_points:
					plot3.set_data(([],[]))
					for __i in range(len(plot4)):
						plot4[__i].set_data(([],[]))
				if overlay_separatrix:
					for __i in range(len(plot5)):
						plot5[__i].set_data(([],[]))
			else:
				i_time = np.abs(time_int-efit_reconstruction.time).argmin()
				if overlay_x_point:
					if np.sum(np.isnan(all_time_x_point_location[i_time]))>=len(all_time_x_point_location[i_time]):	# means that all the points calculated are outside the foil
						plot1.set_data(([],[]))
					else:
						plot1.set_data((all_time_x_point_location[i_time][:,0]*(np.shape(cv0)[1]-1)/foil_size[0],all_time_x_point_location[i_time][:,1]*(np.shape(cv0)[0]-1)/foil_size[1]))
				if overlay_mag_axis:
					# if np.sum(np.isnan(all_time_mag_axis_location[i_time]))>=len(all_time_mag_axis_location[i_time]):	# means that all the points calculated are outside the foil
					# 	plot2.set_data(([],[]))
					# else:
					plot2.set_data((all_time_mag_axis_location[i_time][:,0]*(np.shape(cv0)[1]-1)/foil_size[0],all_time_mag_axis_location[i_time][:,1]*(np.shape(cv0)[0]-1)/foil_size[1]))
				if overlay_strike_points:
					# if np.sum(np.isnan(all_time_mag_axis_location[i_time]))>=len(all_time_mag_axis_location[i_time]):	# means that all the points calculated are outside the foil
					# 	plot3.set_data(([],[]))
					# 	for __i in range(len(plot4)):
					# 		plot4[__i].set_data(([],[]))
					# else:
					plot3.set_data((all_time_strike_points_location[i_time][:,0]*(np.shape(cv0)[1]-1)/foil_size[0],all_time_strike_points_location[i_time][:,1]*(np.shape(cv0)[0]-1)/foil_size[1]))
					for __i in range(len(plot4)):
						plot4[__i].set_data((all_time_strike_points_location_rot[i_time][__i][:,0]*(np.shape(cv0)[1]-1)/foil_size[0],all_time_strike_points_location_rot[i_time][__i][:,1]*(np.shape(cv0)[0]-1)/foil_size[1]))
				if overlay_separatrix:
					# if np.sum(np.isnan(all_time_mag_axis_location[i_time]))>=len(all_time_mag_axis_location[i_time]):	# means that all the points calculated are outside the foil
					for __i in range(len(plot5)):
						plot5[__i].set_data((all_time_separatrix[i_time][__i][:,0]*(np.shape(cv0)[1]-1)/foil_size[0],all_time_separatrix[i_time][__i][:,1]*(np.shape(cv0)[0]-1)/foil_size[1]))
		if len(list(additional_polygons_dict.keys()))!=0:
			i_time2 = np.abs(time_int-additional_polygons_dict['time']).argmin()
			# if np.sum(np.isnan(all_time_mag_axis_location[i_time]))>=len(all_time_mag_axis_location[i_time]):	# means that all the points calculated are outside the foil
			for __i in range(additional_polygons_dict['number_of_polygons']):
				plot7[__i].set_data((additional_polygons_dict[str(__i)][i_time2][0],additional_polygons_dict[str(__i)][i_time2][1]))
			# 	masked = np.ma.masked_where(mask == 0, mask)
			# 	ax.imshow(masked, 'gray', interpolation='none', alpha=0.2,origin='lower',extent = [0,np.shape(data)[0]-1,0,np.shape(data)[1]-1])

			# In this version you don't have to do anything to the colorbar,
			# it updates itself when the mappable it watches (im) changes
	# else:
	# 	def animate(i):
	# 		arr = frames[i]
	# 		if extvmax=='auto':
	# 			vmax = np.max(arr)
	# 		elif extvmax=='allmax':
	# 			vmax = np.max(data)
	# 		else:
	# 			vmax = extvmax
	#
	# 		if extvmin=='auto':
	# 			vmin = np.min(arr)
	# 		elif extvmin=='allmin':
	# 			vmin = np.min(data)
	# 		else:
	# 			vmin = extvmin
	# 		im.set_data(arr)
	# 		im.set_clim(vmin, vmax)
	# 		tx.set_text(prelude + 'Frame {0}'.format(i)+', t %.3gs, int %.3gms' %(timesteps[i],integration))
	# 		# if len(np.shape(mask)) == 2:
	# 		# 	# 	im.imshow(mask,'binary',interpolation='none',alpha=0.3)
	# 		# 	masked = np.ma.masked_where(mask == 0, mask)
	# 		# 	ax.imshow(masked, 'gray', interpolation='none', alpha=0.2,origin='lower',extent = [0,np.shape(data)[0]-1,0,np.shape(data)[1]-1])

	ani = animation.FuncAnimation(fig, animate, frames=len(data[0]))

	if EFIT_output_requested == False:
		return ani
	else:
		return ani,efit_reconstruction

def image_from_data(data,xlabel=(),ylabel=(),barlabel=(),cmap='rainbow',form_factor_size=15,ref_time=None,extvmin='auto',extvmax='auto',image_extent=[],mask=[0],mask_alpha=0.2,prelude='',vline=None,hline=None,EFIT_path=EFIT_path_default,include_EFIT=False,efit_reconstruction=None,EFIT_output_requested = False,pulse_ID=None,overlay_x_point=False,overlay_mag_axis=False,overlay_structure=False,overlay_strike_points=False,overlay_separatrix=False,overlay_res_bolo=False,structure_alpha=0.5,foil_size=foil_size,generic_overlays = []):
	import matplotlib.animation as animation
	import numpy as np
	from matplotlib import cm	# to print nan as white

	if len(image_extent)==4:
		form_factor = (image_extent[1]-image_extent[0])/(image_extent[3]-image_extent[2])
	else:
		form_factor = (np.shape(data[0][0])[1])/(np.shape(data[0][0])[0])
	fig = plt.figure(figsize=(form_factor_size*form_factor, form_factor_size))
	ax = fig.add_subplot(111)

	# I like to position my colorbars this way, but you don't have to
	# div = make_axes_locatable(ax)
	# cax = div.append_axes('right', '5%', '5%')

	# def f(x, y):
	#	 return np.exp(x) + np.sin(y)

	# x = np.linspace(0, 1, 120)
	# y = np.linspace(0, 2 * np.pi, 100).reshape(-1, 1)

	if len(image_extent)==4:
		ver = np.arange(0,np.shape(data[0][0])[0])
		up = np.abs(image_extent[3]-ver).argmin()
		down = np.abs(image_extent[2]-ver).argmin()
		hor = np.arange(0,np.shape(data[0][0])[1])
		left = np.abs(image_extent[0]-hor).argmin()
		right = np.abs(image_extent[1]-hor).argmin()
		data = data.astype(float)
		data[0][:,:,:left] = np.nan
		data[0][:,:,right+1:] = np.nan
		data[0][:,:down] = np.nan
		data[0][:,up+1:] = np.nan

	# This is now a list of arrays rather than a list of artists
	frames = [None]*len(data[0])
	frames[0]=data[0,0]

	for i in range(len(data[0])):
		# x	   += 1
		# curVals  = f(x, y)
		frames[i]=(data[0,i])

	cv0 = frames[0]
	masked_array = np.ma.array (cv0, mask=np.isnan(cv0))
	# exec('cmap=cm.' + cmap)
	cmap=cm.rainbow
	cmap.set_bad('white',1.)
	im = ax.imshow(masked_array,cmap=cmap, origin='lower', interpolation='none') # Here make an AxesImage rather than contour
	if len(image_extent)==4:
		ax.set_ylim(top=image_extent[3],bottom=image_extent[2])
		ax.set_xlim(right=image_extent[1],left=image_extent[0])


	if len(np.shape(mask))==2:
		if np.shape(mask)!=np.shape(data[0][0]):
			print('The shape of the mask '+str(np.shape(mask))+' does not match with the shape of the record '+str(np.shape(data[0][0])))
		masked = np.ma.masked_where(mask == 0, mask)
		im2 = ax.imshow(masked, 'gray', interpolation='none', alpha=mask_alpha,origin='lower',extent = [0,np.shape(cv0)[1]-1,0,np.shape(cv0)[0]-1])

	if int(pulse_ID) > 45517:	# restricted to MU02
		stand_off_length = 0.06	# m
	else:
		stand_off_length = 0.045	# m
	Rf=1.48967 + 0.01 + 0.003 + 0.002 + stand_off_length	# m	radius of the centre of the foil
	plane_equation = np.array([1,-1,0,2**0.5 * Rf])	# plane of the foil
	centre_of_foil = np.array([-Rf/(2**0.5), Rf/(2**0.5), -0.7])	# x,y,z

	if ref_time==None:
		include_EFIT = False

	if include_EFIT:
		try:
			if efit_reconstruction == None:
				print('reading '+EFIT_path+'/epm0'+str(pulse_ID)+'.nc')
				efit_reconstruction = mclass(EFIT_path+'/epm0'+str(pulse_ID)+'.nc',pulse_ID=pulse_ID)
			else:
				print('EFIT reconstruction externally supplied')
			EFIT_dt = np.median(np.diff(efit_reconstruction.time))
		except Exception as e:
			print('reading '+EFIT_path+'/epm0'+str(pulse_ID)+'.nc failed')
			logging.exception('with error: ' + str(e))
			include_EFIT=False
			overlay_x_point=False
			overlay_mag_axis=False
			overlay_separatrix=False
			overlay_strike_points=False
			overlay_separatrix=False
			efit_reconstruction = None
		if overlay_x_point:
			all_time_x_point_location = return_all_time_x_point_location(efit_reconstruction,plane_equation=plane_equation,centre_of_foil=centre_of_foil)
			plot1 = ax.plot(0,0,'-r', alpha=1)[0]
		if overlay_mag_axis:
			all_time_mag_axis_location = return_all_time_mag_axis_location(efit_reconstruction,plane_equation=plane_equation,centre_of_foil=centre_of_foil)
			plot2 = ax.plot(0,0,'--r', alpha=1)[0]
		if overlay_separatrix or overlay_strike_points:
			all_time_sep_r,all_time_sep_z,r_fine,z_fine = efit_reconstruction_to_separatrix_on_foil(efit_reconstruction)
		if overlay_strike_points:
			all_time_strike_points_location,all_time_strike_points_location_rot = return_all_time_strike_points_location(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine,plane_equation=plane_equation,centre_of_foil=centre_of_foil)
			plot3 = ax.plot(0,0,'xr',markersize=20, alpha=1)[0]
			plot4 = []
			for __i in range(len(all_time_strike_points_location_rot[0])):
				plot4.append(ax.plot(0,0,'-r', alpha=1)[0])
		if overlay_separatrix:
			all_time_separatrix = return_all_time_separatrix(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine,plane_equation=plane_equation,centre_of_foil=centre_of_foil)
			plot5 = []
			for __i in range(len(all_time_separatrix[0])):
				plot5.append(ax.plot(0,0,'--b', alpha=1)[0])
	if overlay_structure:
		structure_point_location_on_foil = return_structure_point_location_on_foil(plane_equation=plane_equation,centre_of_foil=centre_of_foil)
		fueling_point_location_on_foil = return_fueling_point_location_on_foil(plane_equation=plane_equation,centre_of_foil=centre_of_foil)
		for i in range(len(fueling_point_location_on_foil)):
			ax.plot(np.array(fueling_point_location_on_foil[i][:,0])*(np.shape(cv0)[1]-1)/foil_size[0],np.array(fueling_point_location_on_foil[i][:,1])*(np.shape(cv0)[0]-1)/foil_size[1],'+k',markersize=40,alpha=structure_alpha)
			ax.plot(np.array(fueling_point_location_on_foil[i][:,0])*(np.shape(cv0)[1]-1)/foil_size[0],np.array(fueling_point_location_on_foil[i][:,1])*(np.shape(cv0)[0]-1)/foil_size[1],'ok',markersize=5,alpha=structure_alpha)
		for i in range(len(structure_point_location_on_foil)):
			ax.plot(np.array(structure_point_location_on_foil[i][:,0])*(np.shape(cv0)[1]-1)/foil_size[0],np.array(structure_point_location_on_foil[i][:,1])*(np.shape(cv0)[0]-1)/foil_size[1],'--k',alpha=structure_alpha)
	if overlay_res_bolo:
		core_tangential_location_on_foil = return_core_tangential_location_on_foil(plane_equation=plane_equation,centre_of_foil=centre_of_foil)
		core_poloidal_location_on_foil = return_core_poloidal_location_on_foil(plane_equation=plane_equation,centre_of_foil=centre_of_foil)
		for i in range(len(core_tangential_location_on_foil)):
			ax.plot(np.array(core_tangential_location_on_foil[i][:,0])*(np.shape(cv0)[1]-1)/foil_size[0],np.array(core_tangential_location_on_foil[i][:,1])*(np.shape(cv0)[0]-1)/foil_size[1],'--r',alpha=1)
		for i in range(len(core_poloidal_location_on_foil)):
			ax.plot(np.array(core_poloidal_location_on_foil[i][:,0])*(np.shape(cv0)[1]-1)/foil_size[0],np.array(core_poloidal_location_on_foil[i][:,1])*(np.shape(cv0)[0]-1)/foil_size[1],'--r',alpha=1)

	# if len(np.shape(mask)) == 2:
	# im = ax.imshow(mask,'gray',interpolation='none',alpha=1)
	if np.sum(vline == None)==0:
		if np.shape(vline)==():
			vline = max(0,min(vline,np.shape(cv0)[1]-1))
			axvline = ax.axvline(x=vline,linestyle='--',color='k')
		else:
			for i in range(len(vline)):
				vline[i] = max(0,min(vline[i],np.shape(cv0)[1]-1))
				axvline = ax.axvline(x=vline[i],linestyle='--',color='k')
	if np.sum(hline == None)==0:
		if np.shape(hline)==():
			hline = max(0,min(hline,np.shape(cv0)[0]-1))
			axhline = ax.axhline(y=hline,linestyle='--',color='k')
		else:
			for i in range(len(hline)):
				hline[i] = max(0,min(hline[i],np.shape(cv0)[0]-1))
				axhline = ax.axhline(y=hline[i],linestyle='--',color='k')

	if len(generic_overlays)>0:
		for overlay_dict in generic_overlays:
			ax.plot(overlay_dict['edges_horiz'],overlay_dict['edges_vert'],overlay_dict['linestyle'],alpha=overlay_dict['alpha'],linewidth=overlay_dict['linewidth'])

	cb = fig.colorbar(im,fraction=0.0577, pad=0.04).set_label(barlabel)
	cb = ax.set_xlabel(xlabel)
	cb = ax.set_ylabel(ylabel)
	tx = ax.set_title('Frame 0')


	arr = frames[0]
	if extvmax=='auto':
		vmax = np.nanmax(arr)
	elif extvmax=='allmax':
		vmax = np.nanmax(data)
	else:
		vmax = extvmax

	if extvmin=='auto':
		vmin = np.nanmin(arr)
	elif extvmin=='allmin':
		vmin = np.nanmin(data)
	else:
		vmin = extvmin
	masked_array = np.ma.array (arr, mask=np.isnan(arr))
	im.set_data(masked_array)
	im.set_clim(vmin, vmax)
	tx.set_text(prelude)
	if include_EFIT:
		if np.min(np.abs(ref_time-efit_reconstruction.time))>EFIT_dt:	# means that the reconstruction is not available for that time
			if overlay_x_point:
				plot1.set_data(([],[]))
			if overlay_mag_axis:
				plot2.set_data(([],[]))
			if overlay_strike_points:
				plot3.set_data(([],[]))
				for __i in range(len(plot4)):
					plot4[__i].set_data(([],[]))
			if overlay_separatrix:
				for __i in range(len(plot5)):
					plot5[__i].set_data(([],[]))
		else:
			i_time = np.abs(ref_time-efit_reconstruction.time).argmin()
			if overlay_x_point:
				if np.sum(np.isnan(all_time_x_point_location[i_time]))>=len(all_time_x_point_location[i_time]):	# means that all the points calculated are outside the foil
					plot1.set_data(([],[]))
				else:
					plot1.set_data((all_time_x_point_location[i_time][:,0]*(np.shape(cv0)[1]-1)/foil_size[0],all_time_x_point_location[i_time][:,1]*(np.shape(cv0)[0]-1)/foil_size[1]))
			if overlay_mag_axis:
				# if np.sum(np.isnan(all_time_mag_axis_location[i_time]))>=len(all_time_mag_axis_location[i_time]):	# means that all the points calculated are outside the foil
				# 	plot2.set_data(([],[]))
				# else:
				plot2.set_data((all_time_mag_axis_location[i_time][:,0]*(np.shape(cv0)[1]-1)/foil_size[0],all_time_mag_axis_location[i_time][:,1]*(np.shape(cv0)[0]-1)/foil_size[1]))
			if overlay_strike_points:
				# if np.sum(np.isnan(all_time_mag_axis_location[i_time]))>=len(all_time_mag_axis_location[i_time]):	# means that all the points calculated are outside the foil
				# 	plot3.set_data(([],[]))
				# 	for __i in range(len(plot4)):
				# 		plot4[__i].set_data(([],[]))
				# else:
				plot3.set_data((all_time_strike_points_location[i_time][:,0]*(np.shape(cv0)[1]-1)/foil_size[0],all_time_strike_points_location[i_time][:,1]*(np.shape(cv0)[0]-1)/foil_size[1]))
				for __i in range(len(plot4)):
					plot4[__i].set_data((all_time_strike_points_location_rot[i_time][__i][:,0]*(np.shape(cv0)[1]-1)/foil_size[0],all_time_strike_points_location_rot[i_time][__i][:,1]*(np.shape(cv0)[0]-1)/foil_size[1]))
			if overlay_separatrix:
				# if np.sum(np.isnan(all_time_mag_axis_location[i_time]))>=len(all_time_mag_axis_location[i_time]):	# means that all the points calculated are outside the foil
				for __i in range(len(plot5)):
					plot5[__i].set_data((all_time_separatrix[i_time][__i][:,0]*(np.shape(cv0)[1]-1)/foil_size[0],all_time_separatrix[i_time][__i][:,1]*(np.shape(cv0)[0]-1)/foil_size[1]))

	if EFIT_output_requested == False:
		return fig
	else:
		return fig,efit_reconstruction



######################################################################################################


def collect_stat(extpath):

	##print('sys.argv[0] =', sys.argv[0])
	#pathname = os.path.dirname(sys.argv[0])
	##print('path =', pathname)
	#print('full path =', os.path.abspath(pathname))
	#path=os.path.abspath(pathname)


	# path=os.getcwd()
	path=extpath
	print('path =', path)


	filenames=all_file_names(path,'stat.npy')
	#
	# position=[]
	# for i in range(len(path)):
	# 	if path[i]=='/':
	# 		position.append(i)
	# position=max(position)
	# lastpath=path[position+1:]
	# # print('lastpath',lastpath)
	#
	# f = []
	# for (dirpath, dirnames, filenames) in os.walk(path):
	# 	f.extend(filenames)
	# #	break
	#
	# filenames=f
	# # print('filenames',filenames)
	#
	# filefits=[]
	# temp=[]
	# print('len(filenames)',len(filenames))
	# for index in range(len(filenames)):
	# 	# print(filenames[index])
	# 	if filenames[index][-8:]=='stat.npy':
	# 		temp.append(filenames[index])
	# 		# filenames=np.delete(filenames,index)
	# 		# print('suca')
	# filenames=temp
	# filenames=sorted(filenames, key=str.lower)
	#
	# numfiles=len(filenames)


	filename=filenames[0]

	data=np.load(os.path.join(extpath,filename))

	return data

########################################################################################################


def evaluate_back(extpath):

	# THIS FUNCTION PREPARE THE FILES THAT WILL BE USED FOR GETTING THE COUNTS TO TEMPERATURE CALIBRATION
	#
	# UPDATE 10/05/2018 I FOUND AN ERROR AND CHANGED datastat[0]=np.mean(data,axis=(-1,-2))  TO   datastat[0]=np.mean(data,axis=(0,1))

	##print('sys.argv[0] =', sys.argv[0])
	#pathname = os.path.dirname(sys.argv[0])
	##print('path =', pathname)
	#print('full path =', os.path.abspath(pathname))
	#path=os.path.abspath(pathname)

	path=extpath
	# path=os.getcwd()
	print('path =', path)

	position=[]
	for i in range(len(path)):
		if path[i]=='/':
			position.append(i)
	position=max(position)
	lastpath=path[position+1:]
	print('lastpath',lastpath)

	f = []
	for (dirpath, dirnames, filenames) in os.walk(path):
		f.extend(filenames)
	#	break

	filenames=sorted(filenames, key=str.lower)
	temp=[]
	# print('len(filenames)',len(filenames))
	for index in range(len(filenames)):
		# print(filenames[index])
		if filenames[index][-3:]=='npy' and filenames[index][-3-6:] != '_stat.npy':
			temp.append(filenames[index])
			# filenames=np.delete(filenames,index)
			# print('suca')

	filenames=temp

	#filenames=np.delete(filenames,0)
	numfiles=len(filenames)


	print('filenames',filenames)


	data=np.load(os.path.join(path,filenames[0]))
	datashape=np.shape(data)[-2:]

	datastatshape=[2]
	datastatshape.extend(datashape)
	datastat=np.zeros(datastatshape)

	print('np.shape(datastatshape)',np.shape(datastat))

	datastat[0]=np.mean(data,axis=(0,1))
	datastat[1]=np.std(data,axis=(0,1))

	# for i in range(datashape[0]):
	# 	for j in range(datashape[1]):
	# 		datastat[0,i,j]=np.mean(data[0,:,i,j])
	# 		datastat[1,i,j]=np.std(data[0,:,i,j])

	# datastat.extend([np.mean(data[0,:,:,:]),np.std(data[0,:,:,:])])
	# datastat[2]=np.std(data[0,:,:,:])

	# plt.pcolor(datastat[0])
	# plt.colorbar()
	# plt.figure()
	# plt.pcolor(datastat[1])
	# plt.colorbar()
	# plt.show()

	datastat=datastat.tolist()
	datastat.append([np.mean(data[0,:,:,:]),np.std(data[0,:,:,:])])

	np.save(os.path.join(extpath,lastpath+'_stat'),datastat)

######################################################################################################################

def all_file_names(extpath,type):

	# This utility returns a list with all filenames of a defined type in a given folder, orderen alphabetically and in number

	# path=os.getcwd()
	path=extpath
	print('path =', path)

	# position=[]
	# for i in range(len(path)):
	# 	if path[i]=='/':
	# 		position.append(i)
	# position=max(position)
	# lastpath=path[position+1:]
	# print('lastpath',lastpath)

	f = []
	for (dirpath, dirnames, filenames) in os.walk(path):
		f.extend(filenames)
	#	break

	filenames=f
	# print('filenames',filenames)


	temp=[]

	typelen=len(type)
	for index in range(len(filenames)):
		# print(filenames[index])
		if filenames[index][-typelen:]==type:
			temp.append(filenames[index])
		elif filenames[index][:typelen]==type:
			temp.append(filenames[index])

			# filenames=np.delete(filenames,index)
			# print('suca')
	filenames=temp

	if len(filenames)==0:
		print('ERROR - there are no files of type '+type+' in path '+path)
		return ()

	if len(filenames)==1:
		print('len(filenames)',len(filenames))
		return filenames

	filenames=order_filenames(filenames)

	print('len(filenames)',len(filenames))
	return filenames

######################################################################################################

def read_csv(extpath,filenames):

	# SLOW VERSION OF READ_CSV WITH csv.reader

	path=extpath
	print('path =', path)

	numfiles=len(filenames)


	filename=filenames[0]

	# print('os.path.join(path,filename)',os.path.join(path,filename))

	firstrow=-1
	with open(os.path.join(path,filename),'r') as csvfile:
		reader = csv.reader(csvfile)
		# print('reader',reader)
		pointer=0
		for row in reader:
	#		print('shape',np.shape(row))
			# print('row',row)
			if not row:
				temp='empty'
			else:
				temp=row[0]
			if (is_number(temp)) & (firstrow==-1):
				firstrow=pointer
				rowlen=np.shape(row)[0]
				pointer+=1
			elif is_number(temp) & firstrow>-1:
				pointer+=1
			else:
				ponter=0
	lastrow=pointer
	sizey=lastrow-firstrow
	sizex=rowlen


	data=np.zeros((1,numfiles,sizey,sizex))

	print('firstrow,lastrow,sizey,sizex',firstrow,lastrow,sizey,sizex)

	file=0
	for filename in filenames:
		with open(os.path.join(path,filename),'r') as csvfile:
			reader = csv.reader(csvfile)
			# print('reader',reader)
			tempdata=[]
			pointer=sizey-1
			for row in reader:
		#		print('shape',np.shape(row))
				# print('row',row)
				if not row:
					temp='empty'
				else:
					temp=row[0]
				if is_number(temp):
					for k in range(len(row)):
						# print('j,i,pointer,k',j,i,pointer,k)
						data[0,file,pointer,k]=(float(row[k]))
						# print('float(k)',float(row[k]))
					pointer-=1
					# print('row',row)
				else:
					ponter=0
			file+=1

	return data

###########################################################################################################

def read_csv2(extpath,filenames):

	# FAST VERSION OF READ_CVS WITH PANDA READER

	path=extpath
	print('path =', path)

	numfiles=len(filenames)


	filename=filenames[0]

	# print('os.path.join(path,filename)',os.path.join(path,filename))

	firstrow=-1
	csvfile=path+'/'+filename
	reader = pandas.read_csv(csvfile,sep='\r')
	reader=reader.values
	# print('reader',reader)
	pointer=0
	for row in reader:
#		print('shape',np.shape(row))
		# print('row',row)
		# if np.fromstring(row[0],sep=',').size>0:
			# print('sti cazzi2')
			# print(row)
		# print((np.fromstring(row[0],sep=',')).size>0)
		if (((np.fromstring(row[0],sep=',')).size>0) & (firstrow==-1)):
			# print((np.fromstring(row[0],sep=',')).size>0)
			# print('pointer,firstrow',pointer,firstrow)
			firstrow=pointer
			rowlen=np.shape(np.fromstring(row[0],sep=','))[0]
			pointer+=1
		# elif ((np.fromstring(row[0],sep=',')).size>0) & (firstrow>-1):
		# 	pointer+=1
		else:
			pointer+=1

	lastrow=pointer
	sizey=lastrow-firstrow
	sizex=rowlen


	data=np.zeros((1,numfiles,sizey,sizex))

	print('firstrow,lastrow,sizey,sizex',firstrow,lastrow,sizey,sizex)

	file=0
	for filename in filenames:
		csvfile=path+'/'+filename
		# print('csvfile',csvfile)
		reader = pandas.read_csv(csvfile,sep='\r')
		reader=reader.values
		reader=np.delete(reader,(np.linspace(0,firstrow-1,firstrow).astype(int)))
		# print('reader',reader)
		tempdata=[]
		pointer=sizey-1
		# pointer=sizey-1
		for row in reader:
			# tempdata.append(row[0])
			# tempdata=np.array(tempdata[firstrow:])
			data[0,file,pointer]=np.fromstring(row,sep=',')
			# print('np.fromstring(row[0],sep=',')',row)
			pointer-=1
		file+=1
	# data=data.astype(float)
	# print('suca')
	return data

###########################################################################################################


def read_fits(extpath,filenames,filefits):

	if len(filenames)>2:
		print('this is wrong! to make this function work toy must provide as second agrument the first and last frame of the movie as .csv')
		exit()

	numfiles=len(filenames)


	filename=filenames[0]

	# print('os.path.join(path,filename)',os.path.join(path,filename))

	firstrow=-1
	with open(os.path.join(path,filename),'r') as csvfile:
		reader = csv.reader(csvfile)
		# print('reader',reader)
		pointer=0
		for row in reader:
	#		print('shape',np.shape(row))
			# print('row',row)
			if not row:
				temp='empty'
			else:
				temp=row[0]
			if (is_number(temp)) & (firstrow==-1):
				firstrow=pointer
				rowlen=np.shape(row)[0]
				pointer+=1
			elif is_number(temp) & firstrow>-1:
				pointer+=1
			else:
				ponter=0
	lastrow=pointer
	sizey=lastrow-firstrow
	sizex=rowlen


	datafit=fits.open(os.path.join(path,filefits[0]))
	datafit=datafit[0].data

	datafit=datafit+32767
	lenfits=len(datafit)

	data=np.zeros((1,lenfits,sizey,sizex))
	datatest=np.zeros((1,numfiles,sizey,sizex))
	index=0
	for frame in datafit:
		# plt.pcolor(frame)
		# plt.show()
		frame=np.flip(frame,0)
		# plt.pcolor(frame)
		# plt.show()
		data[0,index,:,:]=frame
		index+=1


	print('firstrow,lastrow,sizey,sizex',firstrow,lastrow,sizey,sizex)

	file=0
	for filename in filenames:
		with open(os.path.join(path,filename),'r') as csvfile:
			reader = csv.reader(csvfile)
			# print('reader',reader)
			tempdata=[]
			pointer=sizey-1
			for row in reader:
		#		print('shape',np.shape(row))
				# print('row',row)
				if not row:
					temp='empty'
				else:
					temp=row[0]
				if is_number(temp):
					for k in range(len(row)):
						# print('j,i,pointer,k',j,i,pointer,k)
						datatest[0,file,pointer,k]=(float(row[k]))
						# print('float(k)',float(row[k]))
					pointer-=1
					# print('row',row)
				else:
					ponter=0
			file+=1

	if (np.array_equal(datatest[0,0],data[0,0]))&(np.array_equal(datatest[0,-1],data[0,-1])):
		print('there must be something wrong, the first or last frame of the TITS file do not match with the two csv files')
		exit()

	return data

################################################################################################

def count_to_temp_poly(data,params,errparams,errdata=(0,0)):
	print('ERROR: count_to_temp_poly was deleted 2018/04/02 because count_to_temp_poly2 has a better formulation for the error')
	exit()

	polygrade=len(params[0,0])

	shape=np.shape(data)
	datatemp=np.zeros(shape)

	if np.max(errdata)==0:
		errdata=np.zeros(shape)

	errdatatemp=np.zeros(shape)
	numframes=shape[1]
	sizey=shape[-2]
	sizex=shape[-1]
	sumi2=sum(np.power(np.linspace(1,polygrade-1,polygrade-1),2))

	# OLD VERSION WHEN POLYGEN WAS ABLE TO PRECESSO ONLY SCALARS 2018/03/29
	# for i in range(numframes):
	# 	for j in range(sizey):
	# 		index=0
	# 		for count in data[0,i,j]:
	# 			datatemp[0,i,j,index]=polygen(polygrade)(count,*params[j,index])
	# 			errdatatemp[0,i,j,index]=count*np.sqrt(sum([sumi2*errdata[0,i,j,index]**2/count**2]+np.power(np.divide(errparams[j,index],params[j,index]),2)))
	# 			index+=1

	paramerr=np.sum(np.power(np.divide(errparams,params),2),axis=-1)
	index=0
	for frame in data[0]:
		# print('np.shape(frame)',np.shape(frame))
		# print('np.shape(params)',np.shape(params))
		datatemp[0,index]=polygen(polygrade)(frame,*params)
		errdatatemp[0,index]=np.multiply(datatemp[0,index],np.sqrt(np.add(sumi2*np.divide(np.power(errdata[0,index],2),np.power(frame,2)),paramerr)))
		index+=1

	return datatemp,errdatatemp

################################################################################################

def count_to_temp_poly2(data,params,errparams,errdata=(0,0),averaged_params=False):

	polygrade=len(params[0,0])

	shape=np.shape(data)
	datatemp=np.zeros(shape)

	if np.max(errdata)==0:
		errdata=np.zeros(shape)

	errdatatemp=np.zeros(shape)
	numframes=shape[1]
	sizey=shape[-2]
	sizex=shape[-1]
	#sumi2=sum(np.power(np.linspace(1,polygrade-1,polygrade-1),2))

	# paramerr=np.sum(np.power(np.divide(errparams,params),2),axis=-1)

	if averaged_params==True:
		params_mean=np.mean(params,axis=(0,1))
		params_to_use=np.ones(np.shape(params))*params_mean
		errparams_mean=np.mean(errparams,axis=(0,1))
		errparams_to_use=np.ones(np.shape(errparams))*errparams_mean
	else:
		params_to_use=params
		errparams_to_use=errparams

	index=0
	for frame in data[0]:
		# print('np.shape(frame)',np.shape(frame))
		# print('np.shape(params)',np.shape(params))
		datatemp[0,index]=polygen(polygrade)(frame,*params_to_use)
		temp=0
		for i in range(polygrade):
			temp+=np.power(np.multiply(errparams_to_use[:,:,i],np.power(frame,i)),2)
			if i>0:
				temp+=np.power(np.multiply(np.multiply(np.multiply(params_to_use[:,:,i],np.power(frame,i-1)),i),errdata[0,index]),2)
		# errdatatemp[0,index]=np.sqrt(np.add(sumi2*np.divide(np.power(errdata[0,index],2),np.power(frame,2)),paramerr))
		errdatatemp[0,index]=np.sqrt(temp)
		index+=1




	return datatemp,errdatatemp

################################################################################################

def build_poly_coeff(temperature,files,int,path,nmax):
	# modified 2018-10-08 to build the coefficient only for 1 degree of polinomial
	while np.shape(temperature[0])!=():
		temperature=np.concatenate(temperature)
		files=np.concatenate(files)
	meancounttot=[]
	meancountstdtot=[]
	for file in files:
		data=collect_stat(file)
		meancounttot.append(np.array(data[0]))
		meancountstdtot.append(np.array(data[1]))

	meancounttot=np.array(meancounttot)
	meancountstdtot=np.array(meancountstdtot)
	# nmax=5
	shapex=np.shape(meancounttot[0])[0]
	shapey=np.shape(meancounttot[0])[1]
	score=np.zeros((nmax-1,shapex,shapey))

	#for n in range(2,nmax+1):
	n=nmax
	guess=np.ones(n)
	guess,temp2=curve_fit(polygen3(n), meancounttot[:,0,0],temperature, p0=guess, maxfev=100000000)

	coeff=np.zeros((shapex,shapey,n))
	errcoeff=np.zeros((shapex,shapey,n))

	for j in range(shapex):
		for k in range(shapey):
			x=np.array(meancounttot[:,j,k])
			xerr=np.array(meancountstdtot[:,j,k])
			temp1,temp2=curve_fit(polygen3(n),x, temperature, p0=guess, maxfev=100000000)
			yerr=(polygen3(n)((x+xerr),*temp1)-polygen3(n)((x-xerr),*temp1))/2
			temp1,temp2=curve_fit(polygen3(n),x, temperature, p0=temp1, sigma=yerr, maxfev=100000000)
			# yerr=(polygen3(n)((x+xerr),*temp1)-polygen3(n)((x-xerr),*temp1))/2
			guess=temp1
			coeff[j,k,:]=temp1
			errcoeff[j,k,:]=np.sqrt(np.diagonal(temp2))
			score[n-2,j,k]=rsquared(temperature,polygen3(n)(x,*temp1))
	np.save(os.path.join(path,'coeffpolydeg'+str(n)+'int'+str(int)+'ms'),coeff)
	np.save(os.path.join(path,'errcoeffpolydeg'+str(n)+'int'+str(int)+'ms'),errcoeff)

	print('for a polinomial of degree '+str(n-1)+' the R^2 score is '+str(np.sum(score[n-2])))


###############################################################################################################

def build_poly_coeff2(temperature,files,int,path,nmax):
	print('ERROR: builf_poly_coeff2 was deleted 2018/04/02 because curve_fit must be used without the flag absolute_sigma=True on for significant parameters covariance matrix')
	exit()

	while np.shape(temperature[0])!=():
		temperature=np.concatenate(temperature)
		files=np.concatenate(files)
	meancounttot=[]
	meancountstdtot=[]
	for file in files:
		data=collect_stat(file)
		meancounttot.append(np.array(data[0]))
		meancountstdtot.append(np.array(data[1]))

	meancounttot=np.array(meancounttot)
	meancountstdtot=np.array(meancountstdtot)
	# nmax=5
	shapex=np.shape(meancounttot[0])[0]
	shapey=np.shape(meancounttot[0])[1]
	score=np.zeros((nmax-1,shapex,shapey))

	for n in range(2,nmax+1):
		guess=np.ones(n)
		guess,temp2=curve_fit(polygen3(n), meancounttot[:,0,0],temperature, p0=guess, maxfev=100000000)

		coeff=np.zeros((shapex,shapey,n))
		errcoeff=np.zeros((shapex,shapey,n))

		for j in range(shapex):
			for k in range(shapey):
				x=np.array(meancounttot[:,j,k])
				xerr=np.array(meancountstdtot[:,j,k])
				temp1,temp2=curve_fit(polygen3(n),x, temperature, p0=guess,absolute_sigma=True)
				yerr=(polygen3(n)((x+xerr),*temp1)-polygen3(n)((x-xerr),*temp1))/2
				temp1,temp2=curve_fit(polygen3(n),x, temperature, p0=temp1, sigma=yerr, maxfev=100000000,absolute_sigma=True)
				yerr=(polygen3(n)((x+xerr),*temp1)-polygen3(n)((x-xerr),*temp1))/2
				guess=temp1
				coeff[j,k,:]=temp1
				errcoeff[j,k,:]=np.sqrt(np.diagonal(temp2))
				score[n-2,j,k]=rsquared(temperature,polygen3(n)(x,*temp1))
		np.save(os.path.join(path,'coeffpolydeg'+str(n)+'int'+str(int)+'ms'),coeff)
		np.save(os.path.join(path,'errcoeffpolydeg'+str(n)+'int'+str(int)+'ms'),errcoeff)

		print('for a polinomial with '+str(n)+' coefficients the R^2 score is '+str(np.sum(score[n-2])))

################################################################################################

def build_multiple_poly_coeff(temperaturehot,temperaturecold,fileshot,filescold,inttime,framerate,pathparam,nmax,function_to_use = build_poly_coeff):
	# 08/10/2018 THIS CALCULATE FROM MULTIPLE HOT>ROOM AND COLD >ROOM CYCLES THE COEFFICIENTS FOR ALL THE POSSIBLE COMBINATIONS

	for i in range(len(temperaturehot)):
		temperaturehot[i]=flatten_full(temperaturehot[i])
	for i in range(len(temperaturecold)):
		temperaturecold[i]=flatten_full(temperaturecold[i])
	for i in range(len(fileshot)):
		fileshot[i]=flatten_full(fileshot[i])
	for i in range(len(filescold)):
		filescold[i]=flatten_full(filescold[i])

	for i in range(len(temperaturehot)):
		if len(temperaturehot[i])!=len(fileshot[i]):
			print('Error, temperaturehot'+str(i)+' and fileshot'+str(i)+' length is different')
			exit()
	for i in range(len(temperaturecold)):
		if len(temperaturecold[i])!=len(filescold[i]):
			print('Error, temperaturecold'+str(i)+' and filescold'+str(i)+' length is different')
			exit()

	lengthhot=len(temperaturehot)
	lengthcold=len(temperaturecold)

	# This lies must be placed outside of the function to make it work
	# fileshot=np.array([fileshot1,fileshot2])
	# temperaturehot=np.array([temperaturehot1,temperaturehot2])
	# filescold=np.array([filescold1,filescold2])
	# temperaturecold=np.array([temperaturecold1,temperaturecold2])

	# THIS COMPUTE THE 1-1, 1-2, 2-1, 2-2 PARAMETERS
	for i in range(lengthhot):
		for j in range(lengthcold):
			path=pathparam+'/'+str(inttime)+'ms'+str(framerate)+'Hz'+'/'+'numcoeff'+str(nmax)+'/'+str(i+1)+'-'+str(j+1)
			if not os.path.exists(path):
				os.makedirs(path)
			temperature=[temperaturehot[i],temperaturecold[j]]
			files=[fileshot[i],filescold[j]]
			function_to_use(temperature,files,inttime,path,nmax)

################################################################################################

def build_average_poly_coeff(temperaturehot,temperaturecold,fileshot,filescold,inttime,framerate,pathparam,nmax):
	# 08/10/2018 THIS MAKES THE AVERAGE OF COEFFICIENTS FROM MULTIPLE HOT>ROOM AND COLD >ROOM CYCLES THE COEFFICIENTS

	lengthhot=len(temperaturehot)
	lengthcold=len(temperaturecold)

	first=True
	for i in range(lengthhot):
		for j in range(lengthcold):
			path=pathparam+'/'+str(inttime)+'ms'+str(framerate)+'Hz'+'/'+'numcoeff'+str(nmax)+'/'+str(i+1)+'-'+str(j+1)
			params=np.load(os.path.join(path,'coeffpolydeg'+str(nmax)+'int'+str(inttime)+'ms'+'.npy'))
			if first==True:
				shape=np.shape(params)
				shape=np.concatenate(((lengthhot,lengthcold),shape))
				parameters=np.zeros(shape)
				first=False
			parameters[i,j]=params

	meanparameters=np.mean(parameters,axis=(0,1))
	stdparameters=np.std(parameters,axis=(0,1))

	path=pathparam+'/'+str(inttime)+'ms'+str(framerate)+'Hz'+'/'+'numcoeff'+str(nmax)+'/average'
	if not os.path.exists(path):
		os.makedirs(path)
	np.save(os.path.join(path,'coeffpolydeg'+str(nmax)+'int'+str(inttime)+'ms'),meanparameters)
	np.save(os.path.join(path,'errcoeffpolydeg'+str(nmax)+'int'+str(inttime)+'ms'),stdparameters)


#####################################################################################################

def average_frame(frame,pixelmean,extremedelete=False):

	# Does the average over pixelmean x pixelmean pixels of frame
	#
	# If the flag extremedelete is True for every mean the maximuna and minimum values are deleted
	#
	if pixelmean==1:
		return frame
	shapeorig=np.shape(frame)
	shapeaver=(np.divide(shapeorig,pixelmean)).astype(int)
	frameaver=np.zeros(shapeaver)
	for i in range(shapeaver[0]):
		if ((i+1)*pixelmean)>shapeorig[0]:
			indexi=shapeorig[0]-1
		else:
			indexi=(i+1)*pixelmean
		for j in range(shapeaver[1]):
			if ((j+1)*pixelmean)>shapeorig[1]:
				indexj=shapeorig[1]-1
			else:
				indexj=(j+1)*pixelmean
			flat=np.ravel(frame[i*pixelmean:int(indexi),j*pixelmean:int(indexj)])
			if extremedelete:
				if len(flat)>3:
					flat=np.delete(flat,np.argmax(flat))
					flat=np.delete(flat,np.argmin(flat))
			frameaver[i,j]=np.mean(flat)
			# print(i*pixelmean,indexi,j*pixelmean,indexj,flat)

	return frameaver

#####################################################################################################

def average_multiple_frames(frames,pixelmean,timemean=1,extremedelete=False):

	# Does the average over pixelmean x pixelmean pixels of multiple frames
	#
	# If the flag extremedelete is True for every mean the maximuna and minimum values are deleted
	#

	shapeorig=np.shape(frames)
	nframes=shapeorig[1]
	# framesaver=[]
	# framesaver.append([None] * nframes)
	# framesaver=np.array(framesaver)
	framesaver = [None]*nframes
	for i in range(nframes):
		framesaver[i]=average_frame(frames[0,i],pixelmean,extremedelete)

	if timemean>1:
		timemean = int(timemean)
		reduced_frames = int(nframes/timemean)
		# temp = []
		# temp.append([None] * reduced_frames)
		# temp = np.array(temp)
		temp=[None] * reduced_frames
		for index in range(reduced_frames):
			temp[index] = np.mean(framesaver[index*timemean:(index+1)*timemean],axis=(0))
		return np.array([temp])

	return np.array([framesaver])


#####################################################################################################

def average_multiple_frames2(frames,pixelmean,timemean=1):

	# Created to use the faster function skimage.transform.resize
	# Does the average over pixelmean x pixelmean pixels of multiple frames
	#
	import numpy as np
	from skimage.transform import resize

	shapeorig=np.shape(frames)
	nframes=shapeorig[1]
	shapeaver=(np.divide(shapeorig[-2:],pixelmean)).astype(int)
	# framesaver=[]
	# framesaver.append([None] * nframes)
	# framesaver=np.array(framesaver)
	framesaver = [None]*nframes
	for i in range(nframes):
		# framesaver[i]=average_frame(frames[0,i],pixelmean,extremedelete)
		framesaver[i] =resize(frames[0,i], shapeaver,order=1)
	framesaver = np.array(framesaver)

	if timemean>1:
		timemean = int(timemean)
		reduced_frames = int(nframes/timemean)
		# temp = []
		# temp.append([None] * reduced_frames)
		# temp = np.array(temp)
		temp=[None] * reduced_frames
		for index in range(reduced_frames):
			temp[index] = np.mean(framesaver[index*timemean:(index+1)*timemean],axis=(0))
		return np.array([temp])

	return np.array([framesaver])


#####################################################################################################

def flatten(array):

	# 11/06/2018
	# This function flatten any array of one level.
	# If it is already in one level it return is the way it is
	#
	# array=np.array(array)
	length=len(array)

	done=0
	for item in array:
		if np.shape(item)!=():
			done+=1
	if done==0:
		return array

	temp=[]
	lengthinside=np.zeros(length)
	for i in range(length):
		temp2=np.array(array[i])
		lengthinside=np.shape(temp2)
		if lengthinside==():
			temp.append(array[i])
		else:
			for j in range(lengthinside[0]):
				temp.append(temp2[j])
	try:
		temp=np.array(temp)
	except:
		print('partial flattening obtained')
	return temp

#####################################################################################################

def flatten_full(array):
	# this function flatten an array fully

	while np.shape(array[0])!=():
		array=flatten(array)

	return array

#####################################################################################################

def ddx(array,dx,axis,otheraxis=(),howcropotheraxis=0):

	# this function makes the central difference derivative on the axis AXIS. it reduces the size of the array in that direction by two pixels
	# at the same time it can reduce the size ao the array in other directions too of any number of pixels.

	if axis==0:
		temp=np.divide(array[2:]-array[:-2],2*dx)
	elif axis==1:
		temp=np.divide(array[:,2:]-array[:,:-2],2*dx)
	elif axis==2:
		temp=np.divide(array[:,:,2:]-array[:,:,:-2],2*dx)
	elif axis==3:
		temp=np.divide(array[:,:,:,2:]-array[:,:,:,:-2],2*dx)
	elif axis==4:
		temp=np.divide(array[:,:,:,:,2:]-array[:,:,:,:,:-2],2*dx)
	elif axis==5:
		temp=np.divide(array[:,:,:,:,:,2:]-array[:,:,:,:,:,:-2],2*dx)
	elif axis==6:
		temp=np.divide(array[:,:,:,:,:,:,2:]-array[:,:,:,:,:,:,:-2],2*dx)


	if howcropotheraxis==0:
		return temp

	if otheraxis==():
		print('if you specify the number of pixels to crop tou must specify too the axis where to do that')
		exit()

	numaxis=len(np.shape(array))
	for i in range(len(otheraxis)):
		if otheraxis[i]<0:
			otheraxis[i]=numaxis-otheraxis[i]

	if howcropotheraxis/2-howcropotheraxis//2!=0:
		print('the amount of pixels you want to crop the array must be even, to crop of half on one side and half on the other')
		exit()
	htc=howcropotheraxis//2
	if 0 in otheraxis:
		temp=temp[htc:-htc]
	if 1 in otheraxis:
		temp=temp[:,htc:-htc]
	if 2 in otheraxis:
		temp=temp[:,:,htc:-htc]
	if 3 in otheraxis:
		temp=temp[:,:,:,htc:-htc]
	if 4 in otheraxis:
		temp=temp[:,:,:,:,htc:-htc]
	if 5 in otheraxis:
		temp=temp[:,:,:,:,:,htc:-htc]
	if 6 in otheraxis:
		temp=temp[:,:,:,:,:,:,htc:-htc]

	return temp

#####################################################################################################

def d2dx2(array,dx,axis,otheraxis=(),howcropotheraxis=0):

	# this function makes the tentral difference second derivative on the axis AXIS. it reduces the size of the array in that direction by two pixels
	# at the same time it can reduce the size ao the array in other directions too of any number of pixels.

	if axis==0:
		temp=np.divide(array[2:]-np.multiply(2,array[1:-1])+array[:-2],dx**2)
	elif axis==1:
		temp=np.divide(array[:,2:]-np.multiply(2,array[:,1:-1])+array[:,:-2],dx**2)
	elif axis==2:
		temp=np.divide(array[:,:,2:]-np.multiply(2,array[:,:,1:-1])+array[:,:,:-2],dx**2)
	elif axis==3:
		temp=np.divide(array[:,:,:,2:]-np.multiply(2,array[:,:,:,1:-1])+array[:,:,:,:-2],dx**2)
	elif axis==4:
		temp=np.divide(array[:,:,:,:,2:]-np.multiply(2,array[:,:,:,:,1:-1])+array[:,:,:,:,:-2],dx**2)
	elif axis==5:
		temp=np.divide(array[:,:,:,:,:,2:]-np.multiply(2,array[:,:,:,:,:,1:-1])+array[:,:,:,:,:,:-2],dx**2)
	elif axis==6:
		temp=np.divide(array[:,:,:,:,:,:,2:]-np.multiply(2,array[:,:,:,:,:,:,1:-1])+array[:,:,:,:,:,:,:-2],dx**2)


	if howcropotheraxis==0:
		return temp
	if howcropotheraxis/2-howcropotheraxis//2!=0:
		print('the amount of pixels you want to crop the array must be even, to crop of half on one side and half on the other')
		exit()

	numaxis=len(np.shape(array))
	for i in range(len(otheraxis)):
		if otheraxis[i]<0:
			otheraxis[i]=numaxis-otheraxis[i]

	if howcropotheraxis/2-howcropotheraxis//2!=0:
		print('the amount of pixels you want to crop the array must be even, to crop of half on one side and half on the other')
		exit()
	htc=howcropotheraxis//2
	if 0 in otheraxis:
		temp=temp[htc:-htc]
	if 1 in otheraxis:
		temp=temp[:,htc:-htc]
	if 2 in otheraxis:
		temp=temp[:,:,htc:-htc]
	if 3 in otheraxis:
		temp=temp[:,:,:,htc:-htc]
	if 4 in otheraxis:
		temp=temp[:,:,:,:,htc:-htc]
	if 5 in otheraxis:
		temp=temp[:,:,:,:,:,htc:-htc]
	if 6 in otheraxis:
		temp=temp[:,:,:,:,:,:,htc:-htc]

	return temp


#####################################################################################################

def save_timestamp(extpath):

	# 09/08/2018 This function looks at the CSV files and saves the timestamp of the firs and last one in a _timestamp.npy file

	# path=os.getcwd()

	path=extpath
	print('path =', path)

	position=[]
	for i in range(len(path)):
		if path[i]=='/':
			position.append(i)
	position=max(position)
	lastpath=path[position+1:]
	# print('lastpath',lastpath)

	f = []
	for (dirpath, dirnames, filenames) in os.walk(path):
		f.extend(filenames)
	#	break

	filenames=f
	# print('filenames',filenames)

	filefits=[]
	temp=[]
	print('len(filenames)',len(filenames))
	for index in range(len(filenames)):
		# print(filenames[index])
		if filenames[index][-3:]=='csv':
			temp.append(filenames[index])
			# filenames=np.delete(filenames,index)
			# print('suca')
		# elif filenames[index][-3:]=='fts':
		# 	filefits.append(filenames[index])
	filenames=temp
	filenames=sorted(filenames, key=str.lower)

	timestamp=[]
	# file=0
	for filename in [filenames[0],filenames[-1]]:
		with open(os.path.join(path,filename),'r') as csvfile:
			reader = csv.reader(csvfile)
			# print('reader',reader)
			for row in reader:
				if not not row:
				# else:
					if row[0][0:4]=='Time':
						time=row[0][7:]
						ampm=int(time[0:3])
						hh=int(time[4:6])
						mm=int(time[7:9])
						ss=float(time[10:])
						timess=ss+60*(mm+60*(hh+ampm*12))
						timestamp.append(timess)



	timestamp.append(np.mean(timestamp))
	timestamp=np.array(timestamp)
	np.save(os.path.join(extpath,lastpath+'_timestamp'),timestamp)


####################################################################################################

def search_background_timestamp(extpath,ref_directory):

	# 09/08/2018 This function lools at the timestamp in "extpath" and compare it with all the timestamps in the directories indicated by "ref_directory"
	# then I pick the two that are closer and interpolate in between the two to get the proper background


	ref_directories=[]
	for (dirpath, dirnames, filenames) in os.walk(ref_directory):
		ref_directories.extend(dirnames)


	ref_directories=order_filenames(ref_directories)

	type="_timestamp.npy"
	time=[]
	for directory in ref_directories:
		filename=all_file_names(os.path.join(ref_directory,directory),type)[0]
		timestamp=np.load(os.path.join(ref_directory,directory,filename))
		time.append(timestamp[2])

	time=np.array(time)
	if np.sum(time!=np.sort(time))>0:
		print('Something is wrong, the order of the files does not follows a cronological order, as it should')
		exit()


	filename=all_file_names(os.path.join(extpath),type)[0]
	specific_time=np.load(os.path.join(extpath,filename))[2]
	specific_filename=filename[:-14]

	index=np.searchsorted(time,specific_time)-1

	print('found '+str(specific_time)+' between '+str(time[index])+' and '+str(time[index+1]))

	type="_stat.npy"
	filename=all_file_names(os.path.join(ref_directory,ref_directories[index]),type)[0]
	pre_ref=np.load(os.path.join(ref_directory,ref_directories[index],filename))[0]

	filename=all_file_names(os.path.join(ref_directory,ref_directories[index+1]),type)[0]
	post_ref=np.load(os.path.join(ref_directory,ref_directories[index+1],filename))[0]

	dt=time[index+1]-time[index]
	reference=np.multiply(pre_ref,(time[index+1]-specific_time)/dt)+np.multiply(post_ref,(specific_time-time[index])/dt)

	np.save(os.path.join(extpath,specific_filename+'_reference'),reference)


####################################################################################################

def find_nearest_index(array,value):

	# 14/08/2018 This function returns the index of the closer value to "value" inside an array
	# 08/01/2021 This function is completely reduntant, but I have it in so many places I keep it

	if False:
		array_shape=np.shape(array)
		index = np.abs(np.add(array,-value)).argmin()
		residual_index=index
		cycle=1
		done=0
		position_min=np.zeros(len(array_shape),dtype=int)
		while done!=1:
			length=array_shape[-cycle]
			if residual_index<length:
				position_min[-cycle]=residual_index
				done=1
			else:
				position_min[-cycle]=round(((residual_index/length) %1) *length +0.000000000000001)
				residual_index=residual_index//length
				cycle+=1
		return position_min[0]
	else:
		return np.abs(np.array(array)-value).argmin()


###################################################################################################

def track_change_from_baseframe(data,poscentred,basecounts):

	# Created 27/12/2018 to clean up some legacy data. Now this function is not necessay anymore
	# This function evaluate the average counts on a limited number of locations [poscentered] and in a small window around them [a].
	# Then this is compared with same locations and window for a reference frame, and the whole record will need be scaled of the difference [base_counts_correction]


	data_mean_difference = []
	data_mean_difference_std = []

	for pos in poscentred:
		for a in [5, 10]:
			temp1 = np.mean(data[0, :, pos[1] - a:pos[1] + 1 + a, pos[0] - a:pos[0] + 1 + a], axis=(-1, -2, -3))
			temp1std = np.std(np.mean(data[0, :, pos[1] - a:pos[1] + 1 + a, pos[0] - a:pos[0] + 1 + a], axis=(-1, -2)))
			temp2 = np.mean(basecounts[0, :, pos[1] - a:pos[1] + 1 + a, pos[0] - a:pos[0] + 1 + a], axis=(-1, -2, -3))
			temp2std = np.std(
				np.mean(basecounts[0, :, pos[1] - a:pos[1] + 1 + a, pos[0] - a:pos[0] + 1 + a], axis=(-1, -2)))
			data_mean_difference.append(temp1 - temp2)
			data_mean_difference_std.append(temp1std + temp2std)
	data_mean_difference = np.array(data_mean_difference)
	data_mean_difference_std = np.array(data_mean_difference_std)
	guess = [1]
	base_counts_correction, temp2 = curve_fit(costant,
											  np.linspace(1, len(data_mean_difference), len(data_mean_difference)),
											  data_mean_difference, sigma=data_mean_difference_std, p0=guess,
											  maxfev=100000000)
	print('background correction = ' + str(int(base_counts_correction * 1000) / 1000))

	return base_counts_correction







###################################################################################################

def clear_oscillation_central(data,framerate,oscillation_search_window_begin='auto',oscillation_search_window_end='auto',plot_conparison=False):

	# Created 01/11/2018
	# This function take the raw counts. analyse the fast fourier transform in a selectable interval IN THE CENTER OF THE FRAME.
	# Then search for the peak between 20Hz and 34Hz  and substract the oscillation found to the counts
	print('Use clear_oscillation_central2 instead of clear_oscillation_central')
	exit()


	print('shape of data array is '+str(np.shape(data))+', it should be (x,frames,v pixel,h pixel)')

	data=data[0]
	if oscillation_search_window_begin=='auto':	# in seconds
		force_start=0
	elif (oscillation_search_window_begin<0 or oscillation_search_window_begin>len(data)*framerate):
		print('The initial limit to search for the oscillation ad erase it is out of range (a time in seconds)')
		print('0s will be used instead')
		force_start=0
	else:
		force_start=int(oscillation_search_window_begin/framerate)

	if oscillation_search_window_end=='auto':	# in seconds
		force_end=len(data)
	elif (oscillation_search_window_end<0 or oscillation_search_window_end>(len(data)*framerate) or oscillation_search_window_end<=(force_start*framerate)):
		print('The final limit to search for the oscillation ad erase it is out of range (a time in seconds)')
		print(str(int(len(data)//(2*framerate)))+'s will be used instead')
		force_end=int(len(data)//(2*framerate))
	else:
		force_end=int(oscillation_search_window_end*framerate)

	window = 10
	datasection = data

	if plot_conparison==True:
		poscentred = [[15, 80], [80, 80], [70, 200], [160, 133], [250, 200]]

		spectra_orig = np.fft.fft(data, axis=0)
		# magnitude=np.sqrt(np.add(np.power(real,2),np.power(imag,2)))
		magnitude = 2 * np.abs(spectra_orig) / len(spectra_orig)
		phase = np.angle(spectra_orig)
		freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)

		color = ['m', 'c', 'y', 'b', 'r', 'k', 'g', 'm']
		for i in range(len(poscentred)):
			pos = poscentred[i]
			y = np.mean(magnitude[:, pos[0] - window:pos[0] + window, pos[1] - window:pos[1] + window], axis=(-1, -2))
			# y=magnitude[:,pos[0],pos[1]]
			y = np.array([y for _, y in sorted(zip(freq, y))])
			x = np.sort(freq)
			plt.plot(x, y, color[i], label='original data at the point ' + str(pos))
		# plt.title()

		plt.figure(1)
		plt.title('Amplitued from fast Fourier transform for different groups of ' + str(window * 2) + 'x' + str(
			window * 2) + ' pixels, framerate ' + str(framerate)+'Hz' )
		plt.xlabel('Frequency [Hz]')
		plt.ylabel('Amplitude [au]')
		plt.grid()
		plt.semilogy()
		plt.legend()
	# plt.show()


	sections = 31  # number found with practice, no specific mathematical reasons
	max_time = 5  # seconds of record that I can use to filter the signal. I assume to start from zero
	poscentre = [np.shape(data)[1] // 2, np.shape(data)[2] // 2]
	record_magnitude = []
	record_phase = []
	record_freq = []
	peak_freq_record = []
	peak_value_record = []
	section_frames_record = []

	# I restrict the window over which I search for the oscillation
	datarestricted=data[force_start:force_end]

	if oscillation_search_window_end == 'auto':
		if (len(datarestricted) / framerate) <= 1:
			max_start = int(sections // 2)
		else:
			max_start = min(int(1 + sections / 2), int(
				max_time * framerate / (len(datarestricted) / sections)))  # I can use only a part of the record to filter the signal
	else:
		max_start = int(oscillation_search_window_end * framerate / (len(datarestricted) / sections))

	if (len(datarestricted) / framerate) <= 1:
		min_start = max(1, int(0.2 * framerate / (len(datarestricted) / sections)))
	else:
		extra = 0
		while ((max_start - int(max_start / (5 / 2.5)) + extra) < 7):	# 7 is just a try. this is in a way to have enough fitting to compare
			extra+=1
		min_start = max(1,int(max_start / (5 / 2.5)) - extra)
		# min_start = max(1, int(max_start / (5 / 2.5)) )  # with too little intervals it can interpret noise for signal

	for i in range(min_start, max_start):
		section_frames = (i) * (len(datarestricted) // sections)
		section_frames_record.append(section_frames)
		datasection = datarestricted[0:section_frames, poscentre[0] - window:poscentre[0] + window, poscentre[1] - window:poscentre[1] + window]
		spectra = np.fft.fft(datasection, axis=0)
		magnitude = 2 * np.abs(spectra) / len(spectra)
		record_magnitude.append(magnitude[0:len(magnitude) // 2])
		phase = np.angle(spectra)
		record_phase.append(phase[0:len(magnitude) // 2])
		freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)
		record_freq.append(freq[0:len(magnitude) // 2])
		magnitude_space_averaged = np.mean(magnitude, axis=(-1, -2))
		y = np.array(
			[magnitude_space_averaged for _, magnitude_space_averaged in sorted(zip(freq, magnitude_space_averaged))])
		x = np.sort(freq)
		if plot_conparison == True:
			plt.figure(2)
			plt.plot(x, y, label='size of the analysed window ' + str(section_frames / framerate))
		index_20 = int(find_nearest_index(x, 20))  # I restric the window over which I do the peak search
		index_34 = int(find_nearest_index(x, 34))
		index_7 = int(find_nearest_index(x, 7))
		index_n7 = int(find_nearest_index(x, -7))
		index_n20 = int(find_nearest_index(x, -20))
		index_n34 = int(find_nearest_index(x, -34))
		index_0 = int(find_nearest_index(x, 0))
		noise = np.mean(np.array(
			y[3:index_n34].tolist() + y[index_n20:index_n7].tolist() + y[index_7:index_20].tolist() + y[
																									  index_34:-3].tolist()),
						axis=(-1))
		temp = peakutils.indexes(y[index_20:index_34], thres=noise + np.abs(magnitude.min()),
								 min_dist=(index_34 - index_20) // 2)
		if len(temp) == 1:
			peak_index = index_20 + int(temp)
			peak_freq_record.append(x[peak_index])
			peak_value = float(y[peak_index])
			peak_value_record.append(peak_value)
	record_magnitude = np.array(record_magnitude)
	record_phase = np.array(record_phase)
	record_freq = np.array(record_freq)
	peak_freq_record = np.array(peak_freq_record)
	peak_value_record = np.array(peak_value_record)
	section_frames_record = np.array(section_frames_record)
	if plot_conparison==True:
		plt.figure(2)
		plt.title('Amplitued from fast Fourier transform averaged in a wondow of ' + str(window) + 'pixels around ' + str(
			poscentre) + ', framerate ' + str(framerate) + 'Hz')
		plt.xlabel('Frequency [Hz]')
		plt.ylabel('Amplitude [au]')
		plt.grid()
		plt.semilogy()
		plt.legend()


	# I find the highest peak and that will be the one I use
	index = int(find_nearest_index(peak_value_record, max(peak_value_record)+1))
	section_frames = section_frames_record[index]
	datasection = datarestricted[0:section_frames]
	spectra = np.fft.fft(datasection, axis=0)
	# magnitude=np.sqrt(np.add(np.power(real,2),np.power(imag,2)))
	magnitude = 2 * np.abs(spectra) / len(spectra)
	phase = np.angle(spectra)
	freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)
	freq_to_erase = peak_freq_record[index]
	freq_to_erase_index = int(find_nearest_index(freq, freq_to_erase))
	framenumber = np.linspace(0, len(data) - 1, len(data)) - force_start
	data2 = data - np.multiply(magnitude[freq_to_erase_index], np.cos(np.repeat(np.expand_dims(phase[freq_to_erase_index], axis=0), len(data), axis=0) + np.repeat(np.expand_dims(np.repeat(np.expand_dims(2 * np.pi * freq_to_erase * framenumber / framerate, axis=-1),np.shape(data)[1], axis=-1), axis=-1), np.shape(data)[2], axis=-1)))

	if plot_conparison==True:
		plt.figure(1)
		datasection2 = data2
		spectra = np.fft.fft(datasection2, axis=0)
		# magnitude=np.sqrt(np.add(np.power(real,2),np.power(imag,2)))
		magnitude2 = 2 * np.abs(spectra) / len(spectra)
		phase2 = np.angle(spectra)
		freq = np.fft.fftfreq(len(magnitude2), d=1 / framerate)
		for i in range(len(poscentred)):
			pos = poscentred[i]
			y = np.mean(magnitude2[:, pos[0] - window:pos[0] + window, pos[1] - window:pos[1] + window], axis=(-1, -2))
			# y=magnitude[:,pos[0],pos[1]]
			y = np.array([y for _, y in sorted(zip(freq, y))])
			x = np.sort(freq)
			plt.plot(x, y, color[i] + '--',
					 label='data at the point ' + str(pos) + ', ' + str(freq_to_erase) + 'Hz oscillation substracted')
		# plt.title()


		plt.grid()
		plt.semilogy()
		plt.legend()
		plt.pause(0.0001)




	print('stats of the oscillation removal')
	print('with window of size '+str(section_frames_record[index]/framerate)+'s of '+str(len(data)/framerate)+'s of record')
	print('found oscillation of frequency '+str(freq_to_erase)+'Hz')

	return np.array([data2])


###################################################################################################

def clear_oscillation_central2(data,framerate,oscillation_search_window_begin='auto',oscillation_search_window_end='auto',min_frequency_to_erase=20,max_frequency_to_erase=34,plot_conparison=False,which_plot=[1,2,3],ROI='auto',window=2,force_poscentre='auto',output_noise=False,multiple_frequencies_cleaned=1):

	# Created 15/02/2019
	# This function take the raw counts. analyse the fast fourier transform in a selectable interval IN THE CENTER OF THE FRAME.
	# Then search for the peak between 20Hz and 34Hz  and substract the oscillation found to the counts
	# The difference from clear_oscillation_central is that instead of splitting the interval of interest in smaller chunks and analyse them I analyse the full window available and shift the first and last point.
	# This aproach seems much more efficient

	print('shape of data array is '+str(np.shape(data))+', it should be (x,frames,v pixel,h pixel)')
	# figure_index = plt.gcf().number

	data=data[0]
	if oscillation_search_window_begin=='auto':
		force_start=0
	elif (oscillation_search_window_begin<0 or oscillation_search_window_begin>len(data)*framerate):
		print('The initial limit to search for the oscillation ad erase it is out of range (a time in seconds)')
		print('0s will be used instead')
		force_start=0
	else:
		force_start=int(oscillation_search_window_begin*framerate)

	if oscillation_search_window_end=='auto':
		force_end=len(data)
	elif (oscillation_search_window_end<0 or oscillation_search_window_end>(len(data)/framerate) or oscillation_search_window_end<=force_start/framerate):
		print('The final limit to search for the oscillation ad erase it is out of range (a time in seconds)')
		print(str(int(len(data)//(2*framerate)))+'s will be used instead')
		force_end=int(len(data)//(2*framerate))
	else:
		force_end=int(oscillation_search_window_end*framerate)


	central_freq_for_search = (max_frequency_to_erase-min_frequency_to_erase)/2+min_frequency_to_erase
	if (framerate<2*central_freq_for_search):
		print('There is a problem. The framerate is too low to try to extract the oscillation')
		print('The minimum framrate for doing it is 2*oscillation frequency to detect. Therefore '+str(np.around(2*central_freq_for_search,decimals=1))+'Hz, in this case.')
		print('See http://www.skillbank.co.uk/SignalConversion/rate.htm')
		exit()

	#window = 2	# Previously found that as long as the fft is averaged over at least 4 pixels the peak shape and location does not change
	datasection = data

	if plot_conparison==True:
		# plt.figure()
		# plt.pause(0.01)
		plt.figure()
		figure_index = plt.gcf().number
		if 1 in which_plot:
			data_shape = np.shape(data)
			poscentred = [[int(data_shape[1]*1/5), int(data_shape[2]*1/5)], [int(data_shape[1]*1/2), int(data_shape[2]*1/5)], [int(data_shape[1]*4/5), int(data_shape[2]*1/5)], [int(data_shape[1]*4/5), int(data_shape[2]*1/2)], [int(data_shape[1]*4/5), int(data_shape[2]*4/5)], [int(data_shape[1]*1/2), int(data_shape[2]*1/2)]]

			# spectra_orig = np.fft.fft(data, axis=0)
			# magnitude=np.sqrt(np.add(np.power(real,2),np.power(imag,2)))
			# magnitude = 2 * np.abs(spectra_orig) / len(spectra_orig)
			# phase = np.angle(spectra_orig)
			# freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)

			color = ['m', 'c', 'y', 'b', 'r', 'k', 'g', 'm']
			for i in range(len(poscentred)):
				pos = poscentred[i]
				spectra_orig = np.fft.fft(np.mean(data[:, pos[0] - window:pos[0] + window+1, pos[1] - window:pos[1] + window+1],axis=(-1,-2)), axis=0)
				magnitude = 2 * np.abs(spectra_orig) / len(spectra_orig)
				freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)
				# y = np.mean(magnitude, axis=(-1, -2))
				y = magnitude
				# y=magnitude[:,pos[0],pos[1]]
				y = np.array([y for _, y in sorted(zip(freq, y))])
				x = np.sort(freq)
				plt.plot(x, y*100, color[i], label='original data at the point ' + str(pos) + ' x100')
			# plt.title()


			plt.title('Amplitued from fast Fourier transform in the whole time interval\nfor different groups of ' + str(window * 2+1) + 'x' + str(
				window * 2+1) + ' pixels, framerate %.3gHz' %(framerate) )
			plt.xlabel('Frequency [Hz]')
			plt.ylabel('Amplitude [au]')
			plt.grid()
			plt.semilogy()
			plt.legend(loc='best',fontsize='xx-small')
		else:
			plt.close(figure_index)
	# plt.show()



	frames_for_oscillation = framerate//central_freq_for_search

	number_of_waves = 3
	fft_window_move = int(number_of_waves*frames_for_oscillation)
	if fft_window_move<10:
		number_of_waves=10//frames_for_oscillation	# I want to scan for at least 10 frame shifts
	fft_window_move = int(number_of_waves*frames_for_oscillation)
	step = 1
	while int(fft_window_move/step)>80:
		step+=1		#if framerate is too high i will skip some of the shifts to limit the number of Fourier transforms to 100


	# I restrict the window over which I search for the oscillation
	datarestricted = data[force_start:force_end]#
	len_data_restricted = len(datarestricted)
	if force_poscentre == 'auto':
		poscentre = [np.shape(data)[1] // 2, np.shape(data)[2] // 2]
	else:
		poscentre = force_poscentre

	if (oscillation_search_window_begin=='auto' and oscillation_search_window_end=='auto'):
		while fft_window_move>(len_data_restricted/5):
			fft_window_move-=1		# I want that the majority of the data I analyse remains the same

	if oscillation_search_window_end == 'auto':
		if (len(datarestricted) / framerate) <= 1:
			# max_start = int(sections // 2)
			max_start = int(len(datarestricted) // 2)
		else:
			max_start = int(5*framerate)  # I use 5 seconds of record
	else:
		max_start = len(datarestricted)	# this is actually ineffective, as datarestricted is already limited to force_start:force_end

	if oscillation_search_window_begin == 'auto':
		min_start = 0
	else:
		# min_start = force_start	# alreay enforced through force_start
		min_start = 0

	section_frames = max_start - min_start-fft_window_move

	if ROI=='auto':
		# datarestricted2 = np.mean(datarestricted[:, poscentre[0] -window :poscentre[0] + window+1, poscentre[1] - window:poscentre[1] + window+1],axis=(1,2))
		datarestricted2 = np.mean(datarestricted[:, poscentre[0] -window :poscentre[0] + window+1, poscentre[1] - window:poscentre[1] + window+1],axis=(-1,-2))
	else:
		horizontal_coord = np.arange(np.shape(datarestricted)[2])
		vertical_coord = np.arange(np.shape(datarestricted)[1])
		horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
		select = np.logical_or(np.logical_or(vertical_coord<ROI[0],vertical_coord>ROI[1]),np.logical_or(horizontal_coord<ROI[2],horizontal_coord>ROI[3]))
		datarestricted2 = np.mean(datarestricted[:, select],axis=(-1))

	record_magnitude = []
	record_phase = []
	record_freq = []
	peak_freq_record = []
	peak_value_record = []
	peak_index_record = []
	shift_record = []

	for i in range(int(fft_window_move/step)):
		shift=i*step
		datasection = datarestricted2[min_start:max_start-fft_window_move+shift]
		spectra = np.fft.fft(datasection, axis=0)
		magnitude = 2 * np.abs(spectra) / len(spectra)
		record_magnitude.append(magnitude[0:len(magnitude) // 2])
		phase = np.angle(spectra)
		record_phase.append(phase[0:len(magnitude) // 2])
		freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)
		record_freq.append(freq[0:len(magnitude) // 2])
		# magnitude_space_averaged = np.mean(magnitude, axis=(-1, -2))
		magnitude_space_averaged = cp.deepcopy(magnitude)
		y = np.array([magnitude_space_averaged for _, magnitude_space_averaged in sorted(zip(freq, magnitude_space_averaged))])
		x = np.sort(freq)

		index_min_freq = int(find_nearest_index(x, min_frequency_to_erase))  # I restric the window over which I do the peak search
		index_max_freq = int(find_nearest_index(x, max_frequency_to_erase))
		index_7 = int(find_nearest_index(x, 7))
		index_n7 = int(find_nearest_index(x, -7))
		index_min_freq_n = int(find_nearest_index(x, -min_frequency_to_erase))
		index_max_freq_n = int(find_nearest_index(x, -max_frequency_to_erase))
		index_0 = int(find_nearest_index(x, 0))
		# noise = np.mean(np.array(
		# 	y[3:index_max_freq_n].tolist() + y[index_min_freq_n:index_n7].tolist() + y[index_7:index_min_freq].tolist() + y[
		# 																									index_max_freq:-3].tolist()),
		# 				axis=(-1))
		# temp = peakutils.indexes(y[index_min_freq:index_max_freq], thres=noise + np.abs(magnitude.min()),
		# 						 min_dist=(index_max_freq - index_min_freq) // 2)
		if len(y[index_min_freq:index_max_freq])==0:
			continue
		if plot_conparison == True and (2 in which_plot):
			plt.figure(figure_index+1)
			plt.plot(x, y, label='Applied shift of ' + str(shift))
		temp = int(find_nearest_index(y[index_min_freq:index_max_freq],(y[index_min_freq:index_max_freq]).max()))
		# if len(temp) == 1:
		peak_index = index_min_freq + int(temp)
		peak_freq_record.append(x[peak_index])
		peak_value = float(y[peak_index])
		peak_value_record.append(peak_value)
		peak_index_record.append(peak_index)
		shift_record.append(shift)
	record_magnitude = np.array(record_magnitude)
	record_phase = np.array(record_phase)
	record_freq = np.array(record_freq)
	peak_freq_record = np.array(peak_freq_record)
	peak_value_record = np.array(peak_value_record)
	peak_index_record = np.array(peak_index_record)
	shift_record = np.array(shift_record)
	if plot_conparison==True and (2 in which_plot):
		plt.figure(figure_index+1)
		# # plt.title('Amplitude from fast Fourier transform from %.3gs to %.3gs\naveraged in a window of ' %(force_start/framerate,(force_start+max_start)/framerate)+ str(window*2+1) + ' pixels around ' + str(
		# # 	poscentre) + ', framerate %.3gHz' %(framerate))
		# if ROI=='auto':
		# 	plt.title('Amplitude from fast Fourier transform from %.3gs to %.3gs\naveraged in a window of ' %(force_start/framerate,(force_start+max_start)/framerate)+ str([window * 2+1,window * 2+1]) + ' pixels around ' + str(poscentre) + ', framerate %.3gHz' %(framerate)+'\n'+str(multiple_frequencies_cleaned)+' frequencies around ')
		# else:
		# 	plt.title('Amplitude from fast Fourier transform from %.3gs to %.3gs\naveraged ouside the ROI ' %(force_start/framerate,(force_start+max_start)/framerate)+ str(ROI) + ' pixels around ' + str(ROI) + ', framerate %.3gHz' %(framerate))
		plt.xlabel('Frequency [Hz]')
		plt.ylabel('Amplitude [au]')
		plt.grid()
		plt.semilogy()
		plt.xlim(left=min_frequency_to_erase*0.8,right=max_frequency_to_erase*1.2)
		plt.ylim(bottom=np.median(np.sort(y)[:int(len(y)/4)])*1e-1)
		plt.legend(loc='best',fontsize='xx-small')


	if False:	# this method fails if the oscillation is actually composed of 2 separate oscillations very close to each other. it tries to remove a frequency in between that does not cause any problem. better to just not do it
		try:	# considering that the variation of the peak value of FFT is like a wave this methid seems more fair.
			shift_peaks = shift_record[1:-1][np.logical_and((peak_value_record[1:-1]-peak_value_record[:-2])>=0,(peak_value_record[1:-1]-peak_value_record[2:])>=0)]
			shift_through = shift_record[1:-1][np.logical_and((peak_value_record[1:-1]-peak_value_record[:-2])<=0,(peak_value_record[1:-1]-peak_value_record[2:])<=0)]
			while np.diff(shift_peaks).min()<=2:	# to avoid detection of adiacent "fake" peak or through
				target = np.diff(shift_peaks).argmin()
				shift_through = shift_through[np.logical_not(np.logical_and(shift_through>shift_peaks[target],shift_through<shift_peaks[target+1]))]
				shift_peaks = np.array(shift_peaks[:target].tolist() + [np.mean(shift_peaks[target:target+2])] + shift_peaks[target+2:].tolist())
			while np.diff(shift_through).min()<=2:	# to avoid detection of adiacent "fake" peak or through
				target = np.diff(shift_through).argmin()
				shift_peaks = shift_peaks[np.logical_not(np.logical_and(shift_peaks>shift_through[target],shift_peaks<shift_through[target+1]))]
				shift_through = np.array(shift_through[:target].tolist() + [np.mean(shift_through[target:target+2])] + shift_through[target+2:].tolist())
			shift_roots = np.sort(np.append(shift_peaks,shift_through,0))
			fit = np.polyfit(np.arange(len(shift_roots)),shift_roots,1)
			shift = int(round(np.polyval(fit,[0,1])[np.abs(np.polyval(fit,[0,1])-shift_peaks[0]).argmin()]))
			if peak_value_record[0]>peak_value_record[np.abs(shift_record-shift).argmin()]:
				shift = shift_record[0]
			index = shift//step
			gaussian = lambda x,A,sig,x0,q : A*np.exp(-0.5*(((x-x0)/sig)**2)) + q
			bds = [[0,0,peak_freq_record.min(),-np.inf],[np.inf,np.inf,peak_freq_record.max(),peak_value_record.min()]]
			guess = [peak_value_record.max()-peak_value_record.min(),0.3,peak_freq_record[peak_value_record.argmax()],peak_value_record.min()]
			fit = curve_fit(gaussian, peak_freq_record,peak_value_record, p0=guess, bounds = bds, maxfev=100000000)
			freq_to_erase = fit[0][2]
			if plot_conparison==True:
				if 2 in which_plot:
					plt.figure(figure_index+1)
					plt.plot(np.linspace(peak_freq_record.min(),peak_freq_record.max()),gaussian(np.linspace(peak_freq_record.min(),peak_freq_record.max()),*fit[0]),':k',label='fit')
				if 3 in which_plot:
					plt.figure(figure_index+2)
					plt.plot(shift_record,peak_value_record)
					plt.plot([shift]*2,[peak_value_record.min(),peak_value_record.max()],'--k')
					# plt.title('Amplitude from fast Fourier transform based on window shift\naveraged in a wondow of ' + str(window+1) + ' pixels around ' + str(
					# 	poscentre) + ', framerate %.3gHz' %(framerate))
					if ROI=='auto':
						plt.title('Amplitude from fast Fourier transform based on window shift\naveraged in a window of ' + str([window * 2+1,window * 2+1]) + ' pixels around ' + str(poscentre) + ', framerate %.3gHz' %(framerate))
					else:
						plt.title('Amplitude from fast Fourier transform based on window shift\naveraged ouside the ROI ' + str(ROI) + ' pixels around ' + str(ROI) + ', framerate %.3gHz' %(framerate))
					plt.xlabel('Shift [au]')
					plt.ylabel('Amplitude [au]')
					plt.grid()
					plt.grid()
					plt.semilogy()
			# fit = np.polyfit(peak_freq_record,peak_value_record,2)
			# freq_to_erase = -fit[1]/(fit[0]*2)
		except:	# I find the highest peak and that will be the one I use
			print('search of the best interval shift via the linear peak method failed')
			index = int(find_nearest_index(peak_value_record, max(peak_value_record)+1))
			shift = index * step
			freq_to_erase = peak_freq_record[index]
	else:
		index = int(find_nearest_index(peak_value_record, max(peak_value_record)+1))
		shift = index * step
		freq_to_erase = peak_freq_record[index]
	datasection = datarestricted[min_start:max_start-fft_window_move+shift]
	spectra = np.fft.fft(datasection, axis=0)
	# magnitude=np.sqrt(np.add(np.power(real,2),np.power(imag,2)))
	magnitude = 2 * np.abs(spectra) / len(spectra)
	phase = np.angle(spectra)
	freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)
	freq_to_erase_index = int(find_nearest_index(freq, freq_to_erase))
	freq_to_erase_index_multiple = np.arange(-(multiple_frequencies_cleaned-1)//2,(multiple_frequencies_cleaned-1)//2+1) + freq_to_erase_index

	# 2022/09/05 safety check: I don't wanto to remove real signal, therefore I limit the correction to something realistic
	magnitude[magnitude>2*np.median(magnitude[freq_to_erase_index])] = 2*np.median(magnitude[freq_to_erase_index])

	if plot_conparison==True and (2 in which_plot):
		plt.figure(figure_index+1)
		# plt.title('Amplitude from fast Fourier transform from %.3gs to %.3gs\naveraged in a window of ' %(force_start/framerate,(force_start+max_start)/framerate)+ str(window*2+1) + ' pixels around ' + str(
		# 	poscentre) + ', framerate %.3gHz' %(framerate))
		if ROI=='auto':
			plt.title('Amplitude from fast Fourier transform from %.3gs to %.3gs\naveraged in a window of ' %(force_start/framerate,(force_start+max_start)/framerate)+ str([window * 2+1,window * 2+1]) + ' pixels around ' + str(poscentre) + ', framerate %.3gHz' %(framerate)+'\n'+str(multiple_frequencies_cleaned)+' frequencies around %.5gHz erased' %(freq[freq_to_erase_index]))
		else:
			plt.title('Amplitude from fast Fourier transform from %.3gs to %.3gs\naveraged ouside the ROI ' %(force_start/framerate,(force_start+max_start)/framerate)+ str(ROI) + ' pixels around ' + str(ROI) + ', framerate %.3gHz' %(framerate)+'\n'+str(multiple_frequencies_cleaned)+' frequencies around %.5gHz erased' %(freq[freq_to_erase_index]))

	if plot_conparison==True and (2 in which_plot):
		plt.figure(figure_index+1)
		# plt.plot([freq_to_erase]*2,[peak_value_record.min(),peak_value_record.max()],':k')
		# plt.plot([freq[freq_to_erase_index]]*2,[peak_value_record.min(),peak_value_record.max()],'--k')
		plt.axvline(x=freq_to_erase,color='k',linestyle=':')
		plt.axvline(x=freq[freq_to_erase_index],color='k',linestyle='--')
		if len(freq_to_erase_index_multiple)>1:
			for i_freq in freq_to_erase_index_multiple:
				if i_freq!=freq_to_erase_index:
					plt.axvline(x=freq[i_freq],color='r',linestyle='--')
		plt.ylim(top=peak_value_record.max()*2)
	framenumber = np.linspace(0, len(data) - 1, len(data)) -force_start- min_start
	data2 = cp.deepcopy(data)
	for i_freq in freq_to_erase_index_multiple:
		# print(i_freq)
		data2 -= np.multiply(magnitude[i_freq], np.cos(np.repeat(np.expand_dims(phase[i_freq], axis=0), len(data), axis=0) + np.repeat(np.expand_dims(np.repeat(np.expand_dims(2 * np.pi * freq[i_freq] * framenumber / framerate, axis=-1),np.shape(data)[1], axis=-1), axis=-1), np.shape(data)[2], axis=-1)))
	# data2 = data - np.multiply(magnitude[freq_to_erase_index], np.cos(np.repeat(np.expand_dims(phase[freq_to_erase_index], axis=0), len(data), axis=0) + np.repeat(np.expand_dims(np.repeat(np.expand_dims(2 * np.pi * freq_to_erase * framenumber / framerate, axis=-1),np.shape(data)[1], axis=-1), axis=-1), np.shape(data)[2], axis=-1)))


	# added to visualize the goodness of the result
	if ROI=='auto':
		datasection = np.mean(data2[force_start:force_end][min_start:max_start-fft_window_move+shift][:, poscentre[0] -window :poscentre[0] + window+1, poscentre[1] - window:poscentre[1] + window+1],axis=(-1,-2))
	else:
		horizontal_coord = np.arange(np.shape(datarestricted)[2])
		vertical_coord = np.arange(np.shape(datarestricted)[1])
		horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
		select = np.logical_or(np.logical_or(vertical_coord<ROI[0],vertical_coord>ROI[1]),np.logical_or(horizontal_coord<ROI[2],horizontal_coord>ROI[3]))
		datasection = np.mean(data2[force_start:force_end][min_start:max_start-fft_window_move+shift][:, select],axis=(-1))
	# datasection = data2[force_start:force_end][min_start:max_start-fft_window_move+shift, poscentre[0] -1 :poscentre[0] + window+1, poscentre[1] - 1:poscentre[1] + window+1]
	spectra = np.fft.fft(datasection, axis=0)
	magnitude_space_averaged = 2 * np.abs(spectra) / len(spectra)
	freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)
	# magnitude_space_averaged = np.mean(magnitude, axis=(-1, -2))
	y = np.array([magnitude_space_averaged for _, magnitude_space_averaged in sorted(zip(freq, magnitude_space_averaged))])
	x = np.sort(freq)
	if plot_conparison==True and (2 in which_plot):
		plt.figure(figure_index+1)
		plt.plot(x, y,'--k',label='subtracted')
		# plt.grid()


	# section only for stats
	# datasection = data[:, poscentre[0] - 1:poscentre[0] + window, poscentre[1] - 1:poscentre[1] + window]
	if ROI=='auto':
		datasection = np.mean(data[:, poscentre[0] -window :poscentre[0] + window+1, poscentre[1] - window:poscentre[1] + window+1],axis=(-1,-2))
	else:
		horizontal_coord = np.arange(np.shape(datarestricted)[2])
		vertical_coord = np.arange(np.shape(datarestricted)[1])
		horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
		select = np.logical_or(np.logical_or(vertical_coord<ROI[0],vertical_coord>ROI[1]),np.logical_or(horizontal_coord<ROI[2],horizontal_coord>ROI[3]))
		datasection = np.mean(data[:, select],axis=(-1))
	spectra = np.fft.fft(datasection, axis=0)
	magnitude_space_averaged = 2 * np.abs(spectra) / len(spectra)
	phase = np.angle(spectra)
	freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)
	# magnitude_space_averaged = np.mean(magnitude, axis=(-1, -2))
	y = np.array([magnitude_space_averaged for _, magnitude_space_averaged in sorted(zip(freq, magnitude_space_averaged))])
	x = np.sort(freq)
	index_min_freq = int(find_nearest_index(x, min_frequency_to_erase))  # I restric the window over which I do the peak search
	index_max_freq = int(find_nearest_index(x, max_frequency_to_erase))
	index_7 = int(find_nearest_index(x, 7))
	index_n7 = int(find_nearest_index(x, -7))
	index_min_freq_n = int(find_nearest_index(x, -min_frequency_to_erase))
	index_max_freq_n = int(find_nearest_index(x, -max_frequency_to_erase))
	index_0 = int(find_nearest_index(x, 0))
	noise = (np.array(y[3:index_max_freq_n].tolist() + y[index_min_freq_n:index_n7].tolist() + y[index_7:index_min_freq].tolist() + y[index_max_freq:-3].tolist()))
	# temp = int(find_nearest_index(y[index_min_freq:index_max_freq], (y[index_min_freq:index_max_freq]).max()))
	# peak_index = index_min_freq + int(temp)
	peak_index = index_min_freq + y[index_min_freq:index_max_freq].argmax()
	peak_value_pre_filter = float(y[peak_index])

	# datasection = data2[:, poscentre[0] - 1:poscentre[0] + window, poscentre[1] - 1:poscentre[1] + window]
	if ROI=='auto':
		datasection = np.mean(data2[:, poscentre[0] -window :poscentre[0] + window+1, poscentre[1] - window:poscentre[1] + window+1],axis=(-1,-2))
	else:
		horizontal_coord = np.arange(np.shape(datarestricted)[2])
		vertical_coord = np.arange(np.shape(datarestricted)[1])
		horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
		select = np.logical_or(np.logical_or(vertical_coord<ROI[0],vertical_coord>ROI[1]),np.logical_or(horizontal_coord<ROI[2],horizontal_coord>ROI[3]))
		datasection = np.mean(data2[:, select],axis=(-1))
	spectra = np.fft.fft(datasection, axis=0)
	magnitude_space_averaged = 2 * np.abs(spectra) / len(spectra)
	phase = np.angle(spectra)
	freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)
	# magnitude_space_averaged = np.mean(magnitude, axis=(-1, -2))
	y = np.array([magnitude_space_averaged for _, magnitude_space_averaged in sorted(zip(freq, magnitude_space_averaged))])
	x = np.sort(freq)
	# temp = int(find_nearest_index(y[index_min_freq:index_max_freq], (y[index_min_freq:index_max_freq]).max()))
	# peak_index = index_min_freq + int(temp)
	peak_index = index_min_freq + y[index_min_freq:index_max_freq].argmax()
	peak_value_post_filter = float(y[peak_index])
	if plot_conparison==True and (2 in which_plot):
		plt.figure(figure_index+1)
		plt.axhline(y=np.max(noise),linestyle='--',color='k',label='max noise')
		plt.axhline(y=np.median(noise),linestyle='--',color='b',label='median noise')
		plt.axhline(y=peak_value_pre_filter,linestyle='--',color='r',label='peak pre')
		plt.axhline(y=peak_value_post_filter,linestyle='--',color='g',label='peak post')
		plt.legend(loc='best',fontsize='xx-small')

	if plot_conparison==True:
		if 1 in which_plot:
			plt.figure(figure_index)
			datasection2 = data2
			# spectra = np.fft.fft(datasection2, axis=0)
			# # magnitude=np.sqrt(np.add(np.power(real,2),np.power(imag,2)))
			# magnitude2 = 2 * np.abs(spectra) / len(spectra)
			# phase2 = np.angle(spectra)
			# freq = np.fft.fftfreq(len(magnitude2), d=1 / framerate)
			for i in range(len(poscentred)):
				pos = poscentred[i]
				spectra = np.fft.fft(np.mean(datasection2[:, pos[0] - window:pos[0] + window, pos[1] - window:pos[1] + window],axis=(-1,-2)), axis=0)
				magnitude2 = 2 * np.abs(spectra) / len(spectra)
				freq = np.fft.fftfreq(len(magnitude2), d=1 / framerate)
				# y = np.mean(magnitude2, axis=(-1, -2))
				y = magnitude2
				# y=magnitude[:,pos[0],pos[1]]
				y = np.array([y for _, y in sorted(zip(freq, y))])
				x = np.sort(freq)
				plt.plot(x, y, color[i] + '--',label='data at the point ' + str(pos) + ', ' + str(np.around(freq_to_erase,decimals=2)) + 'Hz oscillation substracted')
				plt.axvline(x=freq_to_erase,color='k',linestyle=':')
				plt.axvline(x=freq[np.abs(freq-freq_to_erase).argmin()],color='k',linestyle='--')
			# plt.title()


			# plt.grid()
			plt.semilogy()
			plt.xlim(left=0)
			plt.ylim(top=y[np.logical_and(x>min_frequency_to_erase,x<max_frequency_to_erase)].max()*2e3)
			plt.legend(loc='best',fontsize='xx-small')
		plt.pause(0.0001)




	print('stats of the oscillation removal')
	print('with window of size '+str(np.around(section_frames/framerate,decimals=5))+'s of '+str(len(data)/framerate)+'s of record')
	print('found oscillation of frequency '+str(freq_to_erase)+'Hz')
	print('On the ROI oscillation magnitude reduced from %.5g[au] to %.5g[au]' %(peak_value_pre_filter,peak_value_post_filter)+'\nwith an approximate maximum noise of %.5g[au] and median of %.5g[au]' %(np.max(noise),np.median(noise)))

	if output_noise:
		return np.array([data2]),peak_value_pre_filter,peak_value_post_filter,np.max(noise),np.median(noise)
	else:
		return np.array([data2])

###################################################################################################

def clear_oscillation_central3(data,time,framerate,oscillation_search_window_begin=6,oscillation_search_window_end=2.5,min_frequency_to_erase=20,max_frequency_to_erase=34,plot_conparison=False,which_plot=[1,2,3],ROI='auto',window=2,force_poscentre='auto'):

	# Created 17/06/2021
	# I try a different approach.
	# I find the oscillation acress the whole foil and subtract it from the whole record without looking at it pixel by pixel

	plt.figure()
	full_average = np.mean(data[0],axis=(-1,-2))
	full_spectra = np.fft.fft(full_average)
	full_magnitude = 2 * np.abs(full_spectra) / len(full_spectra)
	full_freq = np.fft.fftfreq(len(full_magnitude), d=1 / framerate)
	plt.plot(full_freq,full_magnitude,label='full')
	after_pulse_average = np.mean(data[0][int(oscillation_search_window_begin*framerate):],axis=(-1,-2))
	after_pulse_spectra = np.fft.fft(after_pulse_average)
	after_pulse_magnitude = 2 * np.abs(after_pulse_spectra) / len(full_spectra)
	after_pulse_freq = np.fft.fftfreq(len(after_pulse_magnitude), d=1 / framerate)
	plt.plot(after_pulse_freq,after_pulse_magnitude,label='after pulse')
	plt.legend(loc='best',fontsize='x-small')
	plt.xlabel('Frequency [Hz]')
	plt.ylabel('Amplitude [au]')
	plt.grid()
	plt.semilogy()
	plt.pause(0.01)

	from pynfft import NFFT, Solver
	time_cropped = time[np.logical_or(time<oscillation_search_window_end,time>oscillation_search_window_begin)]
	temp1 = data[0][time<oscillation_search_window_end]
	temp1 = np.mean(temp1,axis=(-1,-2))
	# temp1 -= np.mean(temp1)
	temp2 = data[0][time>oscillation_search_window_begin]
	temp2 = np.mean(temp2,axis=(-1,-2))
	# temp2 -= np.mean(temp2)
	data_cropped = np.array(temp1.tolist() + temp2.tolist())
	# data_cropped = np.mean(data[0],axis=(1,2))
	data_cropped -= generic_filter(data_cropped,np.mean,size=[int(framerate/30*2)])
	plt.figure()
	plt.plot(time_cropped,data_cropped)
	plt.pause(0.01)

	temp1 = data[0][time<oscillation_search_window_end]
	temp1 = np.mean(temp1,axis=(-1,-2))
	temp1 -= generic_filter(temp1,np.nanmean,size=[int(framerate/min_frequency_to_erase*10)])
	temp1 -= np.mean(temp1)
	temp2 = data[0][time>oscillation_search_window_begin]
	temp2 = np.mean(temp2,axis=(-1,-2))
	temp2 -= generic_filter(temp2,np.nanmean,size=[int(framerate/min_frequency_to_erase*10)])
	temp2 -= np.mean(temp2)
	data_cropped = np.array(temp1.tolist() + temp2.tolist())
	# plt.figure()
	plt.plot(time_cropped,data_cropped)
	plt.pause(0.01)

	M=len(data_cropped)
	N=M//2
	f = np.empty(M, dtype=np.complex128)
	f_hat = np.empty(N, dtype=np.complex128)


	if True:	# even if simple this seems to wiork well
		this_nfft = NFFT(N=N, M=M)
		this_nfft.x = (time_cropped/time_cropped.max() - 0.5)
		this_nfft.precompute()
		this_nfft.f = data_cropped
		ret2=this_nfft.adjoint()
		magnitude=2*np.abs(ret2)/len(ret2)
		freq = np.arange(-N/2,+N/2,1)/time_cropped.max()
		select = np.logical_and(magnitude>=magnitude.max(),freq>=0)
		# select = magnitude>=magnitude.max()
		peak_freq = freq[select]
		peak_angle = np.angle(ret2[select])
		plt.figure()
		plt.plot(np.abs(freq),magnitude)
		plt.plot(np.abs(freq)[select],magnitude[select],'+')
		plt.axvline(x=peak_freq,color='k',linestyle='--')
		plt.pause(0.01)
		angle = np.angle(ret2)
		plt.figure()
		plt.plot(freq,angle)
		plt.plot(freq[select],angle[select],'+')
		plt.pause(0.01)

		ret2[abs(freq)<20] = 0
		check_nfft = NFFT(N=N, M=M)
		check_nfft.x = (time_cropped/time_cropped.max() - 0.5)
		check_nfft.precompute()
		check_nfft.f_hat = ret2
		f=check_nfft.trafo()

		plt.figure()
		plt.plot((time_cropped/time_cropped.max() - 0.5),data_cropped)
		plt.plot((time_cropped/time_cropped.max() - 0.5),f*data_cropped.max()/f.max(),'--')
		plt.pause(0.01)
		plt.figure()
		plt.plot((time_cropped/time_cropped.max() - 0.5),data_cropped)
		plt.plot((time_cropped/time_cropped.max() - 0.5),f,'--')
		plt.pause(0.01)

		# ret2[freq<0] = 0
		# ret2[abs(freq)<29] = 0
		# ret2[abs(freq)>45] = 0
		ret2[::2]=0
		# ret2[np.logical_not(select)] = 0
		check_nfft = NFFT(N=N, M=len(time))
		check_nfft.x = (time/time.max() - 0.5)
		check_nfft.precompute()
		check_nfft.f_hat = ret2
		f=check_nfft.trafo()
		plt.figure()
		plt.plot((time_cropped/time_cropped.max() - 0.5),data_cropped)
		plt.plot((time/time.max() - 0.5),f*data_cropped.max()/f.max(),'--')
		plt.pause(0.01)

		def fun_cos(x,a,f,p):
			print([a,f,p])
			return a*np.cos(p+2*np.pi*x*f)

		for value in np.linspace(29,30,num=10):
			guess = [1,np.abs(peak_freq),peak_angle]
			# bds = [[0.5,0,-np.pi],[np.inf,np.inf,np.pi]]
			bds = [[1,28,-np.inf],[np.inf,32,np.inf]]
			x_scale = [0.1,30,0.1]
			fit = curve_fit(fun_cos,time_cropped,data_cropped,p0=guess,bounds=bds,ftol=1e-13,xtol=1e-13,verbose=2)
			print(value)
			print(fit)
		plt.figure()
		plt.plot(time_cropped,data_cropped)
		temp = fun_cos(time,*fit[0])
		plt.plot(time,temp,'--')
		temp = fun_cos(time_cropped,*fit[0])
		plt.plot(time_cropped,data_cropped-temp)
		plt.pause(0.01)


		# freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)
		# select = np.logical_and(magnitude>0.1,freq>=0)
		# plt.figure()
		# plt.plot(freq,magnitude)
		# plt.plot(freq[select],magnitude[select],'+')
		# plt.pause(0.01)
	elif False:	# the iterative process seems to diverge
		this_solver = Solver(this_nfft)
		this_solver.y = data_cropped
		this_solver.f_hat_iter = ret2
		this_solver.before_loop()
		while not np.all(this_solver.r_iter < 1e-2):
			this_solver.loop_one_step()
		magnitude=2*np.abs(this_solver.f_hat_iter)/len(this_solver.f_hat_iter)
		plt.figure()
		plt.plot(np.abs(np.arange(-N/2,+N/2,1)),magnitude)
		plt.pause(0.01)
		angle = np.angle(this_solver.f_hat_iter)
		plt.figure()
		plt.plot(np.abs(np.arange(-N/2,+N/2,1)),angle)
		plt.plot(np.abs(np.arange(-N/2,+N/2,1)),magnitude>np.sort(magnitude)[-10],anglemagnitude>np.sort(magnitude)[-10],'+')
		magnitude>np.sort(magnitude)[-10]
		plt.pause(0.01)

	data2 = cp.deepcopy(data[0].T)
	for i_freq,freq_ in enumerate(freq):
		if select[i_freq]==True:
			# break
			data2 -= magnitude[i_freq]*np.cos(angle[i_freq] + 2 * np.pi * freq_ * time)
	data2 = data2.T

	neg_nfft = NFFT(N=N, M=np.shape(data)[1])
	neg_nfft.x = time/time.max() - 0.5
	neg_nfft.precompute()
	neg_nfft.f_hat = ret2
	f = neg_nfft.trafo()

	plt.figure()
	plt.plot(time,np.mean(data[0],axis=(-1,-2)))
	plt.plot(time,np.mean(data2,axis=(-1,-2)),'--')
	plt.pause(0.01)


	cos_fun = lambda x,a,x0,p0,x1,p1,x2,p2: a*np.cos(x0 + 2*np.pi*p0*x)*np.cos(x1 + 2*np.pi*p1*x)*np.cos(x2 + 2*np.pi*p2*x)
	bds = [[1,-np.pi,24,-np.pi,0,-np.pi,0],[np.inf,np.pi,34,np.pi,5,np.pi,15]]
	guess=[1.5,0,29,0,1,]
	x_scale = [1,0.1,0.1,0.1,0.1]
	fit = curve_fit(cos_fun, time_cropped, data_cropped, p0=guess,bounds=bds,x_scale=x_scale,maxfev=int(1e6),verbose=2,ftol=1e-15)
	plt.plot(time,cos_fun(time,*fit[0]))
	plt.figure()
	plt.plot(time,np.mean(data[0],axis=(-1,-2)))
	plt.plot(time,np.mean(data[0],axis=(-1,-2))-cos_fun(time,*fit[0]),'--')
	plt.pause(0.01)

	# path abandoned

###################################################################################################


def clear_oscillation_central4(data,framerate,oscillation_search_window_begin='auto',oscillation_search_window_end='auto',min_frequency_to_erase=20,max_frequency_to_erase=34,plot_conparison=False,which_plot=[1,2,3],ROI='auto',window=2,force_poscentre='auto',output_noise=False,multiple_frequencies_cleaned=1):
	from scipy.signal import find_peaks, peak_prominences as get_proms

	# Created 29/07/2021
	# Function created starting from clear_oscillation_central2, but instead of repeating it over and ofer externally I do an internal loop
	# this is to remove oscillations that involve more than one single frequency (as it always is) in a reliable way

	print('shape of data array is '+str(np.shape(data))+', it should be (x,frames,v pixel,h pixel)')
	# figure_index = plt.gcf().number

	data=data[0]
	if oscillation_search_window_begin=='auto':
		force_start=0
	elif (oscillation_search_window_begin<0 or oscillation_search_window_begin>len(data)*framerate):
		print('The initial limit to search for the oscillation ad erase it is out of range (a time in seconds)')
		print('0s will be used instead')
		force_start=0
	else:
		force_start=int(oscillation_search_window_begin*framerate)

	if oscillation_search_window_end=='auto':
		force_end=len(data)
	elif (oscillation_search_window_end<0 or oscillation_search_window_end>(len(data)/framerate) or oscillation_search_window_end<=force_start/framerate):
		print('The final limit to search for the oscillation ad erase it is out of range (a time in seconds)')
		print(str(int(len(data)//(2*framerate)))+'s will be used instead')
		force_end=int(len(data)//(2*framerate))
	else:
		force_end=int(oscillation_search_window_end*framerate)


	central_freq_for_search = (max_frequency_to_erase-min_frequency_to_erase)/2+min_frequency_to_erase
	if (framerate<2*central_freq_for_search):
		print('There is a problem. The framerate is too low to try to extract the oscillation')
		print('The minimum framrate for doing it is 2*oscillation frequency to detect. Therefore '+str(np.around(2*central_freq_for_search,decimals=1))+'Hz, in this case.')
		print('See http://www.skillbank.co.uk/SignalConversion/rate.htm')
		exit()

	#window = 2	# Previously found that as long as the fft is averaged over at least 4 pixels the peak shape and location does not change
	datasection = data

	if plot_conparison==True:
		# plt.figure()
		# plt.pause(0.01)
		plt.figure()
		figure_index = plt.gcf().number
		if 1 in which_plot:
			data_shape = np.shape(data)
			poscentred = [[int(data_shape[1]*1/5), int(data_shape[2]*1/5)], [int(data_shape[1]*1/5), int(data_shape[2]*4/5)], [int(data_shape[1]*1/2), int(data_shape[2]*1/5)], [int(data_shape[1]*4/5), int(data_shape[2]*1/5)], [int(data_shape[1]*4/5), int(data_shape[2]*1/2)], [int(data_shape[1]*4/5), int(data_shape[2]*4/5)], [int(data_shape[1]*1/2), int(data_shape[2]*1/2)]]

			# spectra_orig = np.fft.fft(data, axis=0)
			# magnitude=np.sqrt(np.add(np.power(real,2),np.power(imag,2)))
			# magnitude = 2 * np.abs(spectra_orig) / len(spectra_orig)
			# phase = np.angle(spectra_orig)
			# freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)

			color = ['m', 'c', 'y', 'b', 'r', 'k', 'g', 'm']
			for i in range(len(poscentred)):
				pos = poscentred[i]
				spectra_orig = np.fft.fft(np.mean(data[:, pos[0] - window:pos[0] + window+1, pos[1] - window:pos[1] + window+1],axis=(-1,-2)), axis=0)
				magnitude = 2 * np.abs(spectra_orig) / len(spectra_orig)
				freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)
				# y = np.mean(magnitude, axis=(-1, -2))
				y = magnitude
				# y=magnitude[:,pos[0],pos[1]]
				y = np.array([y for _, y in sorted(zip(freq, y))])
				x = np.sort(freq)
				plt.plot(x, y*100, color[i], label='original data at the point ' + str(pos) + ' x100')
			# plt.title()


			plt.title('Amplitued from fast Fourier transform in the whole time interval\nfor different groups of ' + str(window * 2+1) + 'x' + str(
				window * 2+1) + ' pixels, framerate %.3gHz' %(framerate) )
			plt.xlabel('Frequency [Hz]')
			plt.ylabel('Amplitude [au]')
			plt.grid()
			plt.semilogy()
			plt.legend(loc='best',fontsize='xx-small')
		else:
			plt.close(figure_index)
	# plt.show()



	frames_for_oscillation = framerate//central_freq_for_search

	number_of_waves = 3
	fft_window_move = int(number_of_waves*frames_for_oscillation)
	if fft_window_move<10:
		number_of_waves=10//frames_for_oscillation	# I want to scan for at least 10 frame shifts
	fft_window_move = int(number_of_waves*frames_for_oscillation)
	step = 1
	while int(fft_window_move/step)>80:
		step+=1		#if framerate is too high i will skip some of the shifts to limit the number of Fourier transforms to 100


	# I restrict the window over which I search for the oscillation
	datarestricted = data[force_start:force_end]#
	len_data_restricted = len(datarestricted)
	if force_poscentre == 'auto':
		poscentre = [np.shape(data)[1] // 2, np.shape(data)[2] // 2]
	else:
		poscentre = force_poscentre

	if (oscillation_search_window_begin=='auto' and oscillation_search_window_end=='auto'):
		while fft_window_move>(len_data_restricted/5):
			fft_window_move-=1		# I want that the majority of the data I analyse remains the same

	if oscillation_search_window_end == 'auto':
		if (len(datarestricted) / framerate) <= 1:
			# max_start = int(sections // 2)
			max_start = int(len(datarestricted) // 2)
		else:
			max_start = int(5*framerate)  # I use 5 seconds of record
	else:
		max_start = len(datarestricted)	# this is actually ineffective, as datarestricted is already limited to force_start:force_end

	if oscillation_search_window_begin == 'auto':
		min_start = 0
	else:
		# min_start = force_start	# alreay enforced through force_start
		min_start = 0

	section_frames = max_start - min_start-fft_window_move

	if ROI=='auto':
		# datarestricted2 = np.mean(datarestricted[:, poscentre[0] -window :poscentre[0] + window+1, poscentre[1] - window:poscentre[1] + window+1],axis=(1,2))
		datarestricted2 = np.mean(datarestricted[:, poscentre[0] -window :poscentre[0] + window+1, poscentre[1] - window:poscentre[1] + window+1],axis=(-1,-2))
	else:
		horizontal_coord = np.arange(np.shape(datarestricted)[2])
		vertical_coord = np.arange(np.shape(datarestricted)[1])
		horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
		select = np.logical_or(np.logical_or(vertical_coord<ROI[0],vertical_coord>ROI[1]),np.logical_or(horizontal_coord<ROI[2],horizontal_coord>ROI[3]))
		datarestricted2 = np.mean(datarestricted[:, select],axis=(-1))


	spectra = np.fft.fft(datarestricted2, axis=0)
	magnitude_space_averaged = 2 * np.abs(spectra) / len(spectra)
	freq = np.fft.fftfreq(len(magnitude_space_averaged), d=1 / framerate)
	# magnitude_space_averaged = np.mean(magnitude, axis=(-1, -2))
	y = np.array([magnitude_space_averaged for _, magnitude_space_averaged in sorted(zip(freq, magnitude_space_averaged))])
	x = np.sort(freq)
	peaks_1 = find_peaks(y)[0]
	peaks = peaks_1[np.logical_and(x[peaks_1]<max_frequency_to_erase+15,x[peaks_1]>max_frequency_to_erase)]
	if len(peaks)==0:
		peaks = np.array(peaks_1[np.logical_and(x[peaks_1]>min_frequency_to_erase-15,x[peaks_1]<min_frequency_to_erase)])
		fit = [0,0,np.mean(np.log(y[peaks]))]	# I do this because I want to avoid that the fit only on the left goes too loo and I filter too much
	else:
		peaks = np.concatenate([peaks_1[np.logical_and(x[peaks_1]>min_frequency_to_erase-15,x[peaks_1]<min_frequency_to_erase)] , peaks_1[np.logical_and(x[peaks_1]<max_frequency_to_erase+15,x[peaks_1]>max_frequency_to_erase)]])
		fit = np.polyfit(x[peaks],np.log(y[peaks]),2)
	# poly2deg = lambda freq,a2,a1,a0 : a0 + a1*freq + a2*(freq**2)
	# guess = [min(fit[0],0),min(fit[1],0),fit[2]]
	# bds = [[-np.inf,-np.inf,-np.inf],[0,0,np.inf]]
	# fit = curve_fit(poly2deg, x[peaks],np.log(y[peaks]), p0=guess, bounds=bds, maxfev=100000000)[0]
	y_reference = np.exp(np.polyval(fit,x[np.logical_and(x>min_frequency_to_erase,x<max_frequency_to_erase)]))
	if min_frequency_to_erase>35:	# this is in place mainly only for the filter around 90Hz
		min_of_fit = x[np.logical_and(x>min_frequency_to_erase-15,x<min_frequency_to_erase)]
		min_of_fit = np.exp(np.polyval(fit,min_of_fit)).min()
		y_reference[y_reference > min_of_fit] = min_of_fit	# this is to avoid that when the parabola is fitted only on the left the fit goes up in the area of interest. the reference is limited to the minimum value of the fit outside of it
	y_reference = (3*np.std(y[peaks]/np.exp(np.polyval(fit,x[peaks])))+1)*y_reference

	y_test = y[np.logical_and(x>min_frequency_to_erase,x<max_frequency_to_erase)]
	if plot_conparison==True and (2 in which_plot):
		plt.figure(figure_index+1)
		plt.plot(x, y,'--r',label='original')
		plt.grid()
		plt.semilogy()
		plt.plot(x[peaks_1],y[peaks_1],'x')
		plt.axvline(x=min_frequency_to_erase,color='k',linestyle='--')
		plt.axvline(x=max_frequency_to_erase,color='k',linestyle='--')
		plt.plot(x[peaks],y[peaks],'o')
		plt.plot(x,np.exp(np.polyval(fit,x)))
		plt.plot(x[np.logical_and(x>min_frequency_to_erase,x<max_frequency_to_erase)],y_reference,'--')
		plt.xlabel('Frequency [Hz]')
		plt.ylabel('Amplitude [au]')
		plt.xlim(left=min_frequency_to_erase-5,right=max_frequency_to_erase+5)
		plt.ylim(bottom=np.median(np.sort(y)[:int(len(y)/4)])*1e-1,top = np.max(y[peaks])*2)

	frequencies_removed_all = []
	first_pass = True
	data2 = cp.deepcopy(data)
	while np.sum((y_test-y_reference)>0)>0:

		record_magnitude = []
		record_phase = []
		record_freq = []
		peak_freq_record = []
		peak_value_record = []
		peak_index_record = []
		shift_record = []

		for i in range(int(fft_window_move/step)):
			shift=i*step
			datasection = datarestricted2[min_start:max_start-fft_window_move+shift]
			spectra = np.fft.fft(datasection, axis=0)
			magnitude = 2 * np.abs(spectra) / len(spectra)
			record_magnitude.append(magnitude[0:len(magnitude) // 2])
			phase = np.angle(spectra)
			record_phase.append(phase[0:len(magnitude) // 2])
			freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)
			record_freq.append(freq[0:len(magnitude) // 2])
			# magnitude_space_averaged = np.mean(magnitude, axis=(-1, -2))
			y = np.array([value for _, value in sorted(zip(freq, magnitude))])
			x = np.sort(freq)

			index_min_freq = np.abs(x-min_frequency_to_erase).argmin()  # I restric the window over which I do the peak search
			index_max_freq = np.abs(x-max_frequency_to_erase).argmin()
			index_7 = np.abs(x-7).argmin()
			index_n7 = np.abs(x-(-7)).argmin()
			index_min_freq_n = np.abs(x-(-min_frequency_to_erase)).argmin()
			index_max_freq_n = np.abs(x-(-max_frequency_to_erase)).argmin()
			index_0 = np.abs(x-0).argmin()
			# noise = np.mean(np.array(
			# 	y[3:index_max_freq_n].tolist() + y[index_min_freq_n:index_n7].tolist() + y[index_7:index_min_freq].tolist() + y[
			# 																									index_max_freq:-3].tolist()),
			# 				axis=(-1))
			# temp = peakutils.indexes(y[index_min_freq:index_max_freq], thres=noise + np.abs(magnitude.min()),
			# 						 min_dist=(index_max_freq - index_min_freq) // 2)
			if len(y[index_min_freq:index_max_freq])==0:
				continue
			# if plot_conparison == True and (2 in which_plot):
			# 	plt.figure(figure_index+1)
			# 	plt.plot(x, y, label='Applied shift of ' + str(shift))
			temp = y[index_min_freq:index_max_freq].argmax()
			# if len(temp) == 1:
			peak_index = index_min_freq + int(temp)
			peak_freq_record.append(x[peak_index])
			peak_value = float(y[peak_index])
			peak_value_record.append(peak_value)
			peak_index_record.append(peak_index)
			shift_record.append(shift)
		record_magnitude = np.array(record_magnitude)
		record_phase = np.array(record_phase)
		record_freq = np.array(record_freq)
		peak_freq_record = np.array(peak_freq_record)
		peak_value_record = np.array(peak_value_record)
		peak_index_record = np.array(peak_index_record)
		shift_record = np.array(shift_record)
		# index = np.array(peak_value_record).argmax()	# this is wrong in the case that the baseline noise is strongly decreasing
		index = (np.array(peak_value_record)/np.exp(np.polyval(fit,peak_freq_record))).argmax()	# here I look at the strongest deviation from the noise baseline
		shift = index * step
		freq_to_erase = peak_freq_record[index]
		datasection = datarestricted[min_start:max_start-fft_window_move+shift]
		spectra = np.fft.fft(datasection, axis=0)
		# magnitude=np.sqrt(np.add(np.power(real,2),np.power(imag,2)))
		magnitude = 2 * np.abs(spectra) / len(spectra)
		phase = np.angle(spectra)
		freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)
		freq_to_erase_index = np.abs(freq-freq_to_erase).argmin()
		frequencies_removed_all.append(freq[freq_to_erase_index])
		freq_to_erase_index_multiple = np.arange(-(multiple_frequencies_cleaned-1)//2,(multiple_frequencies_cleaned-1)//2+1) + freq_to_erase_index
		print(freq_to_erase_index_multiple)

		if plot_conparison==True and (2 in which_plot):
			plt.figure(figure_index+1)
			# plt.plot([freq_to_erase]*2,[peak_value_record.min(),peak_value_record.max()],':k')
			# plt.plot([freq[freq_to_erase_index]]*2,[peak_value_record.min(),peak_value_record.max()],'--k')
			# plt.axvline(x=freq_to_erase,color='k',linestyle=':')
			plt.plot(freq[freq_to_erase_index],record_magnitude[index][np.abs(record_freq[index]-freq[freq_to_erase_index]).argmin()],'rx',markersize=10)
			if len(freq_to_erase_index_multiple)>1:
				for i_freq in freq_to_erase_index_multiple:
					if i_freq!=freq_to_erase_index:
						plt.plot(freq[i_freq],record_magnitude[index][np.abs(record_freq[index]-freq[i_freq]).argmin()],'yx',markersize=10)
			if first_pass:
				plt.ylim(top=peak_value_record.max()*2)
		if plot_conparison==True and (1 in which_plot):
			plt.figure(figure_index)
			plt.plot(freq[freq_to_erase_index],100*record_magnitude[index][np.abs(record_freq[index]-freq[freq_to_erase_index]).argmin()],'rx',markersize=10)
			if len(freq_to_erase_index_multiple)>1:
				for i_freq in freq_to_erase_index_multiple:
					if i_freq!=freq_to_erase_index:
						plt.plot(freq[i_freq],100*record_magnitude[index][np.abs(record_freq[index]-freq[i_freq]).argmin()],'yx',markersize=10)
		framenumber = np.linspace(0, len(data) - 1, len(data)) -force_start- min_start
		for i_freq in freq_to_erase_index_multiple:
			# print(i_freq)
			data2 -= np.multiply(magnitude[i_freq], np.cos(np.repeat(np.expand_dims(phase[i_freq], axis=0), len(data), axis=0) + np.repeat(np.expand_dims(np.repeat(np.expand_dims(2 * np.pi * freq[i_freq] * framenumber / framerate, axis=-1),np.shape(data)[1], axis=-1), axis=-1), np.shape(data)[2], axis=-1)))
		# data2 = data - np.multiply(magnitude[freq_to_erase_index], np.cos(np.repeat(np.expand_dims(phase[freq_to_erase_index], axis=0), len(data), axis=0) + np.repeat(np.expand_dims(np.repeat(np.expand_dims(2 * np.pi * freq_to_erase * framenumber / framerate, axis=-1),np.shape(data)[1], axis=-1), axis=-1), np.shape(data)[2], axis=-1)))

		datarestricted = data2[force_start:force_end]#
		if ROI=='auto':
			# datarestricted2 = np.mean(datarestricted[:, poscentre[0] -window :poscentre[0] + window+1, poscentre[1] - window:poscentre[1] + window+1],axis=(1,2))
			datarestricted2 = np.mean(datarestricted[:, poscentre[0] -window :poscentre[0] + window+1, poscentre[1] - window:poscentre[1] + window+1],axis=(-1,-2))
		else:
			horizontal_coord = np.arange(np.shape(datarestricted)[2])
			vertical_coord = np.arange(np.shape(datarestricted)[1])
			horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
			select = np.logical_or(np.logical_or(vertical_coord<ROI[0],vertical_coord>ROI[1]),np.logical_or(horizontal_coord<ROI[2],horizontal_coord>ROI[3]))
			datarestricted2 = np.mean(datarestricted[:, select],axis=(-1))

		spectra = np.fft.fft(datarestricted2, axis=0)
		magnitude_space_averaged = 2 * np.abs(spectra) / len(spectra)
		freq = np.fft.fftfreq(len(magnitude_space_averaged), d=1 / framerate)
		# magnitude_space_averaged = np.mean(magnitude, axis=(-1, -2))
		y = np.array([magnitude_space_averaged for _, magnitude_space_averaged in sorted(zip(freq, magnitude_space_averaged))])
		x = np.sort(freq)
		y_test = y[np.logical_and(x>min_frequency_to_erase,x<max_frequency_to_erase)]
		first_pass = False

	frequencies_removed_all = np.round(np.array(frequencies_removed_all)*100)/100
	# added to visualize the goodness of the result
	if plot_conparison==True and (2 in which_plot):
		plt.figure(figure_index+1)
		plt.plot(x, y,'--k',label='subtracted')
		# plt.grid()
		plt.legend(loc='best',fontsize='xx-small')
		if ROI=='auto':
			plt.title('Amplitude from fast Fourier transform from %.3gs to %.3gs\naveraged in a window of ' %(force_start/framerate,(force_start+max_start)/framerate)+ str([window * 2+1,window * 2+1]) + ' pixels around ' + str(poscentre) + ', framerate %.3gHz' %(framerate)+'\nremoved freq: '+str(frequencies_removed_all)+' Hz')
		else:
			plt.title('Amplitude from fast Fourier transform from %.3gs to %.3gs\naveraged ouside the ROI ' %(force_start/framerate,(force_start+max_start)/framerate)+ str(ROI) + ' pixels around ' + str(ROI) + ', framerate %.3gHz' %(framerate)+'\nremoved freq: '+str(frequencies_removed_all)+' Hz')


	# section only for stats
	# datasection = data[:, poscentre[0] - 1:poscentre[0] + window, poscentre[1] - 1:poscentre[1] + window]
	if ROI=='auto':
		datasection = np.mean(data[:, poscentre[0] -window :poscentre[0] + window+1, poscentre[1] - window:poscentre[1] + window+1],axis=(-1,-2))
	else:
		horizontal_coord = np.arange(np.shape(datarestricted)[2])
		vertical_coord = np.arange(np.shape(datarestricted)[1])
		horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
		select = np.logical_or(np.logical_or(vertical_coord<ROI[0],vertical_coord>ROI[1]),np.logical_or(horizontal_coord<ROI[2],horizontal_coord>ROI[3]))
		datasection = np.mean(data[:, select],axis=(-1))
	spectra = np.fft.fft(datasection, axis=0)
	magnitude_space_averaged = 2 * np.abs(spectra) / len(spectra)
	phase = np.angle(spectra)
	freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)
	# magnitude_space_averaged = np.mean(magnitude, axis=(-1, -2))
	y = np.array([magnitude_space_averaged for _, magnitude_space_averaged in sorted(zip(freq, magnitude_space_averaged))])
	x = np.sort(freq)
	index_min_freq = np.abs(x-(min_frequency_to_erase)).argmin()	# I restric the window over which I do the peak search
	index_max_freq = np.abs(x-(max_frequency_to_erase)).argmin()
	index_7 = np.abs(x-(7)).argmin()
	index_n7 = np.abs(x-(-7)).argmin()
	index_min_freq_n = np.abs(x-(-min_frequency_to_erase)).argmin()
	index_max_freq_n = np.abs(x-(-max_frequency_to_erase)).argmin()
	index_0 = np.abs(x-(0)).argmin()
	noise = (np.array(y[3:index_max_freq_n].tolist() + y[index_min_freq_n:index_n7].tolist() + y[index_7:index_min_freq].tolist() + y[index_max_freq:-3].tolist()))
	# temp = int(find_nearest_index(y[index_min_freq:index_max_freq], (y[index_min_freq:index_max_freq]).max()))
	# peak_index = index_min_freq + int(temp)
	peak_index = index_min_freq + y[index_min_freq:index_max_freq].argmax()
	peak_value_pre_filter = float(y[peak_index])

	# datasection = data2[:, poscentre[0] - 1:poscentre[0] + window, poscentre[1] - 1:poscentre[1] + window]
	if ROI=='auto':
		datasection = np.mean(data2[:, poscentre[0] -window :poscentre[0] + window+1, poscentre[1] - window:poscentre[1] + window+1],axis=(-1,-2))
	else:
		horizontal_coord = np.arange(np.shape(datarestricted)[2])
		vertical_coord = np.arange(np.shape(datarestricted)[1])
		horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
		select = np.logical_or(np.logical_or(vertical_coord<ROI[0],vertical_coord>ROI[1]),np.logical_or(horizontal_coord<ROI[2],horizontal_coord>ROI[3]))
		datasection = np.mean(data2[:, select],axis=(-1))
	spectra = np.fft.fft(datasection, axis=0)
	magnitude_space_averaged = 2 * np.abs(spectra) / len(spectra)
	phase = np.angle(spectra)
	freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)
	# magnitude_space_averaged = np.mean(magnitude, axis=(-1, -2))
	y = np.array([magnitude_space_averaged for _, magnitude_space_averaged in sorted(zip(freq, magnitude_space_averaged))])
	x = np.sort(freq)
	# temp = int(find_nearest_index(y[index_min_freq:index_max_freq], (y[index_min_freq:index_max_freq]).max()))
	# peak_index = index_min_freq + int(temp)
	peak_index = index_min_freq + y[index_min_freq:index_max_freq].argmax()
	peak_value_post_filter = float(y[peak_index])
	if plot_conparison==True and (2 in which_plot):
		plt.figure(figure_index+1)
		plt.axhline(y=np.max(noise),linestyle='--',color='k',label='max noise')
		plt.axhline(y=np.median(noise),linestyle='--',color='b',label='median noise')
		plt.axhline(y=peak_value_pre_filter,linestyle='--',color='r',label='peak pre')
		plt.axhline(y=peak_value_post_filter,linestyle='--',color='g',label='peak post')
		plt.legend(loc='best',fontsize='xx-small')

	if plot_conparison==True:
		if 1 in which_plot:
			plt.figure(figure_index)
			datasection2 = data2
			# spectra = np.fft.fft(datasection2, axis=0)
			# # magnitude=np.sqrt(np.add(np.power(real,2),np.power(imag,2)))
			# magnitude2 = 2 * np.abs(spectra) / len(spectra)
			# phase2 = np.angle(spectra)
			# freq = np.fft.fftfreq(len(magnitude2), d=1 / framerate)
			for i in range(len(poscentred)):
				pos = poscentred[i]
				spectra = np.fft.fft(np.mean(datasection2[:, pos[0] - window:pos[0] + window, pos[1] - window:pos[1] + window],axis=(-1,-2)), axis=0)
				magnitude2 = 2 * np.abs(spectra) / len(spectra)
				freq = np.fft.fftfreq(len(magnitude2), d=1 / framerate)
				# y = np.mean(magnitude2, axis=(-1, -2))
				y = magnitude2
				# y=magnitude[:,pos[0],pos[1]]
				y = np.array([y for _, y in sorted(zip(freq, y))])
				x = np.sort(freq)
				plt.plot(x, y, color[i] + '--',label='data at the point ' + str(pos) + ', oscillation substracted')
			# plt.title()


			# plt.grid()
			plt.semilogy()
			plt.xlim(left=0)
			plt.ylim(top=y[np.logical_and(x>min_frequency_to_erase,x<max_frequency_to_erase)].max()*2e3)
			plt.legend(loc='best',fontsize='xx-small')
		plt.pause(0.0001)




	print('stats of the oscillation removal')
	print('with window of size '+str(np.around(section_frames/framerate,decimals=5))+'s of '+str(len(data)/framerate)+'s of record')
	try:
		print('found oscillation of frequency '+str(frequencies_removed_all)+'Hz')
	except:
		print('no frequency needed removing')
	print('On the ROI oscillation magnitude reduced from %.5g[au] to %.5g[au]' %(peak_value_pre_filter,peak_value_post_filter)+'\nwith an approximate maximum noise of %.5g[au] and median of %.5g[au]' %(np.max(noise),np.median(noise)))

	if output_noise:
		return np.array([data2]),peak_value_pre_filter,peak_value_post_filter,np.max(noise),np.median(noise)
	else:
		return np.array([data2])


def real_mean_filter_agent(datasection,freq_to_erase_frames):
	freq_to_erase_frames = max(1,freq_to_erase_frames)	# safety added in case I forgot there is a bad behaviour if

	filter_agent = cp.deepcopy(datasection)
	padded_by = int(np.ceil(freq_to_erase_frames/2))+1
	temp = np.pad(filter_agent, ((padded_by,padded_by),(0,0),(0,0)), mode='reflect')
	for ii in range(-int(np.floor((freq_to_erase_frames-1)/2)),int(np.floor((freq_to_erase_frames-1)/2))+1):	# this is to add integer frames
		if ii!=0:
			filter_agent += temp[padded_by+ii:-padded_by+ii]
	filter_agent += temp[padded_by-int(np.floor((freq_to_erase_frames-1)/2))-1:-padded_by-int(np.floor((freq_to_erase_frames-1)/2))-1]*((freq_to_erase_frames-1)/2-int(np.floor((freq_to_erase_frames-1)/2)))	# this is to add fractionas of frames
	filter_agent += temp[padded_by+int(np.floor((freq_to_erase_frames-1)/2))+1:-padded_by+int(np.floor((freq_to_erase_frames-1)/2))+1]*((freq_to_erase_frames-1)/2-int(np.floor((freq_to_erase_frames-1)/2)))
	filter_agent/=freq_to_erase_frames
	return filter_agent

def clear_oscillation_central5(data,framerate,oscillation_search_window_begin='auto',oscillation_search_window_end='auto',min_frequency_to_erase=20,max_frequency_to_erase=34,plot_conparison=False,which_plot=[1,2,3],ROI='auto',window=2,force_poscentre='auto',output_noise=False,multiple_frequencies_cleaned=1):
	from scipy.signal import find_peaks, peak_prominences as get_proms

	# Created 04/08/2021
	# Function created starting from clear_oscillation_central4, the mean filter is applied before each frequency subtraction

	print('shape of data array is '+str(np.shape(data))+', it should be (x,frames,v pixel,h pixel)')
	# figure_index = plt.gcf().number

	data=data[0]
	if oscillation_search_window_begin=='auto':
		force_start=0
	elif (oscillation_search_window_begin<0 or oscillation_search_window_begin>len(data)*framerate):
		print('The initial limit to search for the oscillation ad erase it is out of range (a time in seconds)')
		print('0s will be used instead')
		force_start=0
	else:
		force_start=int(oscillation_search_window_begin*framerate)

	if oscillation_search_window_end=='auto':
		force_end=len(data)
	elif (oscillation_search_window_end<0 or oscillation_search_window_end>(len(data)/framerate) or oscillation_search_window_end<=force_start/framerate):
		print('The final limit to search for the oscillation ad erase it is out of range (a time in seconds)')
		print(str(int(len(data)//(2*framerate)))+'s will be used instead')
		force_end=int(len(data)//(2*framerate))
	else:
		force_end=int(oscillation_search_window_end*framerate)


	central_freq_for_search = (max_frequency_to_erase-min_frequency_to_erase)/2+min_frequency_to_erase
	if (framerate<2*central_freq_for_search):
		print('There is a problem. The framerate is too low to try to extract the oscillation')
		print('The minimum framrate for doing it is 2*oscillation frequency to detect. Therefore '+str(np.around(2*central_freq_for_search,decimals=1))+'Hz, in this case.')
		print('See http://www.skillbank.co.uk/SignalConversion/rate.htm')
		exit()

	#window = 2	# Previously found that as long as the fft is averaged over at least 4 pixels the peak shape and location does not change
	datasection = data

	if plot_conparison==True:
		# plt.figure()
		# plt.pause(0.01)
		plt.figure()
		figure_index = plt.gcf().number
		if 1 in which_plot:
			data_shape = np.shape(data)
			poscentred = [[int(data_shape[1]*1/5), int(data_shape[2]*1/5)], [int(data_shape[1]*1/5), int(data_shape[2]*4/5)], [int(data_shape[1]*1/2), int(data_shape[2]*1/5)], [int(data_shape[1]*4/5), int(data_shape[2]*1/5)], [int(data_shape[1]*4/5), int(data_shape[2]*1/2)], [int(data_shape[1]*4/5), int(data_shape[2]*4/5)], [int(data_shape[1]*1/2), int(data_shape[2]*1/2)]]

			# spectra_orig = np.fft.fft(data, axis=0)
			# magnitude=np.sqrt(np.add(np.power(real,2),np.power(imag,2)))
			# magnitude = 2 * np.abs(spectra_orig) / len(spectra_orig)
			# phase = np.angle(spectra_orig)
			# freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)

			color = ['m', 'c', 'y', 'b', 'r', 'k', 'g', 'm']
			for i in range(len(poscentred)):
				pos = poscentred[i]
				spectra_orig = np.fft.fft(np.mean(data[:, pos[0] - window:pos[0] + window+1, pos[1] - window:pos[1] + window+1],axis=(-1,-2)), axis=0)
				magnitude = 2 * np.abs(spectra_orig) / len(spectra_orig)
				freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)
				# y = np.mean(magnitude, axis=(-1, -2))
				y = magnitude
				# y=magnitude[:,pos[0],pos[1]]
				y = np.array([y for _, y in sorted(zip(freq, y))])
				x = np.sort(freq)
				plt.plot(x, y*100, color[i], label='original data at the point ' + str(pos) + ' x100')
			# plt.title()


			plt.title('Amplitued from fast Fourier transform in the whole time interval\nfor different groups of ' + str(window * 2+1) + 'x' + str(
				window * 2+1) + ' pixels, framerate %.3gHz' %(framerate) )
			plt.xlabel('Frequency [Hz]')
			plt.ylabel('Amplitude [au]')
			plt.grid()
			plt.semilogy()
			plt.legend(loc='best',fontsize='xx-small')
		else:
			plt.close(figure_index)
	# plt.show()



	frames_for_oscillation = framerate//central_freq_for_search

	number_of_waves = 3
	fft_window_move = int(number_of_waves*frames_for_oscillation)
	if fft_window_move<10:
		number_of_waves=10//frames_for_oscillation	# I want to scan for at least 10 frame shifts
	fft_window_move = int(number_of_waves*frames_for_oscillation)
	step = 1
	while int(fft_window_move/step)>80:
		step+=1		#if framerate is too high i will skip some of the shifts to limit the number of Fourier transforms to 100


	# I restrict the window over which I search for the oscillation
	datarestricted = data[force_start:force_end]#
	len_data_restricted = len(datarestricted)
	if force_poscentre == 'auto':
		poscentre = [np.shape(data)[1] // 2, np.shape(data)[2] // 2]
	else:
		poscentre = force_poscentre

	if (oscillation_search_window_begin=='auto' and oscillation_search_window_end=='auto'):
		while fft_window_move>(len_data_restricted/5):
			fft_window_move-=1		# I want that the majority of the data I analyse remains the same

	if oscillation_search_window_end == 'auto':
		if (len(datarestricted) / framerate) <= 1:
			# max_start = int(sections // 2)
			max_start = int(len(datarestricted) // 2)
		else:
			max_start = int(5*framerate)  # I use 5 seconds of record
	else:
		max_start = len(datarestricted)	# this is actually ineffective, as datarestricted is already limited to force_start:force_end

	if oscillation_search_window_begin == 'auto':
		min_start = 0
	else:
		# min_start = force_start	# alreay enforced through force_start
		min_start = 0

	section_frames = max_start - min_start-fft_window_move

	if ROI=='auto':
		# datarestricted2 = np.mean(datarestricted[:, poscentre[0] -window :poscentre[0] + window+1, poscentre[1] - window:poscentre[1] + window+1],axis=(1,2))
		datarestricted2 = np.mean(datarestricted[:, poscentre[0] -window :poscentre[0] + window+1, poscentre[1] - window:poscentre[1] + window+1],axis=(-1,-2))
	else:
		horizontal_coord = np.arange(np.shape(datarestricted)[2])
		vertical_coord = np.arange(np.shape(datarestricted)[1])
		horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
		select = np.logical_or(np.logical_or(vertical_coord<ROI[0],vertical_coord>ROI[1]),np.logical_or(horizontal_coord<ROI[2],horizontal_coord>ROI[3]))
		datarestricted2 = np.mean(datarestricted[:, select],axis=(-1))


	spectra = np.fft.fft(datarestricted2, axis=0)
	magnitude_space_averaged = 2 * np.abs(spectra) / len(spectra)
	freq = np.fft.fftfreq(len(magnitude_space_averaged), d=1 / framerate)
	# magnitude_space_averaged = np.mean(magnitude, axis=(-1, -2))
	y = np.array([magnitude_space_averaged for _, magnitude_space_averaged in sorted(zip(freq, magnitude_space_averaged))])
	x = np.sort(freq)
	peaks_1 = find_peaks(y)[0]
	peaks = peaks_1[np.logical_and(x[peaks_1]<max_frequency_to_erase+15,x[peaks_1]>max_frequency_to_erase)]
	if len(peaks)==0:
		peaks = np.array(peaks_1[np.logical_and(x[peaks_1]>min_frequency_to_erase-15,x[peaks_1]<min_frequency_to_erase)])
		fit = [0,0,np.mean(np.log(y[peaks]))]	# I do this because I want to avoid that the fit only on the left goes too loo and I filter too much
	else:
		peaks = np.concatenate([peaks_1[np.logical_and(x[peaks_1]>min_frequency_to_erase-15,x[peaks_1]<min_frequency_to_erase)] , peaks_1[np.logical_and(x[peaks_1]<max_frequency_to_erase+15,x[peaks_1]>max_frequency_to_erase)]])
		fit = np.polyfit(x[peaks],np.log(y[peaks]),2)
	# poly2deg = lambda freq,a2,a1,a0 : a0 + a1*freq + a2*(freq**2)
	# guess = [min(fit[0],0),min(fit[1],0),fit[2]]
	# bds = [[-np.inf,-np.inf,-np.inf],[0,0,np.inf]]
	# fit = curve_fit(poly2deg, x[peaks],np.log(y[peaks]), p0=guess, bounds=bds, maxfev=100000000)[0]
	y_reference = np.exp(np.polyval(fit,x[np.logical_and(x>min_frequency_to_erase,x<max_frequency_to_erase)]))
	if min_frequency_to_erase>35:	# this is in place mainly only for the filter around 90Hz
		min_of_fit = x[np.logical_and(x>min_frequency_to_erase-15,x<min_frequency_to_erase)]
		min_of_fit = np.exp(np.polyval(fit,min_of_fit)).min()
		y_reference[y_reference > min_of_fit] = min_of_fit	# this is to avoid that when the parabola is fitted only on the left the fit goes up in the area of interest. the reference is limited to the minimum value of the fit outside of it
	y_reference = (3*np.std(y[peaks]/np.exp(np.polyval(fit,x[peaks])))+1)*y_reference

	y_test = y[np.logical_and(x>min_frequency_to_erase,x<max_frequency_to_erase)]
	if plot_conparison==True and (2 in which_plot):
		plt.figure(figure_index+1)
		plt.plot(x, y,'--r',label='original')
		plt.grid()
		plt.semilogy()
		plt.plot(x[peaks_1],y[peaks_1],'x')
		plt.axvline(x=min_frequency_to_erase,color='k',linestyle='--')
		plt.axvline(x=max_frequency_to_erase,color='k',linestyle='--')
		plt.plot(x[peaks],y[peaks],'o')
		plt.plot(x,np.exp(np.polyval(fit,x)))
		plt.plot(x[np.logical_and(x>min_frequency_to_erase,x<max_frequency_to_erase)],y_reference,'--')
		plt.xlabel('Frequency [Hz]')
		plt.ylabel('Amplitude [au]')
		plt.xlim(left=min_frequency_to_erase-5,right=max_frequency_to_erase+5)
		plt.ylim(bottom=np.median(np.sort(y)[:int(len(y)/4)])*1e-1,top = np.max(y[peaks])*2)

	frequencies_removed_all = []
	first_pass = True
	data2 = cp.deepcopy(data)
	while np.sum((y_test-y_reference)>0)>0:

		record_magnitude = []
		record_phase = []
		record_freq = []
		peak_freq_record = []
		peak_value_record = []
		peak_index_record = []
		shift_record = []

		for i in range(int(fft_window_move/step)):
			shift=i*step
			datasection = datarestricted2[min_start:max_start-fft_window_move+shift]
			spectra = np.fft.fft(datasection, axis=0)
			magnitude = 2 * np.abs(spectra) / len(spectra)
			record_magnitude.append(magnitude[0:len(magnitude) // 2])
			phase = np.angle(spectra)
			record_phase.append(phase[0:len(magnitude) // 2])
			freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)
			record_freq.append(freq[0:len(magnitude) // 2])
			# magnitude_space_averaged = np.mean(magnitude, axis=(-1, -2))
			y = np.array([value for _, value in sorted(zip(freq, magnitude))])
			x = np.sort(freq)

			index_min_freq = np.abs(x-min_frequency_to_erase).argmin()  # I restric the window over which I do the peak search
			index_max_freq = np.abs(x-max_frequency_to_erase).argmin()
			index_7 = np.abs(x-7).argmin()
			index_n7 = np.abs(x-(-7)).argmin()
			index_min_freq_n = np.abs(x-(-min_frequency_to_erase)).argmin()
			index_max_freq_n = np.abs(x-(-max_frequency_to_erase)).argmin()
			index_0 = np.abs(x-0).argmin()
			# noise = np.mean(np.array(
			# 	y[3:index_max_freq_n].tolist() + y[index_min_freq_n:index_n7].tolist() + y[index_7:index_min_freq].tolist() + y[
			# 																									index_max_freq:-3].tolist()),
			# 				axis=(-1))
			# temp = peakutils.indexes(y[index_min_freq:index_max_freq], thres=noise + np.abs(magnitude.min()),
			# 						 min_dist=(index_max_freq - index_min_freq) // 2)
			if len(y[index_min_freq:index_max_freq])==0:
				continue
			# if plot_conparison == True and (2 in which_plot):
			# 	plt.figure(figure_index+1)
			# 	plt.plot(x, y, label='Applied shift of ' + str(shift))
			temp = y[index_min_freq:index_max_freq].argmax()
			# if len(temp) == 1:
			peak_index = index_min_freq + int(temp)
			peak_freq_record.append(x[peak_index])
			peak_value = float(y[peak_index])
			peak_value_record.append(peak_value)
			peak_index_record.append(peak_index)
			shift_record.append(shift)
		record_magnitude = np.array(record_magnitude)
		record_phase = np.array(record_phase)
		record_freq = np.array(record_freq)
		peak_freq_record = np.array(peak_freq_record)
		peak_value_record = np.array(peak_value_record)
		peak_index_record = np.array(peak_index_record)
		shift_record = np.array(shift_record)
		# index = np.array(peak_value_record).argmax()	# this is wrong in the case that the baseline noise is strongly decreasing
		index = (np.array(peak_value_record)/np.exp(np.polyval(fit,peak_freq_record))).argmax()	# here I look at the strongest deviation from the noise baseline
		shift = index * step
		freq_to_erase = peak_freq_record[index]
		datasection = datarestricted[min_start:max_start-fft_window_move+shift]
		print('filtering '+str([freq_to_erase,framerate/freq_to_erase,index]))
		filter_agent = real_mean_filter_agent(datasection,framerate/freq_to_erase)	# added to make sure to dynamically remove only the wanted frequency
		spectra = np.fft.fft(datasection-filter_agent, axis=0)
		# magnitude=np.sqrt(np.add(np.power(real,2),np.power(imag,2)))
		magnitude = 2 * np.abs(spectra) / len(spectra)
		phase = np.angle(spectra)
		freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)
		freq_to_erase_index = np.abs(freq-freq_to_erase).argmin()
		frequencies_removed_all.append(freq[freq_to_erase_index])
		freq_to_erase_index_multiple = np.arange(-(multiple_frequencies_cleaned-1)//2,(multiple_frequencies_cleaned-1)//2+1) + freq_to_erase_index
		print(freq_to_erase_index_multiple)

		if plot_conparison==True and (2 in which_plot):
			plt.figure(figure_index+1)
			# plt.plot([freq_to_erase]*2,[peak_value_record.min(),peak_value_record.max()],':k')
			# plt.plot([freq[freq_to_erase_index]]*2,[peak_value_record.min(),peak_value_record.max()],'--k')
			# plt.axvline(x=freq_to_erase,color='k',linestyle=':')
			plt.plot(freq[freq_to_erase_index],record_magnitude[index][np.abs(record_freq[index]-freq[freq_to_erase_index]).argmin()],'rx',markersize=10)
			if len(freq_to_erase_index_multiple)>1:
				for i_freq in freq_to_erase_index_multiple:
					if i_freq!=freq_to_erase_index:
						plt.plot(freq[i_freq],record_magnitude[index][np.abs(record_freq[index]-freq[i_freq]).argmin()],'yx',markersize=10)
			if first_pass:
				plt.ylim(top=peak_value_record.max()*2)
		if plot_conparison==True and (1 in which_plot):
			plt.figure(figure_index)
			plt.plot(freq[freq_to_erase_index],100*record_magnitude[index][np.abs(record_freq[index]-freq[freq_to_erase_index]).argmin()],'rx',markersize=10)
			if len(freq_to_erase_index_multiple)>1:
				for i_freq in freq_to_erase_index_multiple:
					if i_freq!=freq_to_erase_index:
						plt.plot(freq[i_freq],100*record_magnitude[index][np.abs(record_freq[index]-freq[i_freq]).argmin()],'yx',markersize=10)
		framenumber = np.linspace(0, len(data) - 1, len(data)) -force_start- min_start
		for i_freq in freq_to_erase_index_multiple:
			# print(i_freq)
			data2 -= np.multiply(magnitude[i_freq], np.cos(np.repeat(np.expand_dims(phase[i_freq], axis=0), len(data), axis=0) + np.repeat(np.expand_dims(np.repeat(np.expand_dims(2 * np.pi * freq[i_freq] * framenumber / framerate, axis=-1),np.shape(data)[1], axis=-1), axis=-1), np.shape(data)[2], axis=-1)))
		# data2 = data - np.multiply(magnitude[freq_to_erase_index], np.cos(np.repeat(np.expand_dims(phase[freq_to_erase_index], axis=0), len(data), axis=0) + np.repeat(np.expand_dims(np.repeat(np.expand_dims(2 * np.pi * freq_to_erase * framenumber / framerate, axis=-1),np.shape(data)[1], axis=-1), axis=-1), np.shape(data)[2], axis=-1)))

		datarestricted = data2[force_start:force_end]#
		if ROI=='auto':
			# datarestricted2 = np.mean(datarestricted[:, poscentre[0] -window :poscentre[0] + window+1, poscentre[1] - window:poscentre[1] + window+1],axis=(1,2))
			datarestricted2 = np.mean(datarestricted[:, poscentre[0] -window :poscentre[0] + window+1, poscentre[1] - window:poscentre[1] + window+1],axis=(-1,-2))
		else:
			horizontal_coord = np.arange(np.shape(datarestricted)[2])
			vertical_coord = np.arange(np.shape(datarestricted)[1])
			horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
			select = np.logical_or(np.logical_or(vertical_coord<ROI[0],vertical_coord>ROI[1]),np.logical_or(horizontal_coord<ROI[2],horizontal_coord>ROI[3]))
			datarestricted2 = np.mean(datarestricted[:, select],axis=(-1))

		spectra = np.fft.fft(datarestricted2, axis=0)
		magnitude_space_averaged = 2 * np.abs(spectra) / len(spectra)
		freq = np.fft.fftfreq(len(magnitude_space_averaged), d=1 / framerate)
		# magnitude_space_averaged = np.mean(magnitude, axis=(-1, -2))
		y = np.array([magnitude_space_averaged for _, magnitude_space_averaged in sorted(zip(freq, magnitude_space_averaged))])
		x = np.sort(freq)
		y_test = y[np.logical_and(x>min_frequency_to_erase,x<max_frequency_to_erase)]
		first_pass = False

	frequencies_removed_all = np.round(np.array(frequencies_removed_all)*100)/100
	# added to visualize the goodness of the result
	if plot_conparison==True and (2 in which_plot):
		plt.figure(figure_index+1)
		plt.plot(x, y,'--k',label='subtracted')
		# plt.grid()
		plt.legend(loc='best',fontsize='xx-small')
		if ROI=='auto':
			plt.title('Amplitude from fast Fourier transform from %.3gs to %.3gs\naveraged in a window of ' %(force_start/framerate,(force_start+max_start)/framerate)+ str([window * 2+1,window * 2+1]) + ' pixels around ' + str(poscentre) + ', framerate %.3gHz' %(framerate)+'\nremoved freq: '+str(frequencies_removed_all)+' Hz')
		else:
			plt.title('Amplitude from fast Fourier transform from %.3gs to %.3gs\naveraged ouside the ROI ' %(force_start/framerate,(force_start+max_start)/framerate)+ str(ROI) + ' pixels around ' + str(ROI) + ', framerate %.3gHz' %(framerate)+'\nremoved freq: '+str(frequencies_removed_all)+' Hz')


	# section only for stats
	# datasection = data[:, poscentre[0] - 1:poscentre[0] + window, poscentre[1] - 1:poscentre[1] + window]
	if ROI=='auto':
		datasection = np.mean(data[:, poscentre[0] -window :poscentre[0] + window+1, poscentre[1] - window:poscentre[1] + window+1],axis=(-1,-2))
	else:
		horizontal_coord = np.arange(np.shape(datarestricted)[2])
		vertical_coord = np.arange(np.shape(datarestricted)[1])
		horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
		select = np.logical_or(np.logical_or(vertical_coord<ROI[0],vertical_coord>ROI[1]),np.logical_or(horizontal_coord<ROI[2],horizontal_coord>ROI[3]))
		datasection = np.mean(data[:, select],axis=(-1))
	spectra = np.fft.fft(datasection, axis=0)
	magnitude_space_averaged = 2 * np.abs(spectra) / len(spectra)
	phase = np.angle(spectra)
	freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)
	# magnitude_space_averaged = np.mean(magnitude, axis=(-1, -2))
	y = np.array([magnitude_space_averaged for _, magnitude_space_averaged in sorted(zip(freq, magnitude_space_averaged))])
	x = np.sort(freq)
	index_min_freq = np.abs(x-(min_frequency_to_erase)).argmin()	# I restric the window over which I do the peak search
	index_max_freq = np.abs(x-(max_frequency_to_erase)).argmin()
	index_7 = np.abs(x-(7)).argmin()
	index_n7 = np.abs(x-(-7)).argmin()
	index_min_freq_n = np.abs(x-(-min_frequency_to_erase)).argmin()
	index_max_freq_n = np.abs(x-(-max_frequency_to_erase)).argmin()
	index_0 = np.abs(x-(0)).argmin()
	noise = (np.array(y[3:index_max_freq_n].tolist() + y[index_min_freq_n:index_n7].tolist() + y[index_7:index_min_freq].tolist() + y[index_max_freq:-3].tolist()))
	# temp = int(find_nearest_index(y[index_min_freq:index_max_freq], (y[index_min_freq:index_max_freq]).max()))
	# peak_index = index_min_freq + int(temp)
	peak_index = index_min_freq + y[index_min_freq:index_max_freq].argmax()
	peak_value_pre_filter = float(y[peak_index])

	# datasection = data2[:, poscentre[0] - 1:poscentre[0] + window, poscentre[1] - 1:poscentre[1] + window]
	if ROI=='auto':
		datasection = np.mean(data2[:, poscentre[0] -window :poscentre[0] + window+1, poscentre[1] - window:poscentre[1] + window+1],axis=(-1,-2))
	else:
		horizontal_coord = np.arange(np.shape(datarestricted)[2])
		vertical_coord = np.arange(np.shape(datarestricted)[1])
		horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
		select = np.logical_or(np.logical_or(vertical_coord<ROI[0],vertical_coord>ROI[1]),np.logical_or(horizontal_coord<ROI[2],horizontal_coord>ROI[3]))
		datasection = np.mean(data2[:, select],axis=(-1))
	spectra = np.fft.fft(datasection, axis=0)
	magnitude_space_averaged = 2 * np.abs(spectra) / len(spectra)
	phase = np.angle(spectra)
	freq = np.fft.fftfreq(len(magnitude), d=1 / framerate)
	# magnitude_space_averaged = np.mean(magnitude, axis=(-1, -2))
	y = np.array([magnitude_space_averaged for _, magnitude_space_averaged in sorted(zip(freq, magnitude_space_averaged))])
	x = np.sort(freq)
	# temp = int(find_nearest_index(y[index_min_freq:index_max_freq], (y[index_min_freq:index_max_freq]).max()))
	# peak_index = index_min_freq + int(temp)
	peak_index = index_min_freq + y[index_min_freq:index_max_freq].argmax()
	peak_value_post_filter = float(y[peak_index])
	if plot_conparison==True and (2 in which_plot):
		plt.figure(figure_index+1)
		plt.axhline(y=np.max(noise),linestyle='--',color='k',label='max noise')
		plt.axhline(y=np.median(noise),linestyle='--',color='b',label='median noise')
		plt.axhline(y=peak_value_pre_filter,linestyle='--',color='r',label='peak pre')
		plt.axhline(y=peak_value_post_filter,linestyle='--',color='g',label='peak post')
		plt.legend(loc='best',fontsize='xx-small')

	if plot_conparison==True:
		if 1 in which_plot:
			plt.figure(figure_index)
			datasection2 = data2
			# spectra = np.fft.fft(datasection2, axis=0)
			# # magnitude=np.sqrt(np.add(np.power(real,2),np.power(imag,2)))
			# magnitude2 = 2 * np.abs(spectra) / len(spectra)
			# phase2 = np.angle(spectra)
			# freq = np.fft.fftfreq(len(magnitude2), d=1 / framerate)
			for i in range(len(poscentred)):
				pos = poscentred[i]
				spectra = np.fft.fft(np.mean(datasection2[:, pos[0] - window:pos[0] + window, pos[1] - window:pos[1] + window],axis=(-1,-2)), axis=0)
				magnitude2 = 2 * np.abs(spectra) / len(spectra)
				freq = np.fft.fftfreq(len(magnitude2), d=1 / framerate)
				# y = np.mean(magnitude2, axis=(-1, -2))
				y = magnitude2
				# y=magnitude[:,pos[0],pos[1]]
				y = np.array([y for _, y in sorted(zip(freq, y))])
				x = np.sort(freq)
				plt.plot(x, y, color[i] + '--',label='data at the point ' + str(pos) + ', oscillation substracted')
			# plt.title()


			# plt.grid()
			plt.semilogy()
			plt.xlim(left=0)
			plt.ylim(top=y[np.logical_and(x>min_frequency_to_erase,x<max_frequency_to_erase)].max()*2e3)
			plt.legend(loc='best',fontsize='xx-small')
		plt.pause(0.0001)




	print('stats of the oscillation removal')
	print('with window of size '+str(np.around(section_frames/framerate,decimals=5))+'s of '+str(len(data)/framerate)+'s of record')
	try:
		print('found oscillation of frequency '+str(frequencies_removed_all)+'Hz')
	except:
		print('no frequency needed removing')
	print('On the ROI oscillation magnitude reduced from %.5g[au] to %.5g[au]' %(peak_value_pre_filter,peak_value_post_filter)+'\nwith an approximate maximum noise of %.5g[au] and median of %.5g[au]' %(np.max(noise),np.median(noise)))

	if output_noise:
		return np.array([data2]),peak_value_pre_filter,peak_value_post_filter,np.max(noise),np.median(noise)
	else:
		return np.array([data2])

###################################################################################################


def abs_angle(angle,deg=False):
	import numpy as np

	out = angle
	if deg==False:
		if out<=0:
			out=angle+2*np.pi
	elif deg==True:
		if out<=0:
			out=angle+360

	return out



###################################################################################################



def split_fixed_length(A,length):
	A=np.array(A)
	length = int(length)
	B=[]
	while len(A)>length:
		B.append(A[:length].tolist())
		A=A[length:]
	if len(A)>0:
		B.append(A.tolist())
	return np.array(B)



###################################################################################################


# function to print on screen a whole array even if it is very large

def fullprint(*args, **kwargs):
	from pprint import pprint
	import numpy as np
	opt = np.get_printoptions()
	np.set_printoptions(threshold=np.inf)
	pprint(*args, **kwargs)
	np.set_printoptions(**opt)



###################################################################################################


def find_dead_pixels(data, start_interval='auto',end_interval='auto', framerate='auto',from_data=True,treshold_for_bad_low_std=0,treshold_for_bad_std=10,treshold_for_bad_difference=13,verbose=2):

	# Created 26/02/2019
	# function that finds the dead pixels. 'data' can be a string with the path of the record or the data itself.
	# if the path is given the oscillation filtering is done
	# default tresholds for std and difference found 25/02/2019

	# Output legend:
		# 3 = treshold for difference with neighbouring pixels trepassed
		# 6 = treshold for std trepassed
		# 9 = both treshold trepassed

	if from_data==False:
		if framerate=='auto':
			print('if you load from file you must specify the framerate')
			exit()
		path=data
		print('Loading a recod from '+path)
		filenames = all_file_names(path, '.npy')[0]
		data = np.load(os.path.join(path, filenames))
		data2 = clear_oscillation_central2(data, framerate, plot_conparison=False)
		data = np.array(data2)
	else:
		data = np.array(data)

	if end_interval=='auto':
		end_interval=np.shape(data)[1]
	else:
		if framerate=='auto':
			print('if you specify an end time you must specify the framerate')
			exit()
		else:
			end_interval=end_interval*framerate
			if (end_interval>np.shape(data)[1]):
				end_interval = np.shape(data)[1]

	if start_interval=='auto':
		start_interval=0
	else:
		if framerate=='auto':
			print('if you specify a start time you must specify the framerate')
			exit()
		else:
			start_interval=start_interval*framerate
			if (start_interval>end_interval):
				print("You set the start interval after the end one, I'm going to use zero seconds")
				start_interval = 0


	data=data[0,start_interval:end_interval]

	# in /home/ffederic/work/irvb/vacuum_chamber_testing/Aug13_2018/irvb_sample-000018
	# I found that some frames can be completely missing, filled with 0 or 64896
	# and this messes with calculating std and mean, so I need to remove this frames
	min_data = np.nanmin(data,axis=(-1,-2))
	# max_data = np.nanmin(data,axis=-1,-2)		# I don't want to use this because saturation could cause this too
	data = data[min_data>0]

	mean=np.mean(data,axis=(0))
	std=np.std(data,axis=(0))
	flag_check=np.zeros(np.shape(std))
	flag = np.ones(np.shape(std))
	# for i in range(np.shape(std)[0]):
	# 	for j in range(np.shape(std)[1]):
	# 		if std[i,j]>treshold_for_bad_std:
	# 			flag_check[i,j]=6
	# 			flag[i, j] = 0
	flag_check[std>treshold_for_bad_std] = 6
	flag[std>treshold_for_bad_std] = 0
	flag_check[std<=treshold_for_bad_low_std] = 6
	flag[std<=treshold_for_bad_low_std] = 0
	for repeats in [0, 1]:
		mean_flag = mean * flag
		for i in range(1, np.shape(std)[0] - 1):
			for j in range(1, np.shape(std)[1] - 1):
				if flag_check[i,j] in [3,9]:
					continue
				temp = mean_flag[i - 1, j - 1:j + 2].tolist() + [mean_flag[i, j - 1].tolist()] + [mean_flag[i, j + 1].tolist()] + mean_flag[i + 1, j - 1:j + 2].tolist()
				if len(temp) == 0:
					# flag_check[i, j] += 3
					# flag[i, j] = 0
					continue
				else:
					temp2 = [x for x in temp if x != 0]
					# following added to avoid problems when 2 dead pixels are adjacent. the 3 as multimplier is completely arbitrary, it's just to not modify the behaviour when this funcion is used somewhere else
					temp2 = [x for x in temp2 if x < np.median(temp2)+3*treshold_for_bad_difference]
					temp2 = [x for x in temp2 if x > np.median(temp2)-3*treshold_for_bad_difference]
					if len(temp2) == 0:
						# flag_check[i, j] += 3
						# flag[i, j] = 0
						continue
					else:
						if (mean[i, j] > max(temp2) + treshold_for_bad_difference or mean[i, j] < min(temp2) - treshold_for_bad_difference):
						# if np.abs(mean[i, j] - np.median(temp2))> treshold_for_bad_difference:
							flag_check[i, j] += 3
							flag[i, j] = 0
	# follows a slower version that checks also the edges
	# i_indexes = (np.ones_like(std).T*np.linspace(0,np.shape(std)[0]-1,np.shape(std)[0])).T
	# j_indexes = np.ones_like(std) * np.linspace(0, np.shape(std)[1] - 1, np.shape(std)[1])
	# # gna=np.zeros_like(std)
	# for repeats in [0, 1]:
	# 	for i in range(np.shape(std)[0] ):
	# 		for j in range(np.shape(std)[1] ):
	# 			if flag_check[i,j] in [3,9]:
	# 				continue
	# 			temp = mean[np.logical_and(flag , np.logical_and(np.abs(i_indexes-i)<=1 , np.logical_and( np.abs(j_indexes-j)<=1 , np.logical_or(i_indexes!=i , j_indexes!=j))))]
	# 			# gna[i,j] = np.std(temp)
	# 			if (mean[i, j] > max(temp) + treshold_for_bad_difference or mean[i, j] < min(temp) - treshold_for_bad_difference):
	# 				flag_check[i, j] += 3
	# 				flag[i, j] = 0


	if verbose>1:
		counter = collections.Counter(flatten_full(flag_check))
		print('Number of pixels that trepass '+str(treshold_for_bad_difference)+' counts difference with neighbours: '+str(counter.get(3)))
		print('Number of pixels with standard deviation > '+str(treshold_for_bad_std)+' counts: '+str(counter.get(6)))
		print('Number of pixels that trepass both limits: '+str(counter.get(9)))
	elif verbose>0:
		counter = collections.Counter(flatten_full(flag_check))
		print('diff>'+str(treshold_for_bad_difference)+': '+str(counter.get(3)) + ' , ' + 'std>' +str(treshold_for_bad_std)+': '+str(counter.get(6)) + ' , ' + 'both: '+str(counter.get(9)))
	return flag_check


###################################################################################################

def find_dead_pixels_data_acquisition_stage(data,treshold_for_bad_difference=30,verbose=2):
	from scipy.ndimage import median_filter
	import collections
	# Created 17/12/2021
	# function that finds the dead pixels looking at the 8 around it. it is used only to check if during the recording the order of the digitizer switches
	# if the path is given the oscillation filtering is done
	# default tresholds for std and difference found 25/02/2019

	data = np.array(data)
	median=median_filter(data,footprint=[[1,1,1],[1,0,1],[1,1,1]])
	flag_check = (np.abs(data-median)>treshold_for_bad_difference)*3

	if verbose>1:
		counter = collections.Counter(flag_check.flatten())
		print('Number of pixels that trepass '+str(treshold_for_bad_difference)+' counts difference with neighbours: '+str(counter.get(3)))
	elif verbose>0:
		counter = collections.Counter(flag_check.flatten())
		print('diff>'+str(treshold_for_bad_difference)+': '+str(counter.get(3)))
	return flag_check


###################################################################################################


def replace_dead_pixels(data,flag, framerate='auto',from_data=True):

	# Created 26/02/2019
	# function replace dead pixels with the average from their neighbours

	if from_data==False:
		if framerate=='auto':
			print('if you load from file you must specify the framerate')
			exit()
		path=data
		print('Loading a recod from '+path)
		filenames = all_file_names(path, '.npy')[0]
		data = np.load(os.path.join(path, filenames))
		data2 = clear_oscillation_central2(data, framerate, plot_conparison=False)
		data = np.array(data2)[0]
		del data2
	else:
		data = np.array(data)[0]

	flag=np.array(flag)

	if np.shape(data[0])!=np.shape(flag):
		print('There is something wrong, the shape of the data '+str(np.shape(data[0]))+' and the dead pixels map '+str(np.shape(flag))+' you want to use are not the same')
		bla=gna-sblu	# I want it to fail here, so I know where it did
		exit()

	data2=np.array([data])

	for i in range(np.shape(data)[-2]):
		for j in range(np.shape(data)[-1]):
			if flag[i,j]!=0:
				temp=[]
				for i_ in [-1,0,1]:
					for j_ in [-1, 0, 1]:
						if (i_!=0 and j_!=0):
							if flag[i+i_,j+j_]==0:
								temp.append(data[:,i+i_, j +j_])
				data2[0,:,i,j]=np.median(temp,axis=(0))

	return data2


###################################################################################################


def record_rotation(data,rotangle, foilcenter= [160, 133],	foilhorizw = 0.09, foilvertw = 0.07,foilhorizwpixel = 240,precisionincrease = 10):

	# Created 26/02/2019
	# function created to rotate and crop a whole record given central point and rotation angle

	mean = data[0,0]

	testrot = mean
	foilrot = rotangle * 2 * np.pi / 360
	foilrotdeg = rotangle
	foilvertwpixel = int(np.around((foilhorizwpixel * foilvertw) / foilhorizw))
	r = ((foilhorizwpixel ** 2 + foilvertwpixel ** 2) ** 0.5) / 2  # HALF DIAGONAL
	a = foilvertwpixel / np.cos(foilrot)
	tgalpha = np.tan(foilrot)
	delta = -(a ** 2) / 4 + (1 + tgalpha ** 2) * (r ** 2)
	foilx = np.add(foilcenter[0], [(-0.5 * a * tgalpha + delta ** 0.5) / (1 + tgalpha ** 2),
								   (-0.5 * a * tgalpha - delta ** 0.5) / (1 + tgalpha ** 2),
								   (0.5 * a * tgalpha - delta ** 0.5) / (1 + tgalpha ** 2),
								   (0.5 * a * tgalpha + delta ** 0.5) / (1 + tgalpha ** 2),
								   (-0.5 * a * tgalpha + delta ** 0.5) / (1 + tgalpha ** 2)])
	foily = np.add(foilcenter[1] - tgalpha * foilcenter[0],
				   [tgalpha * foilx[0] + a / 2, tgalpha * foilx[1] + a / 2, tgalpha * foilx[2] - a / 2,
					tgalpha * foilx[3] - a / 2, tgalpha * foilx[0] + a / 2])
	foilxint = (np.rint(foilx)).astype(int)
	foilyint = (np.rint(foily)).astype(int)


	dummy = np.ones(np.multiply(np.shape(testrot), precisionincrease))
	dummy[foilcenter[1] * precisionincrease, foilcenter[0] * precisionincrease] = 2
	dummy[int(foily[0] * precisionincrease), int(foilx[0] * precisionincrease)] = 3
	dummy[int(foily[1] * precisionincrease), int(foilx[1] * precisionincrease)] = 4
	dummy[int(foily[2] * precisionincrease), int(foilx[2] * precisionincrease)] = 5
	dummy[int(foily[3] * precisionincrease), int(foilx[3] * precisionincrease)] = 6
	dummy2 = rotate(dummy, foilrotdeg, axes=(-1, -2), order=0)
	foilcenterrot = (
		np.rint(
			[np.where(dummy2 == 2)[1][0] / precisionincrease, np.where(dummy2 == 2)[0][0] / precisionincrease])).astype(
		int)
	foilxrot = (
		np.rint([np.where(dummy2 == 3)[1][0] / precisionincrease, np.where(dummy2 == 4)[1][0] / precisionincrease,
				 np.where(dummy2 == 5)[1][0] / precisionincrease, np.where(dummy2 == 6)[1][0] / precisionincrease,
				 np.where(dummy2 == 3)[1][0] / precisionincrease])).astype(int)
	foilyrot = (
		np.rint([np.where(dummy2 == 3)[0][0] / precisionincrease, np.where(dummy2 == 4)[0][0] / precisionincrease,
				 np.where(dummy2 == 5)[0][0] / precisionincrease, np.where(dummy2 == 6)[0][0] / precisionincrease,
				 np.where(dummy2 == 3)[0][0] / precisionincrease])).astype(int)

	foillx = min(foilxrot)
	foilrx = max(foilxrot)
	foilhorizwpixel = foilrx - foillx
	foildw = min(foilyrot)
	foilup = max(foilyrot)
	foilvertwpixel = foilup - foildw

	datarot = rotate(data, foilrotdeg, axes=(-1, -2))
	datacrop = datarot[:, :, foildw:foilup, foillx:foilrx]

	return datacrop


#####################################################################################################

def sensitivities_matrix_averaging_foil_pixels(sensitivities,h_pixels,pixelmean_h,pixelmean_v):

	# Created 27/02/2019
	# Does the average of a homogeneous sensitivity matrix on foil pixels
	# It can return a non homogeneous foil, and loose resolution due to the residual of h_pixels/pixelmean_h and v_pixels/pixelmean_v
	#
	#


	shapeorig=np.shape(sensitivities)
	npixels=shapeorig[0]
	nvoxels=shapeorig[1]

	v_pixels=npixels//h_pixels

	h_end_pixels = h_pixels//pixelmean_h
	v_end_pixels = v_pixels // pixelmean_v

	print('the final number of pixels is (vertical,horizontal)'+str([v_end_pixels,h_end_pixels]))
	print('the loss of pixels is (vertical,horizontal)'+str([v_pixels-v_end_pixels*pixelmean_v,h_pixels-h_end_pixels*pixelmean_h]))

	h_shift = (h_pixels-h_end_pixels*pixelmean_h)//2
	v_shift = (v_pixels-v_end_pixels*pixelmean_v)//2

	sensitivities_averaged=[]
	for voxel in range(nvoxels):
		voxel_sensitivity=[]
		foil_image=np.array(split_fixed_length(sensitivities[:,voxel],h_pixels))
		for i in range(v_end_pixels):
			for j in range(h_end_pixels):
				voxel_sensitivity.append(np.mean(foil_image[v_shift+i*pixelmean_v:v_shift+(i+1)*pixelmean_v,h_shift+j*pixelmean_h:h_shift+(j+1)*pixelmean_h]))
		sensitivities_averaged.append(voxel_sensitivity)

	sensitivities_averaged=np.array(sensitivities_averaged)

	return sensitivities_averaged.T


#####################################################################################################

def sensitivities_matrix_averaging_foil_pixels_extra(sensitivities,h_pixels,pixelmean_h):

	# Created 27/02/2019
	# Does the average of a homogeneous sensitivity matrix on foil pixels
	# It can loose resolution due to the residual of h_pixels/pixelmean_h and v_pixels/pixelmean_v
	#
	# Additionally from sensitivities_matrix_averaging_foil_pixels this function calculate also the pixels obtained from shifting the area of the average, one by one
	# the additional constrain is that the foil grid will be homogeneous


	pixelmean_v=pixelmean_h
	shapeorig=np.shape(sensitivities)
	npixels=shapeorig[0]
	nvoxels=shapeorig[1]

	v_pixels=npixels//h_pixels

	h_end_pixels = h_pixels//pixelmean_h
	v_end_pixels = v_pixels // pixelmean_v

	print('the final number of pixels is (vertical,horizontal)'+str([v_end_pixels,h_end_pixels]))
	print('the loss of pixels is (vertical,horizontal)' + str([v_pixels - v_end_pixels * pixelmean_v, h_pixels - h_end_pixels * pixelmean_h]))
	print('but the sensityvity matrix generated will be sized '+str([nvoxels,v_end_pixels*h_end_pixels+(v_end_pixels-1)*(h_end_pixels-1)*(pixelmean_h-1)]))
	print('instead of (vertical,horizontal)'+str([nvoxels,v_end_pixels*h_end_pixels]))

	h_shift = (h_pixels-h_end_pixels*pixelmean_h)//2
	v_shift = (v_pixels-v_end_pixels*pixelmean_v)//2


	sensitivities_averaged=[]
	for voxel in range(nvoxels):
		voxel_sensitivity=[]
		foil_image=np.array(split_fixed_length(sensitivities[:,voxel],h_pixels))
		for i in range(v_end_pixels):
			for j in range(h_end_pixels):
				voxel_sensitivity.append(np.mean(foil_image[v_shift+i*pixelmean_v:v_shift+(i+1)*pixelmean_v,h_shift+j*pixelmean_h:h_shift+(j+1)*pixelmean_h]))
		sensitivities_averaged.append(voxel_sensitivity)
	intermediate=(np.array(sensitivities_averaged).T).tolist()

	print('check 1')
	print('intermediate matrix size is ' + str(np.shape(intermediate)))


	for extra_shift in range(1,pixelmean_h,1):
		sensitivities_averaged = []
		for voxel in range(nvoxels):
			voxel_sensitivity=[]
			foil_image=np.array(split_fixed_length(sensitivities[:,voxel],h_pixels))
			for i in range(v_end_pixels-1):
				for j in range(h_end_pixels-1):
					voxel_sensitivity.append(np.mean(foil_image[extra_shift+v_shift+i*pixelmean_v:extra_shift+v_shift+(i+1)*pixelmean_v,extra_shift+h_shift+j*pixelmean_h:extra_shift+h_shift+(j+1)*pixelmean_h]))
			sensitivities_averaged.append(voxel_sensitivity)
		print('check ' + str(extra_shift + 1))
		print('sensitivities_averaged matrix size is ' + str(np.shape(sensitivities_averaged)))
		intermediate.extend((np.array(sensitivities_averaged).T).tolist())

	intermediate=np.array(intermediate)

	print('sensitivity matrix size is '+str(np.shape(intermediate)))

	return intermediate

#####################################################################################################

def foil_measurement_averaging_foil_pixels_extra(d,h_pixels,pixelmean_h):

	# Created 27/02/2019
	# Does the average of a homogeneous sensitivity matrix on foil pixels
	# It can loose resolution due to the residual of h_pixels/pixelmean_h and v_pixels/pixelmean_v
	#
	# Additionally, this function calculate also the pixels obtained from shifting the area of the average, one by one
	# the additional constrain is that the grid foil will be homogeneous




	pixelmean_v=pixelmean_h
	shapeorig=np.shape(d)
	npixels=shapeorig[0]

	v_pixels=npixels//h_pixels

	h_end_pixels = h_pixels//pixelmean_h
	v_end_pixels = v_pixels // pixelmean_v

	print('the final number of pixels is (vertical,horizontal)'+str([v_end_pixels,h_end_pixels]))
	print('but the foil reading matrix generated will be sized '+str([1,v_end_pixels*h_end_pixels+(v_end_pixels-1)*(h_end_pixels-1)*(pixelmean_h-1)]))
	print('the loss of pixels is (vertical,horizontal)'+str([v_pixels-v_end_pixels*pixelmean_v,h_pixels-h_end_pixels*pixelmean_h]))

	h_shift = (h_pixels-h_end_pixels*pixelmean_h)//2
	v_shift = (v_pixels-v_end_pixels*pixelmean_v)//2

	foil_image = np.array(split_fixed_length(d, h_pixels))

	averaged_foil_reading=[]
	for i in range(v_end_pixels):
		for j in range(h_end_pixels):
			averaged_foil_reading.append(np.mean(foil_image[v_shift+i*pixelmean_v:v_shift+(i+1)*pixelmean_v,h_shift+j*pixelmean_h:h_shift+(j+1)*pixelmean_h]))



	for extra_shift in range(1,pixelmean_h,1):
		for i in range(v_end_pixels-1):
			for j in range(h_end_pixels-1):
				averaged_foil_reading.append(np.mean(foil_image[extra_shift+v_shift+i*pixelmean_v:extra_shift+v_shift+(i+1)*pixelmean_v,extra_shift+h_shift+j*pixelmean_h:extra_shift+h_shift+(j+1)*pixelmean_h]))

	averaged_foil_reading=np.array(averaged_foil_reading)

	print('power on the foil array size is '+str(np.shape(averaged_foil_reading)))

	return averaged_foil_reading



#####################################################################################################

def sensitivities_matrix_averaging_foil_pixels_loseless(sensitivities,h_pixels,pixelmean_h,pixelmean_v):

	# Created 28/03/2019
	# Does the average of a homogeneous sensitivity matrix on foil pixels
	# Different from sensitivities_matrix_averaging_foil_pixels because I do it loseless. I do not discard the last pixels
	#
	#


	shapeorig=np.shape(sensitivities)
	npixels=shapeorig[0]
	nvoxels=shapeorig[1]

	v_pixels=npixels//h_pixels

	h_end_pixels = np.ceil(h_pixels/pixelmean_h).astype('int')
	v_end_pixels = np.ceil(v_pixels/pixelmean_v).astype('int')
	pixels_averaged = pixelmean_h*pixelmean_v

	print('the final number of pixels is (vertical,horizontal)'+str([v_end_pixels,h_end_pixels]))

	sensitivities_averaged=[]
	for voxel in range(nvoxels):
		voxel_sensitivity=[]
		foil_image=np.array(split_fixed_length(sensitivities[:,voxel],h_pixels))
		for i in range(v_end_pixels):
			for j in range(h_end_pixels):
				voxel_sensitivity.append(np.sum(foil_image[i*pixelmean_v:(i+1)*pixelmean_v,j*pixelmean_h:(j+1)*pixelmean_h])/pixels_averaged)
		sensitivities_averaged.append(voxel_sensitivity)

	sensitivities_averaged=np.array(sensitivities_averaged)


	return sensitivities_averaged.T


#####################################################################################################

def foil_measurement_averaging_foil_pixels_loseless(d,h_pixels,pixelmean_h,pixelmean_v):

	# Created 01/04/2019
	# Does the average of a regular sensitivity matrix on foil pixels
	#


	shapeorig=np.shape(d)
	npixels=shapeorig[0]

	v_pixels=npixels//h_pixels

	h_end_pixels = np.ceil(h_pixels/pixelmean_h).astype('int')
	v_end_pixels = np.ceil(v_pixels/pixelmean_v).astype('int')
	pixels_averaged = pixelmean_h*pixelmean_v

	print('the final number of pixels is (vertical,horizontal)'+str([v_end_pixels,h_end_pixels]))

	foil_image = np.array(split_fixed_length(d, h_pixels))

	averaged_foil_reading=[]
	for i in range(v_end_pixels):
		for j in range(h_end_pixels):
			averaged_foil_reading.append(np.sum(foil_image[i*pixelmean_v:(i+1)*pixelmean_v,j*pixelmean_h:(j+1)*pixelmean_h])/pixels_averaged)


	averaged_foil_reading=np.array(averaged_foil_reading)

	print('power on the foil array size is '+str(np.shape(averaged_foil_reading)))

	return averaged_foil_reading


#####################################################################################################

def sensitivities_matrix_averaging_foil_pixels_extra_loseless(sensitivities,h_pixels,pixelmean_h,pixelmean_v):

	# Created 30/03/2019
	# Does the average of a regular sensitivity matrix on foil pixels
	# Additionally from sensitivities_matrix_averaging_foil_pixels this function calculate also the pixels obtained from shifting the area of the average, one by one
	#
	# Different from sensitivities_matrix_averaging_foil_pixels_extra because I do it loseless. I do not discard the last pixels
	#
	#


	shapeorig=np.shape(sensitivities)
	npixels=shapeorig[0]
	nvoxels=shapeorig[1]

	v_pixels=npixels//h_pixels

	h_end_pixels = np.ceil(h_pixels/pixelmean_h).astype('int')
	v_end_pixels = np.ceil(v_pixels/pixelmean_v).astype('int')
	pixels_averaged = pixelmean_h*pixelmean_v

	print('the final number of pixels is (vertical,horizontal)'+str([v_end_pixels,h_end_pixels]))
	print('but the sensityvity matrix generated will be sized '+str([nvoxels,v_end_pixels*h_end_pixels+(v_end_pixels-1)*(h_end_pixels-1)*(pixelmean_h-1)*(pixelmean_v-1)]))
	print('instead of (vertical,horizontal)'+str([nvoxels,v_end_pixels*h_end_pixels]))


	sensitivities_averaged=[]
	for voxel in range(nvoxels):
		voxel_sensitivity=[]
		foil_image=np.array(split_fixed_length(sensitivities[:,voxel],h_pixels))
		for i in range(v_end_pixels):
			for j in range(h_end_pixels):
				voxel_sensitivity.append(np.sum(foil_image[i*pixelmean_v:(i+1)*pixelmean_v,j*pixelmean_h:(j+1)*pixelmean_h])/pixels_averaged)
		sensitivities_averaged.append(voxel_sensitivity)
	intermediate=(np.array(sensitivities_averaged).T).tolist()

	print('check 1')
	print('intermediate matrix size is ' + str(np.shape(intermediate)))

	for extra_v_shift in range(0, pixelmean_v, 1):
		for extra_h_shift in range(0,pixelmean_h,1):
			if (extra_v_shift+extra_v_shift)==0:
				continue
			sensitivities_averaged = []
			for voxel in range(nvoxels):
				voxel_sensitivity=[]
				foil_image=np.array(split_fixed_length(sensitivities[:,voxel],h_pixels))
				if extra_v_shift==0:
					for i in range(v_end_pixels):
						for j in range(h_end_pixels-1):
							voxel_sensitivity.append(np.sum(foil_image[extra_v_shift+i*pixelmean_v:extra_v_shift+(i+1)*pixelmean_v,extra_h_shift+j*pixelmean_h:extra_h_shift+(j+1)*pixelmean_h])/pixels_averaged)
				elif extra_h_shift == 0:
					for i in range(v_end_pixels-1):
						for j in range(h_end_pixels):
							voxel_sensitivity.append(np.sum(foil_image[extra_v_shift+i*pixelmean_v:extra_v_shift+(i+1)*pixelmean_v,extra_h_shift+j*pixelmean_h:extra_h_shift+(j+1)*pixelmean_h])/pixels_averaged)
				else:
					for i in range(v_end_pixels-1):
						for j in range(h_end_pixels-1):
							voxel_sensitivity.append(np.sum(foil_image[extra_v_shift+i*pixelmean_v:extra_v_shift+(i+1)*pixelmean_v,extra_h_shift+j*pixelmean_h:extra_h_shift+(j+1)*pixelmean_h])/pixels_averaged)
				sensitivities_averaged.append(voxel_sensitivity)
			print('check ' + str(extra_v_shift + extra_h_shift + 1))
			print('sensitivities_averaged matrix size is ' + str(np.shape(sensitivities_averaged)))
			intermediate.extend((np.array(sensitivities_averaged).T).tolist())

	intermediate=np.array(intermediate)

	print('sensitivity matrix size is '+str(np.shape(intermediate)))

	return intermediate

#####################################################################################################

def foil_measurement_averaging_foil_pixels_extra_loseless(d,h_pixels,pixelmean_h,pixelmean_v):

	# Created 30/03/2019
	# Does the average of a rgular sensitivity matrix on foil pixels
	#
	# Additionally, this function calculate also the pixels obtained from shifting the area of the average, one by one



	shapeorig=np.shape(d)
	npixels=shapeorig[0]

	v_pixels=npixels//h_pixels

	h_end_pixels = np.ceil(h_pixels/pixelmean_h).astype('int')
	v_end_pixels = np.ceil(v_pixels/pixelmean_v).astype('int')
	pixels_averaged = pixelmean_h*pixelmean_v

	print('the final number of pixels is (vertical,horizontal)'+str([v_end_pixels,h_end_pixels]))
	print('but the foil reading matrix generated will be sized '+str([1,v_end_pixels*h_end_pixels+(v_end_pixels-1)*(h_end_pixels-1)*(pixelmean_h-1)*(pixelmean_v-1)]))

	foil_image = np.array(split_fixed_length(d, h_pixels))

	averaged_foil_reading=[]
	for i in range(v_end_pixels):
		for j in range(h_end_pixels):
			averaged_foil_reading.append(np.sum(foil_image[i*pixelmean_v:(i+1)*pixelmean_v,j*pixelmean_h:(j+1)*pixelmean_h])/pixels_averaged)

	for extra_v_shift in range(0, pixelmean_v, 1):
		for extra_h_shift in range(0,pixelmean_h,1):
			if (extra_v_shift+extra_v_shift)==0:
				continue
			if extra_v_shift == 0:
				for i in range(v_end_pixels):
					for j in range(h_end_pixels-1):
						averaged_foil_reading.append(np.sum(foil_image[extra_v_shift+i*pixelmean_v:extra_v_shift+(i+1)*pixelmean_v,extra_h_shift+j*pixelmean_h:extra_h_shift+(j+1)*pixelmean_h])/pixels_averaged)
			elif extra_h_shift == 0:
				for i in range(v_end_pixels-1):
					for j in range(h_end_pixels):
						averaged_foil_reading.append(np.sum(foil_image[extra_v_shift+i*pixelmean_v:extra_v_shift+(i+1)*pixelmean_v,extra_h_shift+j*pixelmean_h:extra_h_shift+(j+1)*pixelmean_h])/pixels_averaged)
			else:
				for i in range(v_end_pixels-1):
					for j in range(h_end_pixels-1):
						averaged_foil_reading.append(np.sum(foil_image[extra_v_shift+i*pixelmean_v:extra_v_shift+(i+1)*pixelmean_v,extra_h_shift+j*pixelmean_h:extra_h_shift+(j+1)*pixelmean_h])/pixels_averaged)

	averaged_foil_reading=np.array(averaged_foil_reading)

	print('power on the foil array size is '+str(np.shape(averaged_foil_reading)))

	return averaged_foil_reading




#####################################################################################################

def record_binning(data,time_averaging,spatial_averaging_h,spatial_averaging_v,flag,plot_info=False):

	if np.shape(flag) != np.shape(data[0,0]):
		print('ERROR\nThe shape of the flags and record is not equal\nflag is ('+str(np.shape(flag))+') while record is ('+str(np.shape(data))+')')
		exit()

	pixel_t, pixel_h, pixel_v = np.shape(data[0])
	pixel_h_new = np.floor(pixel_h / spatial_averaging_h).astype('int')
	pixel_v_new = np.floor(pixel_v / spatial_averaging_v).astype('int')
	pixel_t_new = np.floor(pixel_t / time_averaging).astype('int')
	print('record will be binned to (txhxv) ' + str(pixel_t_new) + 'x' + str(pixel_h_new) + 'x' + str(pixel_v_new) + ' pixels')

	# if (time_averaging==1 and spatial_averaging_h==1 and spatial_averaging_v==1) :
	# 	return data

	binning_h = np.abs(np.linspace(0, pixel_h - 1, pixel_h) // spatial_averaging_h).astype('int')
	binning_h[binning_h > pixel_h_new - 1] = pixel_h_new - 1
	binning_v = np.abs(np.linspace(0, pixel_v - 1, pixel_v) // spatial_averaging_v).astype('int')
	binning_v[binning_v > pixel_v_new - 1] = pixel_v_new - 1
	binning_t = np.abs(np.linspace(0, pixel_t - 1, pixel_t) // time_averaging).astype('int')
	binning_t[binning_t > pixel_t_new - 1] = pixel_t_new - 1
	# pixels_to_bin = np.array([[(((np.ones((pixel_v, pixel_h)) * binning_h).T + binning_v * pixel_h_new).astype('int')).tolist()]*len(data_to_check[0])])
	pixels_to_bin = ((np.ones((pixel_h, pixel_v)) * binning_v).T + binning_h * pixel_v_new).T.astype('int')

	# temp = np.zeros((len(data[0]),np.max(binning_h) + 1, np.max(binning_v) + 1))
	temp = np.zeros((len(data[0]), np.max(pixels_to_bin) + 1))
	for bin_index in range(np.max(pixels_to_bin) + 1):
		temp[:,bin_index] = np.mean(data[0,:,np.logical_and(pixels_to_bin == bin_index,flag==0)], axis=0)
	# data_to_check_not_binned = copy.deepcopy(data)
	data_spatially_binned = temp.reshape((len(temp), pixel_h_new,pixel_v_new))


	pixels_to_bin = binning_t.astype('int')

	pixels_to_bin = binning_t.astype('int')
	temp = np.zeros((np.max(pixels_to_bin) + 1, *np.shape(data_spatially_binned[0])))
	for bin_index in range(np.max(pixels_to_bin) + 1):
		temp[bin_index] = np.mean(data_spatially_binned[pixels_to_bin == bin_index], axis=0)
	# data_to_check_not_binned = copy.deepcopy(data_to_check)
	# data_binned = cp.deepcopy(temp)

	return np.array([temp])



###################################################################################################


def replace_dead_pixels_2(data_, framerate='auto',from_data=True):

	import copy as cp
	# Created 14/10/2019
	# function replace dead pixels with the average from their neighbours
	# different from# replace_dead_pixels because here you don't use a flag, but the dead pixels are the one that are nan

	# if from_data==False:
	# 	if framerate=='auto':
	# 		print('if you load from file you must specify the framerate')
	# 		exit()
	# 	path=data_
	# 	print('Loading a recod from '+path)
	# 	filenames = all_file_names(path, '.npy')[0]
	# 	data_ = np.load(os.path.join(path, filenames))
	# 	data2 = clear_oscillation_central2(data_, framerate, plot_conparison=False)
	# 	data_ = np.array(data2)
	# else:
	# 	data_ = np.array(data_)


	if np.sum(np.isnan(data_))==0:
		print('there are no "not a number" pixels')
		return data_
	else:
		pixel_t, pixel_h, pixel_v = np.shape(data_[0])
		while  np.sum(np.isnan(data_))!=0:
			sample = cp.deepcopy(data_[0,0])
			while np.sum(np.isnan(sample))!=0:
				where_is = np.isnan(sample).argmax()
				i = int(np.floor(where_is/pixel_v))
				j = int(where_is-i*pixel_v)
				positions = np.array([[i-1,j-1],[i,j-1],[i+1,j-1],[i-1,j],[i,j],[i+1,j],[i-1,j+1],[i,j+1],[i+1,j+1]]).astype('int')
				temp = []
				for index in range(len(positions)):
					if (positions[index][0]>=0 and positions[index][1]>=0 and positions[index,0]<pixel_h and positions[index,1]<pixel_v) :
						# print(str(positions[index])+' is good')
						temp.append(data_[0,:,positions[index][0],positions[index][1]])
				sample[i, j] = 0
				if np.sum(np.isfinite(temp))>0:
					# print('fixing ' + str([i, j]))
					data_[0,:,i,j] = np.nanmean(temp,axis=(-1,-2))
		return data_


###################################################################################################


def log_log_fit_derivative(x,y):
	# 22/10/2019 calculates the derivative of a function by fitting its log-log plot and coming back to the original coordinates
	import numpy as np
	from scipy.interpolate import splrep, splev

	x=np.array(x)
	y = np.array(y)

	x_hat = np.log(x)
	y_hat = np.log(y)

	f_hat = splrep(x_hat,y_hat,s=0)

	f_derivative = np.exp(y_hat)*splev(x_hat,f_hat,der=1)*(1/x)

	return f_derivative

####################################################################################################

# set of functions that allow to extract all information I need directly from the .ats file of the FLIR camera

def hex8_to_int(hex):
	temp = hex[-2:]+hex[4:6]+hex[2:4]+hex[0:2]
	return int(temp,16)

def hex16_to_int(hex):
	temp = hex[-2:]+hex[-4:-2]+hex[-6:-4]+hex[-8:-6]+hex[-10:-8]+hex[-12:-10]+hex[-14:-12]+hex[-16:-14]
	return int(temp,16)

def hex16_to_int_a(hex):
	temp = hex[-14:-12]+hex[-16:-14]+hex[-10:-8]+hex[-12:-10]+hex[-6:-4]+hex[-8:-6]+hex[-2:]+hex[-4:-2]
	return int(temp,16)

def hex20_to_hex20_unflipped(hex):
	temp = hex[-18:-16]+hex[-20:-18]+hex[-14:-12]+hex[-16:-14]+hex[-10:-8]+hex[-12:-10]+hex[-6:-4]+hex[-8:-6]+hex[-2:]+hex[-4:-2]
	return temp

def hex8_to_int_a(hex):
	temp = hex[-6:-4]+hex[-8:-6]+hex[-2:]+hex[-4:-2]
	return int(temp,16)

def hex4_to_float_a(hex):
	import struct
	temp = hex[-2:]+hex[0:2]
	return int(temp,16)/100

def hex4_to_int(hex):
	temp = hex[-2:]+hex[0:2]
	return int(temp,16)

def hex2_to_int(hex):
	temp = hex
	return int(temp,16)

def hex8_to_float(hex):
	import struct
	temp = hex[-2:]+hex[4:6]+hex[2:4]+hex[0:2]
	return struct.unpack('!f', bytes.fromhex(temp))[0]

def hex16_to_float(hex):
	import struct
	temp = hex[-2:]+hex[-4:-2]+hex[-6:-4]+hex[-8:-6]+hex[-10:-8]+hex[-12:-10]+hex[-14:-12]+hex[-16:-14]
	int_value = struct.unpack('!Q', bytes.fromhex(temp))[0]
	float_value = struct.unpack('d', struct.pack('Q', int_value))[0]
	return float_value

def FLIR_record_header_decomposition(header):
	# There is certainly a field for the filter used, but I don't have files with different filters on to compare.

	# 06/06/23 edit: This does not work for modernt .ats files, I try to make universal
	SourceInfo_hex = ('SourceInfo'.encode('utf-8')).hex()	# I try to use "SourceInfo" as a universal marker
	SourceInfo_position = header.find(SourceInfo_hex)

	# camera_type = bytearray.fromhex(header[678:724]).decode()
	camera_type = bytearray.fromhex(header[SourceInfo_position+178:SourceInfo_position+224]).decode()
	camera_type = camera_type.replace('\x00','')
	# width = hex4_to_int(header[814:818])
	width = hex4_to_int(header[SourceInfo_position+314:SourceInfo_position+318])
	# height = hex4_to_int(header[818:822])
	height = hex4_to_int(header[SourceInfo_position+318:SourceInfo_position+322])
	# camera_SN = int(bytearray.fromhex(header[1034:1050]).decode())
	camera_SN = int((bytearray.fromhex(header[SourceInfo_position+534:SourceInfo_position+550]).decode()).replace('\x00',''))
	# lens = bytearray.fromhex(header[1066:1074]).decode()
	lens = bytearray.fromhex(header[SourceInfo_position+566:SourceInfo_position+574]).decode()
	out = dict([('camera_type',camera_type),('width',width),('height',height),('camera_SN',camera_SN),('lens',lens)])
	try:
		IntegrationTime = hex16_to_float(header[SourceInfo_position+2204:SourceInfo_position+2220])
		FrameRate = hex8_to_float(header[SourceInfo_position+2222:SourceInfo_position+2222+8])
		out = dict([('camera_type',camera_type),('width',width),('height',height),('camera_SN',camera_SN),('lens',lens),('IntegrationTime',IntegrationTime),('FrameRate',FrameRate)])
		IntegrationTime_position = header[SourceInfo_position+2204-100:SourceInfo_position+2220+500].find('0000000000000000000001')+SourceInfo_position+2204-100 + len('0000000000000000000001')
		if header[IntegrationTime_position+100:IntegrationTime_position+300].find('0000000000000000000001')!=-1:
			IntegrationTime_position_next = header[IntegrationTime_position+100:IntegrationTime_position+300].find('0000000000000000000001')+IntegrationTime_position+100 + len('0000000000000000000001')
			out['settings_table'] = dict([])
			out['settings_table']['0'] = dict([])
			out['settings_table']['0']['IntegrationTime'] = IntegrationTime
			out['settings_table']['0']['FrameRate'] = FrameRate
			out['settings_table']['1'] = dict([])
			out['settings_table']['1']['IntegrationTime'] = hex16_to_float(header[IntegrationTime_position_next:IntegrationTime_position_next+16])
			out['settings_table']['1']['FrameRate'] = hex8_to_float(header[IntegrationTime_position_next+16+2:IntegrationTime_position_next+16+2+8])
		if header[IntegrationTime_position_next+100:IntegrationTime_position_next+300].find('0000000000000000000001')!=-1:
			IntegrationTime_position_next = header[IntegrationTime_position_next+100:IntegrationTime_position_next+300].find('0000000000000000000001')+IntegrationTime_position_next+100 + len('0000000000000000000001')
			out['settings_table']['2'] = dict([])
			out['settings_table']['2']['IntegrationTime'] = hex16_to_float(header[IntegrationTime_position_next:IntegrationTime_position_next+16])
			out['settings_table']['2']['FrameRate'] = hex8_to_float(header[IntegrationTime_position_next+16+2:IntegrationTime_position_next+16+2+8])
		if header[IntegrationTime_position_next+100:IntegrationTime_position_next+300].find('0000000000000000000001')!=-1:
			IntegrationTime_position_next = header[IntegrationTime_position_next+100:IntegrationTime_position_next+300].find('0000000000000000000001')+IntegrationTime_position_next+100 + len('0000000000000000000001')
			out['settings_table']['3'] = dict([])
			out['settings_table']['3']['IntegrationTime'] = hex16_to_float(header[IntegrationTime_position_next:IntegrationTime_position_next+16])
			out['settings_table']['3']['FrameRate'] = hex8_to_float(header[IntegrationTime_position_next+16+2:IntegrationTime_position_next+16+2+8])
	except:
		pass
	return out

def FLIR_frame_header_decomposition(header):
	def return_requested_output(request):
		if request=='time':
			return hex16_to_int(header[16:32])	# microseconds
		elif request=='frame_counter':
			return hex8_to_int(header[32:40])
		elif request=='NUCpresetUsed':
			return hex4_to_int(header[60:64])	# ID
		elif request=='DetectorTemp':
			return hex8_to_float(header[64:72])	# K
		elif request=='SensorTemp_0':
			return hex8_to_float(header[72:80])	# K
		elif request=='SensorTemp_1':
			return hex8_to_float(header[80:88])	# K
		elif request=='SensorTemp_2':
			return hex8_to_float(header[88:96])	# K
		elif request=='SensorTemp_3':
			return hex8_to_float(header[96:104])	# K
		elif request=='MasterClock':
			return hex8_to_float(header[104:112])	# unknown
		elif request=='IntegrationTime':
			return hex8_to_float(header[112:120])	# ns
		elif request=='FrameRate':
			return hex8_to_float(header[120:128])	# Hz
		elif request=='ExternalTrigger':
			return int(header[130:132])
		elif request=='PageIndex':
			# return hex8_to_int(header[134:142])	# index of the digitiser used # wrong, this was valod only in external clocks
			return int(header[134:136])	# index of the digitiser used
		elif request=='ClockType':
			return int(header[140:142])	# 1=internal clock, 0=external clock
	return return_requested_output

def FLIR_research_studio_frame_header_decomposition(header,length_header_marker):
	def return_requested_output(request):
		if request=='time':
			return hex8_to_int_a(header[4:20-8])*1e6 + hex8_to_int_a(header[4+8:20])	# microseconds. it could be a true timestamp or the time since startup, I dunno
			# return hex16_to_int_a(header[4:20])	# microseconds. it could be a true timestamp or the time since startup, I dunno
			# return header[4:20]

		if request=='Preset':
			return hex2_to_int(header[20:20+2])%4
		if request=='GigENucOn':
			if hex2_to_int(header[20:20+2])>=84:
				return True
			else:
				return False

		elif request=='FrontPanelTemp':
			return hex4_to_float_a(header[24:24+4])	# K
		elif request=='AirGapTemp':
			return hex4_to_float_a(header[28:28+4])	# K
		elif request=='InternalTemp':
			return hex4_to_float_a(header[32:32+4])	# K
		elif request=='FPATemp':
			return hex4_to_float_a(header[36:36+4])	# K
		elif request=='frame_counter':
			return hex8_to_int(header[40:40+8])
		elif request=='frame_number':
			return hex8_to_int(header[40+212*2+length_header_marker:40+8+212*2+length_header_marker])

		elif request=='FilterID':
			return hex4_to_int(header[60:60+4])
		elif request=='frame_within_record':
			return hex4_to_int(header[64:64+4])


		elif request=='FPAControlWord':
			return hex20_to_hex20_unflipped(header[68:68+20])
		elif request=='SaturationThreshold':
			return hex4_to_int(header[88:88+4])
		elif request=='SaturatedPixels':
			return hex8_to_int(header[92:92+8])



		elif request=='ROICType':
			return hex2_to_int(header[118:118+2])
		elif request=='CameraType':
			return hex2_to_int(header[120:120+2])
		elif request=='CameraSubType':
			return hex2_to_int(header[122:122+2])
		elif request=='AnalogOffset':
			return hex4_to_float_a(header[124:124+4])/10	# don't know
		elif request=='MainBoardTemp':
			return hex4_to_float_a(header[128:128+4])	# K
		elif request=='PowerBoardTemp':
			return hex4_to_float_a(header[132:132+4])	# K
		elif request=='DigitizerBoardTemp':
			return hex4_to_float_a(header[136:136+4])	# K
		elif request=='TempScalingFactor_0C':
			return hex4_to_float_a(header[140:140+4])	# K

		elif request=='LockinPhase':
			return hex8_to_int(header[156:156+8])
		elif request=='LockinPeriod':
			return hex8_to_int(header[164:164+8])
		elif request=='LockinValue':
			return hex8_to_int(header[172:172+8])
		elif request=='ADResolution':
			return hex4_to_int(header[180:180+4])
		elif request=='CameraSN':
			return hex4_to_int(header[184:184+4])

		# elif request=='NUCpresetUsed':
		# 	return hex4_to_int(header[60:64])	# ID
		# this is stuff that there was before, but there isn't now, or better I was not able to identify
		# elif request=='SensorTemp_1':
		# 	return hex8_to_float(header[80:88])	# K
		# elif request=='SensorTemp_2':
		# 	return hex8_to_float(header[88:96])	# K
		# elif request=='SensorTemp_3':
		# 	return hex8_to_float(header[96:104])	# K
		# elif request=='MasterClock':
		# 	return hex8_to_float(header[104:112])	# unknown
		# elif request=='IntegrationTime':
		# 	return hex8_to_float(header[112:120])	# ns
		# elif request=='FrameRate':
		# 	return hex8_to_float(header[120:128])	# Hz
		# elif request=='ExternalTrigger':
		# 	return int(header[130:132])
		# elif request=='PageIndex':
		# 	# return hex8_to_int(header[134:142])	# index of the digitiser used # wrong, this was valod only in external clocks
		# 	return int(header[134:136])	# index of the digitiser used
		# elif request=='ClockType':
		# 	return int(header[140:142])	# 1=internal clock, 0=external clock
	return return_requested_output



def raw_to_image(raw_digital_level,width,height,digital_level_bytes):
	import textwrap
	pixels = width*height
	# raw_digital_level_splitted = textwrap.wrap(raw_digital_level, 4)
	# iterator=map(hex4_to_int,raw_digital_level_splitted)
	# return np.flip(np.array(list(iterator)).reshape((height,width)),axis=0)
	# start=tm.time()
	if False:
		counts_digital_level = []
		for i in range(pixels):
			counts_digital_level.append(hex4_to_int(raw_digital_level[i*digital_level_bytes:(i+1)*digital_level_bytes]))
		counts_digital_level = np.flip(np.array(counts_digital_level).astype(int).reshape((height,width)),axis=0)
	else:	# ~20% faster method
		# swapped_hex_string = re.sub(r"(\w{2})(\w{2})", r"\2\1", raw_digital_level)
		# Split into 4-character chunks and convert each chunk to decimal in one line
		counts_digital_level = list(map(lambda x: int(x[2:] + x[:2], 16), (raw_digital_level[i:i+4] for i in range(0, len(raw_digital_level), 4))))
		counts_digital_level = np.flip(np.reshape(counts_digital_level,(height,width)),axis=0).astype(int)
	# print(tm.time()-start)

	if np.nanmax(np.abs(counts_digital_level))<2**8/2-1:
		counts_digital_level = counts_digital_level.astype(np.int8)
	elif np.nanmax(np.abs(counts_digital_level))<2**16/2-1:
		counts_digital_level = counts_digital_level.astype(np.int16)
	elif np.nanmax(np.abs(counts_digital_level))<2**32/2-1:
		counts_digital_level = counts_digital_level.astype(np.int32)
	return counts_digital_level


def ats_to_dict(full_path,digital_level_bytes=4,header_marker = '49494d536d'):	# instead of header_marker = '4949'. WARNING: only 4949 is the real marker. the rest is part of a timestamp but I need it to disambiguate in some edge cases
	data = open(full_path,'rb').read()
	hexdata = data.hex()
	# raw_for_digitizer = b'\x18K\x00\x00zD\x00\x00 A\x00\x00\x00'
	header_length = 142
	main_header = FLIR_record_header_decomposition(hexdata)
	width = main_header['width']
	height = main_header['height']
	camera_SN = main_header['camera_SN']
	# digital_level_bytes = 4
	data_length = width*height*digital_level_bytes
	digitizer_ID = []
	data = []
	time_of_measurement = []
	frame_counter = []
	DetectorTemp = []
	SensorTemp_0 = []
	SensorTemp_3 = []
	Preset = []
	# last = 0
	if hexdata.find(header_marker)!=-1 and hexdata.find('0000'+header_marker)==-1:	# I add the second condition because sometimes 4949 is part of some time/temperature thing in some of the frame headers
		# header_marker = '4949'
		length_header_marker = len(header_marker)
		# raw_pointer_after_string = 11 + len(hex_for_digitizer)
		# hex_pointer_after_string = 15 + length_header_marker
		present_header_marker_position = hexdata.find(header_marker)
		next_header_marker_position = hexdata[present_header_marker_position+length_header_marker+header_length:].find(header_marker)+header_length
		standard_researchIR_file = True
		marker_after_theframe_data = True
	elif hexdata.find('420043004400450046004700480049004a004b004c004d004e004f0050005100520053005400550056005700'):	# 2024/10/24 it seems that .ats files generated with FLIR research studio have this header BEFORE the camera data
		header_marker = '420043004400450046004700480049004a004b004c004d004e004f0050005100520053005400550056005700'
		# reduction = 160
		# if hexdata.find(header_marker+'580059005a005b005c005d005e005f00'+'00000000ffffffff00000000aaaaaaaa55555555ffffffff00000000ffffffff010002000400080010002000400080')!=-1:
		# 	reduction = 32
		# 	# header_marker = header_marker+'580059005a005b005c005d005e005f00'+'00000000ffffffff00000000aaaaaaaa55555555ffffffff00000000ffffffff010002000400080010002000400080'
		pos = [0]
		# gna2=['']
		gna = 0
		# while len(pos)<50:
		while gna!=-1:
			gna = hexdata[pos[-1]+len(header_marker):pos[-1]+2000+data_length].find(header_marker)
			pos.append(gna + pos[-1]+len(header_marker))
			# gna2.append(hexdata[pos[-1]-244:pos[-1]-244+4])
		# for i in range(len(pos)):
		# 	print(hexdata[pos[i]:pos[i]+len(header_marker)]==hexdata[pos[i+1]:pos[i+1]+len(header_marker)])
		pos = pos[1:-1]
		temp = np.diff(pos)==min(np.diff(pos))
		repetition = np.median(np.diff(np.cumsum(temp)[np.logical_not(temp)])).astype(int) +1
		spacing = min(np.diff(pos)[1:])
		length_header_marker = len(header_marker)
		present_header_start = hexdata.find(header_marker) - 264
		if True:
			# let's calculate exactly the length of the header
			present_data_start = cp.deepcopy(present_header_start)
			for i in range(repetition):
				# print(present_data_start)
				temp = hexdata[present_data_start+10:present_data_start+10+length_header_marker*10].find(header_marker)
				if temp!=-1:
					present_data_start = temp+present_data_start+10
				# print(present_data_start)
			present_data_start += length_header_marker
			if hexdata[present_data_start:present_data_start+length_header_marker*10].find('580059005a005b005c005d005e005f00')!=-1:	# just in case
				present_data_start += hexdata[present_data_start:present_data_start+length_header_marker*10].find('580059005a005b005c005d005e005f00') + len('580059005a005b005c005d005e005f00')
			if hexdata[present_data_start:present_data_start+length_header_marker*10].find('00000000ffffffff00000000aaaaaaaa55555555ffffffff00000000ffffffff010002000400080010002000400080')!=-1:
				present_data_start += hexdata[present_data_start:present_data_start+length_header_marker*10].find('00000000ffffffff00000000aaaaaaaa55555555ffffffff00000000ffffffff010002000400080010002000400080') + len('00000000ffffffff00000000aaaaaaaa55555555ffffffff00000000ffffffff010002000400080010002000400080')
			full_frame_header_length = present_data_start-present_header_start
		else:
			present_header_marker_position = hexdata[present_header_start:present_header_start+length_header_marker*10].find(header_marker)+present_header_start
			# I have to add to it a standard length, as the header is present 3 times
			present_header_marker_position = present_header_marker_position+(repetition-1)*spacing
		next_header_start = hexdata[present_data_start+data_length:present_data_start+data_length*2].find(header_marker) +present_data_start+data_length - 264
		if False:
			next_header_marker_position = hexdata[next_header_start:next_header_start+length_header_marker*10].find(header_marker)+next_header_start
			next_header_marker_position = next_header_marker_position+(repetition-1)*spacing
		else:
			next_data_start = next_header_start+full_frame_header_length
		marker_after_theframe_data = False
	else:	# 2024/10/24 I can't remember now what this is for. it should be for the .ats files generated with flir research studio, but I found a better system for that
		ObjectParameters_hex = ('ObjectParameters'.encode('utf-8')).hex()	# I try to use "SourceInfo" as a universal marker
		ObjectParameters_position = hexdata.find(ObjectParameters_hex)
		Pixels_hex = ('Pixels'.encode('utf-8')).hex()	# I try to use "SourceInfo" as a universal marker
		Pixels_position = hexdata[ObjectParameters_position:].find(Pixels_hex)
		gna = ObjectParameters_position + Pixels_position + hexdata[ObjectParameters_position + Pixels_position:].find('0000ffffffffffffffff0000') + 200
		for i_ in range(1000):
			# print(hexdata[gna+i_*2:gna+i_*2+2])
			if hexdata[gna+i_*2:gna+i_*2+2] != '00':
				break
		real_header_marker = hexdata[gna+2*i_:gna+2*i_+6]
		length_header_marker = len(real_header_marker)
		# hex_pointer_after_string = 15 + length_header_marker
		position = [100,100]
		found = 5	# hexdata[position[-1]+2:].find(real_header_marker)
		while found!=-1:
			if len(position)<10:	# use a large range for the first ones
				found = hexdata[position[-1]+width*height*digital_level_bytes:position[-1]+width*height*digital_level_bytes+100000000+np.diff(position).max()].find(real_header_marker)
			else:
				found = hexdata[position[-1]+width*height*digital_level_bytes:position[-1]+width*height*digital_level_bytes+np.diff(position).max()].find(real_header_marker)
			position.append(found+width*height*digital_level_bytes+position[-1])
		position = np.array(position[2:-1])
		min_dist_between_frames = (np.unique(np.diff(position))[np.unique(np.diff(position))>width*height*digital_level_bytes]).min()
		position = position[1:][np.diff(position)>=min_dist_between_frames]
		present_header_marker_position = np.min(position)
		next_header_marker_position = np.min(position[position > present_header_marker_position]) - present_header_marker_position
		standard_researchIR_file = False
		marker_after_theframe_data = True


	# import time as tm
	# value = data.find(string_for_digitizer)
	# value = hexdata.find(header_marker)
	# last+=value+len(hex_for_digitizer)	# the first one is part of the neader of the whole file
	# value = hexdata[last:].find(hex_for_digitizer)
	# while len(hexdata)-last>header_length:
	if marker_after_theframe_data:
		max_value = 0
		while True:
			# start_time = tm.time()
			# print(len(time_of_measurement))
			# header = hexdata[last+value+length_header_marker:last+value+header_length+length_header_marker]
			if next_header_marker_position!=header_length-1:
				header_length = next_header_marker_position-data_length
			header_ = hexdata[present_header_marker_position+length_header_marker:present_header_marker_position+length_header_marker+header_length]
			# print(header[134:136])
			# FLIR_frame_header_decomposition_failed = False
			try:
				if FLIR_frame_header_decomposition_failed:
					sbla=ssdas  # I want it to skip this section
				header = FLIR_frame_header_decomposition(header_)

				if len(time_of_measurement)<=1:	# the spacing between separators seems constant, and take very long, so I do it once
					# value = hexdata[last:].find(header_marker)
					IntegrationTime = header('IntegrationTime')
					FrameRate = header('FrameRate')
					ExternalTrigger = header('ExternalTrigger')
					NUCpresetUsed = header('NUCpresetUsed')

				digitizer_ID.append(header('PageIndex'))
				time_of_measurement.append(header('time'))	# time in microseconds from camera startup
				frame_counter.append(header('frame_counter'))
				DetectorTemp.append(header('DetectorTemp'))
				SensorTemp_0.append(header('SensorTemp_0'))
				SensorTemp_3.append(header('SensorTemp_3'))


			except:
				FLIR_frame_header_decomposition_failed = True
				digitizer_ID.append(0)
				time_of_measurement.append(0)	# time in microseconds from camera startup
				frame_counter.append(0)
				DetectorTemp.append(0)
				SensorTemp_0.append(0)
				SensorTemp_3.append(0)

				if len(time_of_measurement)<=1:	# the spacing between separators seems constant, and take very long, so I do it once
					IntegrationTime = main_header['IntegrationTime'] * 1000	# in microseconds
					FrameRate = main_header['FrameRate']
					ExternalTrigger = 0
					NUCpresetUsed = 0
				print('frame header reading fail')
				pass

			# time_lapsed = tm.time()-start_time
			# print(time_lapsed)
			# raw_digital_level = hexdata[last+value-data_length:last+value]
			raw_digital_level = hexdata[present_header_marker_position-data_length:present_header_marker_position]
			# time_lapsed = tm.time()-start_time-time_lapsed
			# print(time_lapsed)
			# start=tm.time()
			data_int = raw_to_image(raw_digital_level,width,height,digital_level_bytes)
			max_value_int = np.nanmax(data_int)
			max_value = max(max_value,max_value_int)
			data.append(data_int)
			# print(tm.time()-start)
			# time_lapsed = tm.time()-start_time-time_lapsed
			# print(time_lapsed)
			# last+=value+header_length+data_length
			if standard_researchIR_file:
				if (hexdata.find(header_marker)!=-1):
					if next_header_marker_position==header_length-1:
						break
					present_header_marker_position += next_header_marker_position + length_header_marker
					next_header_marker_position = hexdata[present_header_marker_position+length_header_marker+header_length:present_header_marker_position+length_header_marker+header_length+data_length*2].find(header_marker)+header_length
			else:
				if present_header_marker_position >= np.nanmax(position):
					break
				present_header_marker_position = np.min(position[position > present_header_marker_position])
				try:
					next_header_marker_position = np.min(position[position > present_header_marker_position]) - present_header_marker_position
				except:
					next_header_marker_position = present_header_marker_position + min_dist_between_frames
	else:	# procedure for files created with FLIR research studio
		try:	# this is to differenciate between multiple and sngle settings
			test = main_header['settings_table']
		except:
			pass	# I still need these quantities for later
		FrameRate = main_header['FrameRate']
		IntegrationTime = main_header['IntegrationTime']
		ExternalTrigger = 0	# I still did not identify where this is
		max_value = 0
		FLIR_frame_header_decomposition_failed = False
		while True:
			# start_time = tm.time()
			# print(len(time_of_measurement))
			# header = hexdata[last+value+length_header_marker:last+value+header_length+length_header_marker]
			if False:
				header_ = hexdata[present_header_start:present_header_start+(repetition)*spacing-reduction]
			else:
				header_ = hexdata[present_header_start:present_header_start+full_frame_header_length]
			# print(header[134:136])
			# FLIR_frame_header_decomposition_failed = False
			header = FLIR_research_studio_frame_header_decomposition(header_,length_header_marker)

			if False:	# I did not identify this yet at the moment
				IntegrationTime = header('IntegrationTime')
				FrameRate = header('FrameRate')
				ExternalTrigger = header('ExternalTrigger')	# 2024/11/12 this would be very usefull but still unknown
				# NUCpresetUsed = header('NUCpresetUsed')

				digitizer_ID.append(header('PageIndex'))
			else:
				digitizer_ID.append(0)
			NUCpresetUsed = not header('GigENucOn')	# I thin't think that this is the real equivalent to NUCpresetUsed, but it is a flag I can use to distinguish NUC and not NUC cases
			Preset.append(header('Preset'))
			time_of_measurement.append(header('time'))	# time in microseconds from camera startup
			frame_counter.append(header('frame_counter'))
			DetectorTemp.append(header('FPATemp'))
			SensorTemp_0.append(header('FrontPanelTemp'))
			SensorTemp_3.append(header('InternalTemp'))
			if False:
				raw_digital_level = hexdata[present_header_start+(repetition)*spacing-reduction+32:present_header_start+(repetition)*spacing-reduction+data_length+32]	# no idea why the +32, but it seems to be necessary
			else:
				raw_digital_level = hexdata[present_data_start+32:present_data_start+data_length+32]
			data_int = raw_to_image(raw_digital_level,width,height,digital_level_bytes)
			max_value_int = np.nanmax(data_int)
			max_value = max(max_value,max_value_int)
			data.append(data_int)

			if hexdata[present_data_start+data_length:present_data_start+data_length*2].find(header_marker)!=-1:
				if False:
					present_header_start = hexdata[present_header_marker_position+data_length:present_header_marker_position+data_length*2].find(header_marker) +present_header_marker_position+data_length - 264
					present_header_marker_position = hexdata[present_header_start:present_header_start+length_header_marker*10].find(header_marker)+present_header_start
					present_header_marker_position = present_header_marker_position+(repetition-1)*spacing
				else:
					present_header_start = hexdata[present_data_start+data_length:present_data_start+data_length*2].find(header_marker) +present_data_start+data_length - 264
					present_data_start = present_header_start+full_frame_header_length
			else:
				break
		if len(np.unique(Preset))==1:	# for some reason in IRVB-MASTU_shot-50429.ats there is only 1 marker, and it is "d", therefore
			Preset = np.zeros_like(Preset).astype(int)

		# print(str(present_header_marker_position)+' / '+str(next_header_marker_position))
		# print(value)
	print('number of frames '+str(len(data)))
	#	 section added to actually find the digitizer and order accordingly
	# I do this because in reality also ResearchIR does not orders the digitizers
	if FrameRate>60 and FrameRate<400 and not(FLIR_frame_header_decomposition_failed):	# I can't see dead pixels in this configuration, this means that calibrations and related measurements can be done only at 383Hz!!. also when the windowing is strong, it is possible no dead pixel is in view
		for treshold_for_bad_difference in [50,45,40,35,30,35,20]:
			dark_bad_pixels_marker = identify_dead_pixel_marker(data,FrameRate,treshold_for_bad_difference=treshold_for_bad_difference)
			if len(dark_bad_pixels_marker[0])>0 and len(dark_bad_pixels_marker[1])>0:
				break
		print('treshold_for_bad_difference '+str(treshold_for_bad_difference))
		digitizer_ID,discarded_frames = generate_digitizer_ID_from_dead_pixels_markers(data,dark_bad_pixels_marker)
		digitizer_ID_order_type = 'less dead pixels digitizer zero'
	elif FrameRate==0 or FLIR_frame_header_decomposition_failed:
		dark_bad_pixels_marker = None
		# digitizer_ID = digitizer_ID_fake
		discarded_frames = []
		digitizer_ID_order_type = 'no ordering possible because no framerate information'
		treshold_for_bad_difference = 0
	elif not marker_after_theframe_data:	# I will assume that in this mode all bad pixels are handled by the camera itself
		dark_bad_pixels_marker = None
		# digitizer_ID = digitizer_ID_fakef
		discarded_frames = []
		digitizer_ID_order_type = 'no ordering done as not needed with the X6980 camera'
		treshold_for_bad_difference = 0
	else:
		dark_bad_pixels_marker = None
		# digitizer_ID = digitizer_ID_fakef
		discarded_frames = []
		digitizer_ID_order_type = 'no ordering possible because low framerate'
		treshold_for_bad_difference = 0
	if np.nanmax(data)<2**15-1:
		data = np.array(data,dtype=np.int16)
	elif np.nanmax(data)<2**31-1:
		data = np.array(data,dtype=np.int32)
	else:
		data = np.array(data,dtype=int)
	data_median = int(np.median(data))
	temp = data-data_median
	if np.abs(temp).max()<2**8/2-1:
		data_minus_median = (temp).astype(np.int8)
		data_type = np.int8
	elif np.abs(temp).max()<2**16/2-1:
		data_minus_median = (temp).astype(np.int16)
		data_type = np.int16
	elif np.abs(temp).max()<2**32/2-1:
		data_minus_median = (temp).astype(np.int32)
		data_type = np.int32
	digitizer_ID = np.array(digitizer_ID)
	time_of_measurement = np.array(time_of_measurement)
	frame_counter = np.array(frame_counter)
	DetectorTemp = np.array(DetectorTemp)
	SensorTemp_0 = np.array(SensorTemp_0)
	SensorTemp_3 = np.array(SensorTemp_3)
	out = dict([])
	# out['data'] = data
	out['data_median'] = data_median
	out['data'] = data_minus_median	# I do this to save memory, also because the change in counts in a single recording is always small
	out['digitizer_ID']=digitizer_ID
	out['time_of_measurement']=time_of_measurement
	out['ExternalTrigger'] = ExternalTrigger
	out['NUCpresetUsed'] = NUCpresetUsed
	out['Preset'] = Preset
	try:	# this is necessary for the recordings with multiple settings
		out['settings_table'] = main_header['settings_table']
	except:
		out['IntegrationTime']=IntegrationTime
		out['FrameRate']=FrameRate
	out['SensorTemp_0'] = SensorTemp_0
	out['SensorTemp_3'] = SensorTemp_3
	out['DetectorTemp'] = DetectorTemp
	out['width'] = width
	out['height'] = height
	out['camera_SN'] = camera_SN
	out['frame_counter'] = frame_counter
	data_per_digitizer,uniques_digitizer_ID = separate_data_with_digitizer(out)
	out['data_time_avg_counts'] = np.array([(np.mean(data,axis=0)) for data in data_per_digitizer])
	out['data_time_avg_counts_std'] = np.array([(np.std(data,axis=0)) for data in data_per_digitizer])
	out['data_time_space_avg_counts'] = np.array([(np.mean(data,axis=(0,1,2))) for data in data_per_digitizer])
	out['data_time_space_avg_counts_std'] = np.array([(np.std(data,axis=(0,1,2))) for data in data_per_digitizer])
	out['uniques_digitizer_ID'] = uniques_digitizer_ID
	# return data,digitizer_ID,time_of_measurement,IntegrationTime,FrameRate,ExternalTrigger,SensorTemp_0,DetectorTemp,width,height,camera_SN,frame_counter
	out['discarded_frames'] = discarded_frames
	out['dead_pixels_markers'] = dark_bad_pixels_marker
	out['digitizer_ID_order_type'] = digitizer_ID_order_type
	out['treshold_for_bad_difference'] = treshold_for_bad_difference
	out['data_type'] = data_type
	return out

def ptw_to_dict(full_path,max_time_s = np.inf):
	os.chdir('/home/ffederic/work/Collaboratory/test/experimental_data/functions')
	import pyradi.ryptw as ryptw
	import datetime
	from scipy.ndimage import median_filter
	import collections

	header = ryptw.readPTWHeader(full_path)
	width = header.h_Cols
	height = header.h_Rows
	camera_SN = header.h_CameraSerialNumber
	NUCpresetUsed = header.h_NucTable
	FrameRate = 1/header.h_CEDIPAquisitionPeriod # Hz
	IntegrationTime = round(header.h_CEDIPIntegrationTime*1e6,0) # microseconds
	ExternalTrigger = None	# I couldn't find this signal in the header

	digitizer_ID_fake = []
	data = []
	time_of_measurement = []
	frame_counter = []
	DetectorTemp = []
	SensorTemp_0 = []
	SensorTemp_3 = []
	AtmosphereTemp = []
	last = 0
	for i in range(header.h_firstframe,header.h_lastframe+1):
		frame = ryptw.getPTWFrames(header, [i])[0][0]
		if i == header.h_firstframe:
			new_shape = np.shape(frame.T)
		frame_header = ryptw.getPTWFrames(header, [i])[1][0]
		# # 2021-12-09 I checked a few recordings and this seems to actually be true, the "dead" pixels flip in a very regular and consistent manner
		# yyyy = frame_header.h_FileSaveYear
		# mm = frame_header.h_FileSaveMonth
		# dd = frame_header.h_FileSaveDay
		hh = frame_header.h_frameHour
		minutes = frame_header.h_frameMinute
		ss_sss = frame_header.h_frameSecond
		temp = datetime.datetime(frame_header.h_FileSaveYear-30,frame_header.h_FileSaveMonth,frame_header.h_FileSaveDay).timestamp() + hh*60*60 + minutes*60 + ss_sss
		if len(time_of_measurement)>=1:
			if temp-time_of_measurement[0]>max_time_s:
				continue
		if int(sys.version[:5].replace('.',''))<390:
			data.append(np.flip(frame.flatten().reshape(new_shape).T,axis=0).astype(int))	# this no longer works with the new version of python (3.9)
		else:
			data.append(np.flip(frame,axis=0).astype(int))
		digitizer_ID_fake.append(frame_header.h_framepointer%2)	# I couldn't find this so as a proxy, given the digitisers are always alternated, I only use if the frame is even or odd
		time_of_measurement.append(temp*1e6)	# I leave if as a true timestamp, t=0 is 1970
		frame_counter.append(frame_header.h_framepointer)	# this is not the same as the other file, as this is a local counter within this record, not commong among all records
		DetectorTemp.append(frame_header.h_detectorTemp)
		SensorTemp_0.append(frame_header.h_detectorTemp)	# this data is missing so I use the more similar again
		SensorTemp_3.append(frame_header.h_sensorTemp4)
		AtmosphereTemp.append(frame_header.h_AtmosphereTemp)	# additional data not present in the .ats format

	print('number of frames '+str(len(data)))

	#	 section added to actually find the digitizer and order accordingly
	if FrameRate>60 and FrameRate<400:	# I can't see dead pixels in this configuration, this means that calibrations and related measurements can be done only at 383Hz!!. also when the windowing is strong, it is possible no dead pixel is in view
		for treshold_for_bad_difference in [50,45,40,35,30,35,20]:
			dark_bad_pixels_marker = identify_dead_pixel_marker(data,FrameRate,treshold_for_bad_difference=treshold_for_bad_difference)
			if len(dark_bad_pixels_marker[0])>0 and len(dark_bad_pixels_marker[1])>0:
				break
		print('treshold_for_bad_difference '+str(treshold_for_bad_difference))
		digitizer_ID,discarded_frames = generate_digitizer_ID_from_dead_pixels_markers(data,dark_bad_pixels_marker)
		digitizer_ID_order_type = 'less dead pixels digitizer zero'
	else:
		dark_bad_pixels_marker = []
		digitizer_ID = digitizer_ID_fake
		discarded_frames = []
		digitizer_ID_order_type = 'no ordering possible because low framerate'
		treshold_for_bad_difference = 0
	# back to the normal process
	if np.nanmax(data)<2**15-1:
		data = np.array(data,dtype=np.int16)
	elif np.nanmax(data)<2**31-1:
		data = np.array(data,dtype=np.int32)
	else:
		data = np.array(data,dtype=int)
	data_median = int(np.median(data))
	if np.abs(data-data_median).max()<2**8/2-1:
		data_minus_median = (data-data_median).astype(np.int8)
		data_type = np.int8
	elif np.abs(data-data_median).max()<2**16/2-1:
		data_minus_median = (data-data_median).astype(np.int16)
		data_type = np.int16
	elif np.abs(data-data_median).max()<2**32/2-1:
		data_minus_median = (data-data_median).astype(np.int32)
		data_type = np.int32
	digitizer_ID = np.array(digitizer_ID)
	time_of_measurement = np.array(time_of_measurement)
	frame_counter = np.array(frame_counter)
	DetectorTemp = np.array(DetectorTemp)
	SensorTemp_0 = np.array(SensorTemp_0)
	SensorTemp_3 = np.array(SensorTemp_3)
	AtmosphereTemp = np.array(AtmosphereTemp)
	out = dict([])
	# out['data'] = data
	out['data_median'] = data_median
	out['data'] = data_minus_median	# I do this to save memory, also because the change in counts in a single recording is always small
	out['digitizer_ID']=digitizer_ID
	out['time_of_measurement']=time_of_measurement
	out['IntegrationTime']=IntegrationTime
	out['FrameRate']=FrameRate
	out['ExternalTrigger'] = ExternalTrigger
	out['NUCpresetUsed'] = NUCpresetUsed
	out['SensorTemp_0'] = SensorTemp_0
	out['SensorTemp_3'] = SensorTemp_3
	out['AtmosphereTemp'] = AtmosphereTemp
	out['DetectorTemp'] = DetectorTemp
	out['width'] = width
	out['height'] = height
	out['camera_SN'] = camera_SN
	out['frame_counter'] = frame_counter
	data_per_digitizer,uniques_digitizer_ID = separate_data_with_digitizer(out)
	out['data_time_avg_counts'] = np.array([(np.mean(data,axis=0)) for data in data_per_digitizer])
	out['data_time_avg_counts_std'] = np.array([(np.std(data,axis=0)) for data in data_per_digitizer])
	out['data_time_space_avg_counts'] = np.array([(np.mean(data,axis=(0,1,2))) for data in data_per_digitizer])
	out['data_time_space_avg_counts_std'] = np.array([(np.std(data,axis=(0,1,2))) for data in data_per_digitizer])
	out['uniques_digitizer_ID'] = uniques_digitizer_ID
	out['discarded_frames'] = discarded_frames
	if len(dark_bad_pixels_marker)==2:
		for i in range(len(dark_bad_pixels_marker)):
			out['dead_pixels_markers_'+str(i)] = dark_bad_pixels_marker[i]
	else:
		out['dead_pixels_markers'] = dark_bad_pixels_marker
	out['digitizer_ID_order_type'] = digitizer_ID_order_type
	out['treshold_for_bad_difference'] = treshold_for_bad_difference
	# return data,digitizer_ID,time_of_measurement,IntegrationTime,FrameRate,ExternalTrigger,SensorTemp_0,DetectorTemp,width,height,camera_SN,frame_counter
	return out

def identify_dead_pixel_marker(data,FrameRate,treshold_for_bad_difference=50):
	from scipy.ndimage import median_filter
	import collections
	bad_pixels_marker = []
	for i in range(int(min(1*FrameRate,len(data)/5))):
		bad_pixels_flag = find_dead_pixels_data_acquisition_stage(data[i]-data[i+1],treshold_for_bad_difference=treshold_for_bad_difference,verbose=1).flatten()
		bad_pixels_marker.extend(np.arange(len(bad_pixels_flag))[bad_pixels_flag>0])
	bad_pixels_marker = np.unique(bad_pixels_marker)
	shape = np.shape(data[0])
	bad_pixels_marker_2 = []
	bad_pixels_marker_str = []
	for i in range(int(min(1*FrameRate,len(data)/5))):
		temp = []
		median = median_filter(data[i],footprint=[[1,1,1],[1,0,1],[1,1,1]])
		for i_ in bad_pixels_marker:
			i__ = np.unravel_index(i_,shape)
			temp.append(np.abs(data[i][i__]-median[i__]))
		bad_pixels_marker_2.append(bad_pixels_marker[np.array(temp)>25])
		bad_pixels_marker_str.append(str(bad_pixels_marker_2[-1]))
	# bad_pixels_marker_2 = np.array(bad_pixels_marker_2)
	counter = collections.Counter(bad_pixels_marker_str)
	most_common_str = [counter.most_common()[0][0],counter.most_common()[1][0]]
	dark_bad_pixels_marker = [bad_pixels_marker_2[(np.array(bad_pixels_marker_str)==most_common_str[0]).argmax()],bad_pixels_marker_2[(np.array(bad_pixels_marker_str)==most_common_str[1]).argmax()]]
	temp=0
	while len(dark_bad_pixels_marker[0])*0.8 < np.sum([value in dark_bad_pixels_marker[1] for value in dark_bad_pixels_marker[0]]):	# safety clause in case by mistake it is used twice the same pattern of dewad pixels for both digitizers
		most_common_str = [counter.most_common()[0][0],counter.most_common()[temp+1+1][0]]
		dark_bad_pixels_marker = [bad_pixels_marker_2[(np.array(bad_pixels_marker_str)==most_common_str[0]).argmax()],bad_pixels_marker_2[(np.array(bad_pixels_marker_str)==most_common_str[1]).argmax()]]
		temp+=1
		print('pattern of dead pixels shifted of '+str(temp))
	# I arbitrarily decide that digitizer_ID=0 is the one with less dead pixels
	if len(dark_bad_pixels_marker[0]) > len(dark_bad_pixels_marker[1]):
		dark_bad_pixels_marker.reverse()

	temp0 = []
	for value in dark_bad_pixels_marker[0]:
		if value in dark_bad_pixels_marker[1]:
			continue
		else:
			temp0.append(value)
	temp1 = []
	for value in dark_bad_pixels_marker[1]:
		if value in dark_bad_pixels_marker[0]:
			continue
		else:
			temp1.append(value)
	dark_bad_pixels_marker = [np.array(temp0),np.array(temp1)]

	print('dead pixel markers:\n'+str(dark_bad_pixels_marker[0])+'\n'+str(dark_bad_pixels_marker[1]))
	return dark_bad_pixels_marker

def generate_digitizer_ID_from_dead_pixels_markers(data,dark_bad_pixels_marker):
	from scipy.ndimage import median_filter
	digitizer_ID = []
	discarded_frames = []
	shape = np.shape(data[0])
	for i in range(len(data)):
		median = median_filter(data[i],footprint=[[1,1,1],[1,0,1],[1,1,1]])
		bad_pixels_marker = [[] , []]
		for i_ in range(len(dark_bad_pixels_marker)):
			for i__ in dark_bad_pixels_marker[i_]:
				i__ = np.unravel_index(i__,shape)
				bad_pixels_marker[i_].append(np.abs(data[i][i__]-median[i__]))
		for i_ in range(len(dark_bad_pixels_marker)):
			bad_pixels_marker[i_] = np.mean(bad_pixels_marker[i_])
		if bad_pixels_marker[0]>bad_pixels_marker[1]:
			digitizer_ID.append(0)
		elif bad_pixels_marker[1]>bad_pixels_marker[0]:
			digitizer_ID.append(1)
		else:
			print('frame n'+str(i)+' discarded')
			discarded_frames.append(i)
	if len(discarded_frames)>0:
		print('discarded frames are '+str(discarded_frames))
		if np.sum(np.array(discarded_frames)>10)>0:
			print('error, this should not have happened')
			exit()
		else:	# it can happen that the first few frames are messed up
			digitizer_ID = np.flip(digitizer_ID,axis=0).tolist()
			len_discarded_frames = len(discarded_frames)
			while len_discarded_frames>0:
				if digitizer_ID[-1] == 0:
					digitizer_ID.append(1)
				else:
					digitizer_ID.append(0)
				len_discarded_frames -=1
			digitizer_ID = np.flip(digitizer_ID,axis=0).tolist()
	if np.sum(np.diff(digitizer_ID)==0)>0:
		print('digitizer inversion at frames '+str(np.arange(len(data)-1)[np.diff(digitizer_ID)==0]))
	return digitizer_ID,discarded_frames


def separate_data_with_digitizer(full_saved_file_dict):
	try:
		data_median = full_saved_file_dict['data_median']
		data = full_saved_file_dict['data']
		data = data.astype(int)
		data += data_median
	except:
		data = full_saved_file_dict['data']
	digitizer_ID = full_saved_file_dict['digitizer_ID']
	uniques_digitizer_ID = np.sort(np.unique(digitizer_ID))
	data_per_digitizer = []
	for ID in uniques_digitizer_ID:
		data_per_digitizer.append(data[digitizer_ID==ID])
	return data_per_digitizer,uniques_digitizer_ID

def generic_separate_with_digitizer(data,digitizer_ID):
	uniques_digitizer_ID = np.sort(np.unique(digitizer_ID))
	data_per_digitizer = []
	for ID in uniques_digitizer_ID:
		data_per_digitizer.append(data[digitizer_ID==ID])
	return data_per_digitizer,uniques_digitizer_ID

def read_IR_file(file,force_regeneration=False):
	# if os.path.exists(file+'.npz'):
	# 	full_saved_file_dict=np.load(file+'.npz')
	# else:
	try:	# test if the digitizer was identified
		full_saved_file_dict=np.load(file+'.npz')
		full_saved_file_dict.allow_pickle = True
		trash = full_saved_file_dict['digitizer_ID_order_type']
		if force_regeneration:
			bla=sblu	# I wont this to generate an error to regenerate the .npz file from scratch
	except Exception as e:
		print('Error '+str(e))
		print(file+'.npz generating')
		if os.path.exists(file+'.ats'):
			print('reading '+file+'.ats')
			full_saved_file_dict = ats_to_dict(file+'.ats')
		elif os.path.exists(file+'.ptw'):
			print('reading '+file+'.ptw')
			full_saved_file_dict = ptw_to_dict(file+'.ptw')
		elif os.path.exists(file+'/'+os.path.basename(file)+'_001.asc') or os.path.exists(file+'/'+os.path.basename(file)+'_0001.asc'):
			full_saved_file_dict = Infratec_to_dict(file)
		print(file+'.npz generated')
		np.savez_compressed(file,**full_saved_file_dict)
	return full_saved_file_dict

def Infratec_to_dict(full_path):	# instead of header_marker = '4949'
	import re
	filenames = all_file_names(full_path,'asc')
	numbers = []
	for filename in filenames:
		numbers.append(re.findall(r'\d+', filename)[-1])
		if len(numbers)==1:
			root = filename[:filename.find(numbers[0]+'.asc')]
			number_length = len(numbers[0])
	numbers = np.array(numbers,dtype=int)
	numbers = np.sort(numbers)

	camera_SN = 0	# not available in the .asc file
	FrameRate = 0	# not available in the .asc file
	IntegrationTime = 0	# not available in the .asc file
	ExternalTrigger = 0	# not available in the .asc file
	NUCpresetUsed = 0	# not available in the .asc file
	# digital_level_bytes = 4
	# data_length = width*height*digital_level_bytes
	digitizer_ID = []
	data = []
	time_of_measurement = []
	frame_counter = []
	DetectorTemp = []
	SensorTemp_0 = []
	SensorTemp_3 = []

	max_value = 0
	for number in numbers:
		raw_digital_level = []
		with open(full_path+'/'+root+str(number).zfill(number_length)+'.asc', 'r') as file:
			# all_lines = file.read().splitlines()
			start_next=False
			for line in file:
				# line = line.strip()
				if line == '[Data]\n' and start_next==False:
					start_next = True
					continue
				elif line[:10] == 'ImageWidth':
					width = int(re.search(r'\d+', line).group())
					continue
				elif line[:11] == 'ImageHeight':
					height = int(re.search(r'\d+', line).group())
					continue
				elif start_next==False:
					continue
				raw_digital_level.append(line.split(',00\t')[:-1])

		max_value = np.nanmax(np.abs(np.array(raw_digital_level,dtype=int)))
		if max_value<2**8/2-1:
			data_int = np.array(raw_digital_level,dtype=np.int8)
		elif max_value<2**16/2-1:
			data_int = np.array(raw_digital_level,dtype=np.int16)
		elif max_value<2**32/2-1:
			data_int = np.array(raw_digital_level,dtype=np.int32)
		else:
			data_int = np.array(raw_digital_level,dtype=int)
		max_value_int = np.nanmax(data_int)
		max_value = max(max_value,max_value_int)
		data.append(data_int)

		digitizer_ID.append(0)
		time_of_measurement.append(0)	# time in microseconds from camera startup
		frame_counter.append(0)
		DetectorTemp.append(0)
		SensorTemp_0.append(0)
		SensorTemp_3.append(0)



		# print(str(present_header_marker_position)+' / '+str(next_header_marker_position))
		# print(value)
	print('number of frames '+str(len(data)))
	#	 section added to actually find the digitizer and order accordingly
	# I do this because in reality also ResearchIR does not orders the digitizers
	if FrameRate>60 and FrameRate<400:	# I can't see dead pixels in this configuration, this means that calibrations and related measurements can be done only at 383Hz!!. also when the windowing is strong, it is possible no dead pixel is in view
		for treshold_for_bad_difference in [50,45,40,35,30,35,20]:
			dark_bad_pixels_marker = identify_dead_pixel_marker(data,FrameRate,treshold_for_bad_difference=treshold_for_bad_difference)
			if len(dark_bad_pixels_marker[0])>0 and len(dark_bad_pixels_marker[1])>0:
				break
		print('treshold_for_bad_difference '+str(treshold_for_bad_difference))
		digitizer_ID,discarded_frames = generate_digitizer_ID_from_dead_pixels_markers(data,dark_bad_pixels_marker)
		digitizer_ID_order_type = 'less dead pixels digitizer zero'
	elif FrameRate==0:
		dark_bad_pixels_marker = None
		# digitizer_ID = digitizer_ID_fake
		discarded_frames = []
		digitizer_ID_order_type = 'no ordering possible because no framerate information'
		treshold_for_bad_difference = 0
	else:
		dark_bad_pixels_marker = None
		# digitizer_ID = digitizer_ID_fakef
		discarded_frames = []
		digitizer_ID_order_type = 'no ordering possible because low framerate'
		treshold_for_bad_difference = 0
	if np.nanmax(data)<2**15-1:
		data = np.array(data,dtype=np.int16)
	elif np.nanmax(data)<2**31-1:
		data = np.array(data,dtype=np.int32)
	else:
		data = np.array(data,dtype=int)
	data_median = int(np.median(data))
	if np.abs(data-data_median).max()<2**8/2-1:
		data_minus_median = (data-data_median).astype(np.int8)
		data_type = np.int8
	elif np.abs(data-data_median).max()<2**16/2-1:
		data_minus_median = (data-data_median).astype(np.int16)
		data_type = np.int16
	elif np.abs(data-data_median).max()<2**32/2-1:
		data_minus_median = (data-data_median).astype(np.int32)
		data_type = np.int32
	digitizer_ID = np.array(digitizer_ID)
	time_of_measurement = np.array(time_of_measurement)
	frame_counter = np.array(frame_counter)
	DetectorTemp = np.array(DetectorTemp)
	SensorTemp_0 = np.array(SensorTemp_0)
	SensorTemp_3 = np.array(SensorTemp_3)
	out = dict([])
	# out['data'] = data
	out['data_median'] = data_median
	out['data'] = data_minus_median	# I do this to save memory, also because the change in counts in a single recording is always small
	out['digitizer_ID']=digitizer_ID
	out['time_of_measurement']=time_of_measurement
	out['IntegrationTime']=IntegrationTime
	out['FrameRate']=FrameRate
	out['ExternalTrigger'] = ExternalTrigger
	out['NUCpresetUsed'] = NUCpresetUsed
	out['SensorTemp_0'] = SensorTemp_0
	out['SensorTemp_3'] = SensorTemp_3
	out['DetectorTemp'] = DetectorTemp
	out['width'] = width
	out['height'] = height
	out['camera_SN'] = camera_SN
	out['frame_counter'] = frame_counter
	data_per_digitizer,uniques_digitizer_ID = separate_data_with_digitizer(out)
	out['data_time_avg_counts'] = np.array([(np.mean(data,axis=0)) for data in data_per_digitizer])
	out['data_time_avg_counts_std'] = np.array([(np.std(data,axis=0)) for data in data_per_digitizer])
	out['data_time_space_avg_counts'] = np.array([(np.mean(data,axis=(0,1,2))) for data in data_per_digitizer])
	out['data_time_space_avg_counts_std'] = np.array([(np.std(data,axis=(0,1,2))) for data in data_per_digitizer])
	out['uniques_digitizer_ID'] = uniques_digitizer_ID
	# return data,digitizer_ID,time_of_measurement,IntegrationTime,FrameRate,ExternalTrigger,SensorTemp_0,DetectorTemp,width,height,camera_SN,frame_counter
	out['discarded_frames'] = discarded_frames
	out['dead_pixels_markers'] = dark_bad_pixels_marker
	out['digitizer_ID_order_type'] = digitizer_ID_order_type
	out['treshold_for_bad_difference'] = treshold_for_bad_difference
	out['data_type'] = data_type
	return out

def build_poly_coeff_multi_digitizer(temperature,files,inttime,pathparam,n):
	# modified 2018-10-08 to build the coefficient only for 1 degree of polinomial
	while np.shape(temperature[0])!=():
		temperature=np.concatenate(temperature)
		files=np.concatenate(files)
	sin_fun = lambda x,A,f,p : A*np.sin(x*f*2*np.pi+p)
	meancounttot=[]
	meancountstdtot=[]
	# all_SensorTemp_0 = []
	# all_DetectorTemp = []
	# all_frame_counter = []
	# all_time_of_measurement = []
	for i_file,file in enumerate(files):
		full_saved_file_dict=read_IR_file(file)
		data_per_digitizer,uniques_digitizer_ID = separate_data_with_digitizer(full_saved_file_dict)
		if i_file==0:
			digitizer_ID = np.array(uniques_digitizer_ID)
		if np.sum(digitizer_ID==uniques_digitizer_ID)<len(digitizer_ID):
			print('ERROR: problem with the ID of the digitizer in \n' + file)
			exit()
		meancounttot.append([np.mean(x,axis=0) for x in data_per_digitizer])
		if False:	# what if I'm exaggerating this because of the oscillation and the baseline drift?
			meancountstdtot.append([np.std(x,axis=0) for x in data_per_digitizer])	# what if I'm exaggerating this because of the oscillation and the baseline drift?
		else:	# this tries to remove the effect of the oscillation. it's marginally (std decrease~1%) better. it is still fast so I can keep it
			a = [np.mean(x,axis=(-1,-2)) for x in data_per_digitizer]
			b = []
			for i in digitizer_ID:
				framerate = float(full_saved_file_dict['FrameRate'])
				time_axis = np.arange(len(a[i]))*2*1 / framerate
				lin_fit = np.polyfit(time_axis,a[i],1)
				baseline = np.polyval(lin_fit,time_axis)
				if framerate>300:	# the oscillation will be present only at high frequency
					bds = [[0,20,-4*np.pi],[np.inf,40,4*np.pi]]
					guess = [1,29,max(-4*np.pi,min(4*np.pi,-np.pi*np.trapz((a[i]-baseline)[time_axis<1/29/2])*2/np.trapz(np.abs(a[i]-baseline)[time_axis<1/29])))]
					fit = curve_fit(sin_fun, time_axis,a[i]-baseline, p0=guess, bounds = bds, maxfev=100000000)
					# plt.figure()
					# plt.plot(time_axis,a[i]-baseline)
					# plt.plot(time_axis,sin_fun(time_axis,*fit[0]))
					# plt.plot(time_axis,sin_fun(time_axis,*guess),'--')
					# plt.pause(0.001)
					b.append((data_per_digitizer[i].T-baseline-sin_fun(time_axis,*fit[0])).T)
				else:
					b.append((data_per_digitizer[i].T-baseline).T)
			meancountstdtot.append([np.std(x,axis=0) for x in b])
		# all_SensorTemp_0.append(np.mean(full_saved_file_dict['SensorTemp_0']))
		# all_DetectorTemp.append(np.mean(full_saved_file_dict['DetectorTemp']))
		# all_time_of_measurement.append(np.mean(full_saved_file_dict['time_of_measurement']))
		# all_frame_counter.append(np.mean(full_saved_file_dict['frame_counter']))

	meancounttot=np.array(meancounttot)
	meancountstdtot=np.array(meancountstdtot)
	shapex=np.shape(meancounttot)[-2]
	shapey=np.shape(meancounttot)[-1]
	score=np.zeros((len(digitizer_ID),shapex,shapey))
	score2=np.zeros((len(digitizer_ID),shapex,shapey))

	# WARNING; THIS CREATE COEFFICIENTS INCOMPATIBLE WITH PREVIOUS build_poly_coeff FUNCTION
	coeff=np.zeros((len(digitizer_ID),shapex,shapey,n))
	errcoeff=np.zeros((len(digitizer_ID),shapex,shapey,n,n))
	coeff2=np.zeros((len(digitizer_ID),shapex,shapey,2))
	errcoeff2=np.zeros((len(digitizer_ID),shapex,shapey,2,2))

	def BB_rad_prob_and_gradient(T_,counts,grads=True):
		def int(arg):
			a1=arg[0]
			a2=arg[1]
			lambda_cam_x = np.linspace(1.5,5.1,10)*1e-6	# m, Range of FLIR SC7500
			lambda_cam = np.array([lambda_cam_x.tolist()]*len(T_)).T
			temp1 = np.trapz(2*scipy.constants.h*(scipy.constants.c**2)/(lambda_cam**5) * 1/( np.exp(scipy.constants.h*scipy.constants.c/(lambda_cam*scipy.constants.k*T_)) -1) ,x=lambda_cam_x,axis=0)
			temp2 = a1*temp1 + a2 - counts
			out = np.sum(temp2**2)
			if grads:
				grad = [np.sum(2*temp2*temp1),np.sum(2*temp2*1)]
				return out,np.array(grad)
			else:
				return out
		return int

	import numdifftools as nd
	def make_standatd_fit_output(function,x_optimal):
		hessian = nd.Hessian(function)
		hessian = hessian(x_optimal)
		covariance = np.linalg.inv(hessian)
		for i in range(len(covariance)):
			covariance[i,i] = np.abs(covariance[i,i])
		fit = [x_optimal,covariance]
		return fit

	BB_rad = lambda T,a1,a2 : a1*2*scipy.constants.h*(scipy.constants.c**2)/((5e-6)**5) * 1/( np.exp(scipy.constants.h*scipy.constants.c/((5e-6)*scipy.constants.k*T)) -1) + a2
	def BB_rad(T_,a1,a2):
		lambda_cam_x = np.linspace(1.5,5.1,10)*1e-6	# m, Range of FLIR SC7500
		lambda_cam = np.array([lambda_cam_x.tolist()]*len(T_)).T
		temp1 = np.trapz(2*scipy.constants.h*(scipy.constants.c**2)/(lambda_cam**5) * 1/( np.exp(scipy.constants.h*scipy.constants.c/(lambda_cam*scipy.constants.k*T_)) -1) ,x=lambda_cam_x,axis=0)
		out = a1*temp1 + a2
		return out

	for j in range(shapex):
		for k in range(shapey):
			for i_z,z in enumerate(digitizer_ID):
				x=np.array(meancounttot[:,z==digitizer_ID,j,k]).flatten()
				xerr=np.array(meancountstdtot[:,z==digitizer_ID,j,k]).flatten()
				# temp1,temp2=np.polyfit(temperature,x,n-1,cov='unscaled')
				temp1=np.polyfit(x,temperature,n-1)	# this correction alone decrease the errors by 2 error by 2 orders of magnitude
				yerr=(np.polyval(temp1,x+xerr)-np.polyval(temp1,x-xerr))/2
				temp1,temp2=np.polyfit(x,temperature,n-1,w=1/yerr,cov='unscaled')
				fit = curve_fit(BB_rad,np.array(temperature)+273,x,sigma=xerr,absolute_sigma=False,p0=[1e4,100])
				# the following is much slower
				# x_optimal, y_opt, opt_info = scipy.optimize.fmin_l_bfgs_b(BB_rad_prob_and_gradient(np.array(temperature)+273,x), x0=[1e4,100], iprint=0, factr=1e2, pgtol=1e-6, maxiter=5000)#,m=1000, maxls=1000, pgtol=1e-10, factr=1e0)#,approx_grad = True)
				# fit = make_standatd_fit_output(BB_rad_prob_and_gradient(np.array(temperature)+273,x,grads=False),x_optimal)
				# plt.figure()
				# plt.errorbar(x,temperature,xerr=xerr,fmt='+')
				# plt.plot(np.sort(x),np.polyval(temp1,np.sort(x)),'--')
				# plt.plot(BB_rad(np.sort(temperature)+273,*fit[0]),np.sort(temperature),':')
				# plt.pause(0.01)
				coeff[i_z,j,k,:]=temp1
				errcoeff[i_z,j,k,:]=temp2
				coeff2[i_z,j,k,:]=fit[0]
				errcoeff2[i_z,j,k,:]=fit[1]
				score[i_z,j,k]=rsquared(temperature,np.polyval(temp1,x))
				score2[i_z,j,k]=rsquared(x,BB_rad(np.array(temperature)+273,*fit[0]))
	np.savez_compressed(os.path.join(pathparam,'coeff_polynomial_deg'+str(n-1)+'int_time'+str(inttime)+'ms'),**dict([('coeff',coeff),('errcoeff',errcoeff),('score',score),('coeff2',coeff2),('errcoeff2',errcoeff2),('score2',score2)]))
	print('for a polinomial of degree '+str(n-1)+' the R^2 score is '+str(np.sum(score[n-2])))

def build_poly_coeff_multi_digitizer_with_no_window_reference(temperature_window,files_window,temperature_no_window,files_no_window,inttime,pathparam,n,wavewlength_top=5.1,wavelength_bottom=1.5):
	# modified 2018-10-08 to build the coefficient only for 1 degree of polinomial
	# modified 2024-07-11 do accommodate also the case in which only the window data is provided, so I can use and optimize this process for all cases
	import collections
	if len(temperature_window)>0:
		while np.shape(temperature_window[0])!=():
			temperature_window=np.concatenate(temperature_window)
			files_window=np.concatenate(files_window)
	temperature_window = np.array(temperature_window)
	files_window = np.array(files_window)
	if len(temperature_no_window)>0:
		while np.shape(temperature_no_window[0])!=():
			temperature_no_window=np.concatenate(temperature_no_window)
			files_no_window=np.concatenate(files_no_window)
	temperature_no_window = np.array(temperature_no_window)
	files_no_window = np.array(files_no_window)

	no_window_data_present = len(files_no_window)>0
	window_data_present = len(files_window)>0

	sin_fun = lambda x,A,f,p : A*np.sin(x*f*2*np.pi+p)
	meancounttot=[]
	meancountstdtot=[]
	dead_pixels_markerstot = [[],[]]
	# all_SensorTemp_0 = []
	# all_DetectorTemp = []
	# all_frame_counter = []
	# all_time_of_measurement = []
	for i_file,file in enumerate(files_window):
		full_saved_file_dict=read_IR_file(file)
		data_per_digitizer,uniques_digitizer_ID = separate_data_with_digitizer(full_saved_file_dict)
		try:
			try:
				dead_pixels_markerstot[0].append(full_saved_file_dict['dead_pixels_markers'][0])
				dead_pixels_markerstot[1].append(full_saved_file_dict['dead_pixels_markers'][1])
			except:
				dead_pixels_markerstot[0].append(full_saved_file_dict['dead_pixels_markers_0'])
				dead_pixels_markerstot[1].append(full_saved_file_dict['dead_pixels_markers_1'])
		except:
			pass
		if i_file==0:
			digitizer_ID = np.array(uniques_digitizer_ID)
		if np.sum(digitizer_ID==uniques_digitizer_ID)<len(digitizer_ID):
			print('ERROR: problem with the ID of the digitizer in \n' + file)
			exit()
		meancounttot.append([np.mean(x,axis=0) for x in data_per_digitizer])
		if False:	# what if I'm exaggerating this because of the oscillation and the baseline drift?
			meancountstdtot.append([np.std(x,axis=0) for x in data_per_digitizer])	# what if I'm exaggerating this because of the oscillation and the baseline drift?
		else:	# this tries to remove the effect of the oscillation. it's marginally (std decrease~1%) better. it is still fast so I can keep it
			a = [np.mean(x,axis=(-1,-2)) for x in data_per_digitizer]
			b = []
			for i in digitizer_ID:
				framerate = float(full_saved_file_dict['FrameRate'])
				time_axis = np.arange(len(a[i]))*2*1 / framerate
				lin_fit = np.polyfit(time_axis,a[i],1)
				baseline = np.polyval(lin_fit,time_axis)
				if framerate>300:	# the oscillation will be present only at high frequency
					bds = [[0,20,-4*np.pi],[np.inf,40,4*np.pi]]
					guess = [1,29,max(-4*np.pi,min(4*np.pi,-np.pi*np.trapz((a[i]-baseline)[time_axis<1/29/2])*2/np.trapz(np.abs(a[i]-baseline)[time_axis<1/29])))]
					fit = curve_fit(sin_fun, time_axis[:10],(a[i]-baseline)[:10], p0=guess, bounds = bds, maxfev=100000000,ftol=1e-15)
					fit = curve_fit(sin_fun, time_axis,(a[i]-baseline), p0=fit[0], bounds = bds, maxfev=100000000,ftol=1e-15)
					# plt.figure()
					# plt.plot(time_axis,a[i]-baseline)
					# plt.plot(time_axis,sin_fun(time_axis,*fit[0]))
					# plt.plot(time_axis,sin_fun(time_axis,*guess),'--')
					# plt.plot(time_axis,a[i]-baseline-sin_fun(time_axis,*fit[0]),':')
					# plt.pause(0.001)
					# sin_fun2 = lambda x,A : A*np.sin(x*fit[0][1]*2*np.pi+fit[0][2])
					# guess=[0]
					# bds=[[-100],[100]]
					# c=median_filter(data,size=[1,5,5])
					# fit2 = curve_fit(sin_fun2, time_axis,c[:,100,100]-baseline-np.mean(c[:,100,100]-baseline), p0=guess, bounds = bds, maxfev=100000000)
					b.append((data_per_digitizer[i].T-baseline-sin_fun(time_axis,*fit[0])).T)
				else:
					b.append((data_per_digitizer[i].T-baseline).T)
			meancountstdtot.append([np.std(x,axis=0) for x in b])
		# all_SensorTemp_0.append(np.mean(full_saved_file_dict['SensorTemp_0']))
		# all_DetectorTemp.append(np.mean(full_saved_file_dict['DetectorTemp']))
		# all_time_of_measurement.append(np.mean(full_saved_file_dict['time_of_measurement']))
		# all_frame_counter.append(np.mean(full_saved_file_dict['frame_counter']))
	meancounttot=np.array(meancounttot)
	meancountstdtot=np.array(meancountstdtot)

	meancounttot_no_window=[]
	meancountstdtot_no_window=[]
	# all_SensorTemp_0 = []
	# all_DetectorTemp = []
	# all_frame_counter = []
	# all_time_of_measurement = []
	for i_file,file in enumerate(files_no_window):
		full_saved_file_dict=read_IR_file(file)
		data_per_digitizer,uniques_digitizer_ID = separate_data_with_digitizer(full_saved_file_dict)
		try:
			try:
				dead_pixels_markerstot[0].append(full_saved_file_dict['dead_pixels_markers'][0])
				dead_pixels_markerstot[1].append(full_saved_file_dict['dead_pixels_markers'][1])
			except:
				dead_pixels_markerstot[0].append(full_saved_file_dict['dead_pixels_markers_0'])
				dead_pixels_markerstot[1].append(full_saved_file_dict['dead_pixels_markers_1'])
		except:
			pass
		if i_file==0:
			digitizer_ID = np.array(uniques_digitizer_ID)
		if np.sum(digitizer_ID==uniques_digitizer_ID)<len(digitizer_ID):
			print('ERROR: problem with the ID of the digitizer in \n' + file)
			exit()
		meancounttot_no_window.append([np.mean(x,axis=0) for x in data_per_digitizer])
		if False:	# what if I'm exaggerating this because of the oscillation and the baseline drift?
			meancountstdtot_no_window.append([np.std(x,axis=0) for x in data_per_digitizer])	# what if I'm exaggerating this because of the oscillation and the baseline drift?
		else:	# this tries to remove the effect of the oscillation. it's marginally (std decrease~1%) better. it is still fast so I can keep it
			a = [np.mean(x,axis=(-1,-2)) for x in data_per_digitizer]
			b = []
			for i in digitizer_ID:
				framerate = float(full_saved_file_dict['FrameRate'])
				time_axis = np.arange(len(a[i]))*2*1 / framerate
				lin_fit = np.polyfit(time_axis,a[i],1)
				baseline = np.polyval(lin_fit,time_axis)
				if framerate>300:	# the oscillation will be present only at high frequency
					bds = [[0,20,-4*np.pi],[np.inf,40,4*np.pi]]
					guess = [1,29,max(-4*np.pi,min(4*np.pi,-np.pi*np.trapz((a[i]-baseline)[time_axis<1/29/2])*2/np.trapz(np.abs(a[i]-baseline)[time_axis<1/29])))]
					fit = curve_fit(sin_fun, time_axis,a[i]-baseline, p0=guess, bounds = bds, maxfev=100000000)
					# plt.figure()
					# plt.plot(time_axis,a[i])
					# plt.plot(time_axis,sin_fun(time_axis,*fit[0])+baseline)
					# plt.plot(time_axis,sin_fun(time_axis,*guess)+baseline,'--')
					# plt.pause(0.001)
					b.append((data_per_digitizer[i].T-baseline-sin_fun(time_axis,*fit[0])).T)
				else:
					b.append((data_per_digitizer[i].T-baseline).T)
			meancountstdtot_no_window.append([np.std(x,axis=0) for x in b])
		# all_SensorTemp_0.append(np.mean(full_saved_file_dict['SensorTemp_0']))
		# all_DetectorTemp.append(np.mean(full_saved_file_dict['DetectorTemp']))
		# all_time_of_measurement.append(np.mean(full_saved_file_dict['time_of_measurement']))
		# all_frame_counter.append(np.mean(full_saved_file_dict['frame_counter']))
	meancounttot_no_window=np.array(meancounttot_no_window)
	meancountstdtot_no_window=np.array(meancountstdtot_no_window)

	if len(dead_pixels_markerstot[0])>0:
		# I want to record what are the dead pixel markers to then apply them to the actual data
		common_dead_pixels_markers = []
		for i_z,z in enumerate(digitizer_ID):
			flat_list = [item for sublist in dead_pixels_markerstot[i_z] for item in sublist]
			counter = collections.Counter(flat_list)
			temp = np.array(counter.most_common())
			if temp[:,1].max()<len(dead_pixels_markerstot[i_z]):
				print("error, one of the order of the digitizers was given flipped or something similar didn't work")
				exit()
			common_dead_pixels_markers.append(temp[temp[:,1] == len(dead_pixels_markerstot[i_z]),0])
	else:
		common_dead_pixels_markers = [[],[]]

	shapex=np.shape(data_per_digitizer[0])[-2]	# changed for the case in which there is only data for no_window
	shapey=np.shape(data_per_digitizer[0])[-1]
	score=np.zeros((len(digitizer_ID),shapex,shapey))
	score2=np.zeros((len(digitizer_ID),shapex,shapey))
	score3=np.zeros((len(digitizer_ID),shapex,shapey))
	score4=np.zeros((len(digitizer_ID),shapex,shapey))

	# WARNING; THIS CREATE COEFFICIENTS INCOMPATIBLE WITH PREVIOUS build_poly_coeff FUNCTION
	coeff=np.zeros((len(digitizer_ID),shapex,shapey,n))
	errcoeff=np.zeros((len(digitizer_ID),shapex,shapey,n,n))
	coeff2=np.zeros((len(digitizer_ID),shapex,shapey,4))
	errcoeff2=np.zeros((len(digitizer_ID),shapex,shapey,4,4))
	coeff3=np.zeros((len(digitizer_ID),shapex,shapey,2))
	errcoeff3=np.zeros((len(digitizer_ID),shapex,shapey,2,2))
	coeff4=np.zeros((len(digitizer_ID),shapex,shapey,2))
	errcoeff4=np.zeros((len(digitizer_ID),shapex,shapey,2,2))

	# lambda_cam_x = np.linspace(1.5,5.1,10)*1e-6	# m, Range of FLIR SC7500
	# lambda_cam_x = np.linspace(wavelength_bottom,wavewlength_top,100)*1e-6	# m, Range of FLIR SC7500
	lambda_cam_x = np.arange(wavelength_bottom,wavewlength_top,0.02)*1e-6	# m, Range of FLIR SC7500
	temperature = temperature_window.tolist() + temperature_no_window.tolist()
	# temperature_range = np.linspace(np.min(temperature),np.max(temperature))

	# temperature_range = np.unique(temperature)
	# photon_flux = []
	# for T_ in temperature_range:
	# 	photon_flux.append(np.trapz(2*scipy.constants.c/(lambda_cam_x**4) * 1/( np.exp(scipy.constants.h*scipy.constants.c/(lambda_cam_x*scipy.constants.k*(T_+273.15))) -1) ,x=lambda_cam_x,axis=0) * inttime/1000)
	# photon_flux = np.array(photon_flux)
	# photon_flux_interpolator = interp1d(temperature_range,photon_flux,bounds_error=False,fill_value='extrapolate')

	photon_dict = calc_interpolators_BB(wavewlength_top=wavewlength_top,wavelength_bottom=wavelength_bottom,inttime=inttime)
	photon_flux_interpolator = photon_dict['photon_flux_interpolator']


	def BB_rad_prob_and_gradient(T_,counts,lambda_cam_x=lambda_cam_x,grads=True):
		def int(arg):
			a1=arg[0]
			a2=arg[1]
			lambda_cam = np.array([lambda_cam_x.tolist()]*len(T_)).T
			# temp1 = np.trapz(2*scipy.constants.c/(lambda_cam**4) * 1/( np.exp(scipy.constants.h*scipy.constants.c/(lambda_cam*scipy.constants.k*(T_+273.15))) -1) ,x=lambda_cam_x,axis=0) * inttime/1000
			temp1 = photon_flux_interpolator(T_)
			temp2 = a1*temp1 + a2 - counts
			out = np.sum(temp2**2)
			if grads:
				grad = [np.sum(2*temp2*temp1),np.sum(2*temp2*1)]
				return out,np.array(grad)
			else:
				return out
		return int

	import numdifftools as nd
	def make_standatd_fit_output(function,x_optimal):
		hessian = nd.Hessian(function)
		hessian = hessian(x_optimal)
		covariance = np.linalg.inv(hessian)
		for i in range(len(covariance)):
			covariance[i,i] = np.abs(covariance[i,i])
		fit = [x_optimal,covariance]
		return fit

	BB_rad = lambda T,a1,a2 : a1*2*scipy.constants.c/((5e-6)**5) * 1/( np.exp(scipy.constants.h*scipy.constants.c/((5e-6)*scipy.constants.k*(T+273.15))) -1) * inttime/1000 + a2

	def BB_rad(number_of_window,lambda_cam_x=lambda_cam_x,emissivity=1):
		def int(T_,a1,a2,a3,a4):
			# lambda_cam_x = np.linspace(1.5,5.1,10)*1e-6	# m, Range of FLIR SC7500
			# lambda_cam = np.array([lambda_cam_x.tolist()]*len(T_)).T
			# temp1 = np.trapz(2*scipy.constants.c/(lambda_cam**4) * 1/( np.exp(scipy.constants.h*scipy.constants.c/(lambda_cam*scipy.constants.k*(T_+273.15))) -1) ,x=lambda_cam_x,axis=0) * inttime/1000
			temp1 = emissivity*photon_flux_interpolator(T_)
			# temp1 = 2*scipy.constants.h*(scipy.constants.c**2)/(lambda_cam_x.max()**5) * 1/( np.exp(scipy.constants.h*scipy.constants.c/(lambda_cam_x.max()*scipy.constants.k*T_)) -1)
			out = a1*temp1 + a2
			out[:number_of_window] = a1*a3*temp1[:number_of_window] + a2 + a4
			return out
		return int

	def BB_rad_window_OR_no_window(lambda_cam_x=lambda_cam_x,emissivity=1):
		def int(T_,a1,a2):
			# lambda_cam_x = np.linspace(1.5,5.1,10)*1e-6	# m, Range of FLIR SC7500
			# lambda_cam = np.array([lambda_cam_x.tolist()]*len(T_)).T
			# temp1 = np.trapz(2*scipy.constants.c/(lambda_cam**4) * 1/( np.exp(scipy.constants.h*scipy.constants.c/(lambda_cam*scipy.constants.k*(T_+273.15))) -1) ,x=lambda_cam_x,axis=0) * inttime/1000
			temp1 = emissivity*photon_flux_interpolator(T_)
			# temp1 = 2*scipy.constants.h*(scipy.constants.c**2)/(lambda_cam_x.max()**5) * 1/( np.exp(scipy.constants.h*scipy.constants.c/(lambda_cam_x.max()*scipy.constants.k*T_)) -1)
			out = a1*temp1 + a2
			return out
		return int

	def deg_2_poly(x,a2,a1,a0):
		out = a2*x**2 + a1*x + a0
		return out

	def BB_rad_counts_to_delta_temp(trash,T_,lambda_cam_x=lambda_cam_x):
		# lambda_cam_x = np.linspace(1.5,5.1,10)*1e-6	# m, Range of FLIR SC7500
		# temp1 = np.trapz(2*scipy.constants.c/(lambda_cam_x**4) * 1/( np.exp(scipy.constants.h*scipy.constants.c/(lambda_cam_x*scipy.constants.k*(T_+273.15))) -1) ,x=lambda_cam_x) * inttime/1000
		temp1 = photon_flux_interpolator(T_)
		# temp1 = 2*scipy.constants.h*(scipy.constants.c**2)/(lambda_cam_x.max()**5) * 1/( np.exp(scipy.constants.h*scipy.constants.c/(lambda_cam_x.max()*scipy.constants.k*T_)) -1)
		return temp1

	def sigma_T_multimpier(T_,lambda_cam_x=lambda_cam_x):
		# lambda_cam_x = np.linspace(1.5,5.1,10)*1e-6	# m, Range of FLIR SC7500
		temp = 2*scipy.constants.c/(lambda_cam_x**4) * 1/( np.exp(scipy.constants.h*scipy.constants.c/(lambda_cam_x*scipy.constants.k*(T_+273.15))) -1) * inttime/1000
		temp1 = np.trapz(temp* scipy.constants.h*scipy.constants.c/(lambda_cam_x*scipy.constants.k*(T_**2)) ,x=lambda_cam_x)
		# temp1 = 2*scipy.constants.h*(scipy.constants.c**2)/(lambda_cam_x.max()**5) * 1/( np.exp(scipy.constants.h*scipy.constants.c/(lambda_cam_x.max()*scipy.constants.k*T_)) -1)
		return temp1

	# problem: how do I deal with dead pixels? if I apply the calibration retroactively I could have old good pixels that just became bad ones.
	# also I cannot guarantee that digitizer 1 will stay digitiser 1 for a different temperature
	# I find the dead pixels here and the coeffs are smoothed over ALL of them
	flag = np.sum([find_dead_pixels([[data]],treshold_for_bad_low_std=-1,treshold_for_bad_difference=100*inttime/2) for data in meancounttot[-1]],axis=0)
	if window_data_present:
		meancounttot = [replace_dead_pixels([meancounttot[:,i]],flag)[0] for i in range(np.shape(meancounttot)[1])]
		meancountstdtot = [replace_dead_pixels([meancountstdtot[:,i]],flag)[0] for i in range(np.shape(meancountstdtot)[1])]
		meancounttot = np.transpose(meancounttot,(1,0,2,3))
		meancountstdtot = np.transpose(meancountstdtot,(1,0,2,3))
	if no_window_data_present:
		meancounttot_no_window = [replace_dead_pixels([meancounttot_no_window[:,i]],flag)[0] for i in range(np.shape(meancounttot_no_window)[1])]
		meancountstdtot_no_window = [replace_dead_pixels([meancountstdtot_no_window[:,i]],flag)[0] for i in range(np.shape(meancountstdtot_no_window)[1])]
		meancounttot_no_window = np.transpose(meancounttot_no_window,(1,0,2,3))
		meancountstdtot_no_window = np.transpose(meancountstdtot_no_window,(1,0,2,3))

	number_of_window = len(temperature_window)
	bds = [[0,-np.inf,0,-np.inf],[np.inf,np.inf,1,np.inf]]
	bds1=np.array(bds)[:,:2]
	x__all = np.array(meancounttot.tolist() + meancounttot_no_window.tolist())
	xerr__all = np.array(meancountstdtot.tolist() + meancountstdtot_no_window.tolist())
	for j in range(shapex):
		for k in range(shapey):
			for i_z,z in enumerate(digitizer_ID):
				if window_data_present:
					x=np.array(meancounttot[:,z==digitizer_ID,j,k]).flatten()
					x_=x__all[:,z==digitizer_ID,j,k].flatten()
					xerr=np.array(meancountstdtot[:,z==digitizer_ID,j,k]).flatten()
					xerr_=xerr__all[:,z==digitizer_ID,j,k].flatten()
					# temp1,temp2=np.polyfit(temperature_window,x,n-1,cov='unscaled')
					temp1=np.polyfit(x,temperature_window,n-1)	# this correction alone decrease the errors by 2 error by 2 orders of magnitude
					yerr=(np.polyval(temp1,x+xerr)-np.polyval(temp1,x-xerr))/2
					temp1,temp2=np.polyfit(x,temperature_window,n-1,w=1/yerr,cov='unscaled')

					x__window=np.array(meancounttot[:,z==digitizer_ID,j,k]).flatten()
					xerr__window=np.array(meancountstdtot[:,z==digitizer_ID,j,k]).flatten()
					fit2 = curve_fit(BB_rad_window_OR_no_window(),np.array(temperature_window),x__window,sigma=xerr__window,absolute_sigma=True,p0=[1e-19,1000],bounds=bds1,x_scale=[1e-20,1])
				if no_window_data_present:
					x__no_window=np.array(meancounttot_no_window[:,z==digitizer_ID,j,k]).flatten()
					xerr__no_window=np.array(meancountstdtot_no_window[:,z==digitizer_ID,j,k]).flatten()
					fit1 = curve_fit(BB_rad_window_OR_no_window(),np.array(temperature_no_window),x__no_window,sigma=xerr__no_window,absolute_sigma=True,p0=[1e-19,1000],bounds=bds1,x_scale=[1e-20,1])
				if window_data_present and no_window_data_present:
					# fit = curve_fit(deg_2_poly,x,temperature_window,sigma=yerr,absolute_sigma=True,p0=[1,1,1])	# equivalent to np.polyfit, just to check that both return the same uncertainty
					fit = curve_fit(BB_rad(number_of_window),np.array(temperature),x_,sigma=xerr_,absolute_sigma=True,p0=[1e-19,1000,1,100],bounds=bds,x_scale=[1e-20,1,1,1])
					# the following is much slower
					# x_optimal, y_opt, opt_info = scipy.optimize.fmin_l_bfgs_b(BB_rad_prob_and_gradient(np.array(temperature_window)+273,x), x0=[1e4,100], iprint=0, factr=1e2, pgtol=1e-6, maxiter=5000)#,m=1000, maxls=1000, pgtol=1e-10, factr=1e0)#,approx_grad = True)
					# fit = make_standatd_fit_output(BB_rad_prob_and_gradient(np.array(temperature_window)+273,x,grads=False),x_optimal)
				elif window_data_present:
					fit = [[1,0,*fit2[0]] , np.zeros((4,4))]
					fit[1][2:,2:] = fit2[1]
				elif no_window_data_present:
					fit = [[*fit1[0],1,0] , np.zeros((4,4))]
					fit[1][:2,:2] = fit1[1]
				if False:	# plot for paper
					plt.figure(figsize=(10, 7))
					plt.errorbar(x,temperature_window,yerr=np.zeros_like(temperature_window),xerr=xerr,fmt='+',color='b',label='view port')
					# plt.errorbar(x_[number_of_window:],temperature_no_window,yerr=np.zeros_like(temperature_no_window),xerr=xerr_[number_of_window:],fmt='+',color='r',label='no view port')
					plt.plot(np.sort(x),np.polyval(temp1,np.sort(x)),'-b',label='polynomial fit n='+str(n-1)+', '+r'$R^2=%.5g$' %(rsquared(temperature_window,np.polyval(temp1,x))))
					plt.plot(BB_rad_window_OR_no_window()(np.sort(temperature_window),*fit2[0]),np.sort(temperature_window),'-.b',label='BB window'+', '+r'$R^2=%.5g$' %(rsquared(x__window,BB_rad_window_OR_no_window()(temperature_window,*fit2[0]))))
					plt.plot(BB_rad_window_OR_no_window()(np.sort(temperature_window),*[fit[0][0]*fit[0][2],fit[0][1]+fit[0][3]]),np.sort(temperature_window),'--y',label='BB fit window+no window, '+r'$R^2=%.5g$' %(rsquared(x_,BB_rad(number_of_window)(temperature,*fit[0]))) + '\n'+r'$C_1=%.3g, C_2=%.3g, C_3=%.3g, C_4=%.3g$' %(fit[0][0],fit[0][1],fit[0][2],fit[0][3]))
					# plt.plot(BB_rad_window_OR_no_window()(np.sort(temperature_no_window),*fit1[0]),np.sort(temperature_no_window),'-.r',label='BB no window')
					# plt.plot(BB_rad_window_OR_no_window()(np.sort(temperature_no_window),*fit[0][:2]),np.sort(temperature_no_window),'--y')
					plt.grid()
					plt.legend(loc='upper left', fontsize='small')
					plt.ylabel('temperature [°C]')
					plt.xlabel('counts [au]')
					plt.title('digitizer,j,k='+str((z,j,k)))
					# plt.pause(0.01)
					plt.savefig('/home/ffederic/work/irvb/flatfield'+'/example_BB_fit'+str((z,j,k))+'.png')#, bbox_inches='tight')
				if False:	# same but with only window
					plt.figure(figsize=(10, 7))
					plt.grid()
					color='r'
					p1 = plt.errorbar(x,temperature_window,yerr=np.zeros_like(temperature_window),xerr=xerr,fmt='+',color=color)
					plt.plot(BB_rad_window_OR_no_window()(np.sort(temperature_window),*fit2[0]),np.sort(temperature_window),'--',color=p1.lines[0].get_color(),label='fit '+str((z,j,k))+' \n'+r'$a_1=%.3g\pm%.3g$''\n'r'$a_2=%.3g\pm%.3g$''\n'r'$R^2=%.5g$' %(fit2[0][0],fit2[1][0,0]**0.5,fit2[0][1],fit2[1][1,1]**0.5,rsquared(x__window,BB_rad_window_OR_no_window()(temperature_window,*fit2[0]))))
					plt.legend(loc='upper left', fontsize='small')
					plt.ylabel('temperature [°C]')
					plt.xlabel('counts [au]')
					plt.title('digitizer,j,k='+str((z,j,k)))
					# plt.pause(0.01)
					plt.savefig('/home/ffederic/work/irvb/flatfield'+'/example_BB_fit'+str((z,j,k))+'2.png')#, bbox_inches='tight')
				if False:	# small piece to check how the uncertainty goes between the 2 models
					delta_counts = 7194-5800
					a1a3 = fit[0][0]*fit[0][2]
					sigma_a1a3 = a1a3 * ((fit[1][0,0]**0.5/fit[0][0])**2 + (fit[1][2,2]**0.5/fit[0][2])**2 + 2*fit[1][0,2]/a1a3)**0.5
					ref = delta_counts/a1a3 + BB_rad_counts_to_delta_temp(1,300)
					sigma_ref = delta_counts/a1a3*(( (estimate_counts_std(5800+delta_counts)*2/delta_counts)**2 + (sigma_a1a3/(a1a3**2))**2 )**0.5)
					# sigma_ref = (sigma_ref**2 + ((sigma_T_multimpier(300)*0.1)**2))**0.5	# I'm not sure if I should consider this
					check = curve_fit(BB_rad_counts_to_delta_temp,1,ref,sigma=[sigma_ref],absolute_sigma=True,p0=[300])
					counts = 5800+delta_counts
					temp = temp1[-1] + temp1[-2] * counts + temp1[-3] * (counts**2)
					counts_std = estimate_counts_std(counts)
					temperature_std = (temp2[2,2] + (counts_std**2)*(temp1[1]**2) + (counts**2+counts_std**2)*temp2[1,1] + (counts_std**2)*(4*counts**2+3*counts_std**2)*(temp1[0]**2) + (counts**4+6*(counts**2)*(counts_std**2)+3*counts_std**4)*temp2[0,0] + 2*counts*temp2[2,1] + 2*(counts**2+counts_std**2)*temp2[2,0] + 2*(counts**3+counts*(counts_std**2))*temp2[1,0])**0.5
				if window_data_present:
					coeff[i_z,j,k,:]=temp1
					errcoeff[i_z,j,k,:]=temp2
					coeff2[i_z,j,k,:]=fit[0]
					errcoeff2[i_z,j,k,:]=fit[1]
					coeff4[i_z,j,k,:]=fit2[0]
					errcoeff4[i_z,j,k,:]=fit2[1]
					score[i_z,j,k]=rsquared(temperature_window,np.polyval(temp1,x))
					score2[i_z,j,k]=rsquared(x_,BB_rad(number_of_window)(np.array(temperature),*fit[0]))
					score4[i_z,j,k]=rsquared(x__window,BB_rad_window_OR_no_window()(temperature_window,*fit2[0]))
				if no_window_data_present:
					coeff3[i_z,j,k,:]=fit1[0]
					errcoeff3[i_z,j,k,:]=fit1[1]
					score3[i_z,j,k]=rsquared(x__no_window,BB_rad_window_OR_no_window()(temperature_no_window,*fit1[0]))
	output_dict = dict([('coeff',coeff),('errcoeff',errcoeff),('score',score),('coeff2',coeff2),('errcoeff2',errcoeff2),('score2',score2),('coeff3',coeff3),('errcoeff3',errcoeff3),('score3',score3),('coeff4',coeff4),('errcoeff4',errcoeff4),('score4',score4)])
	output_dict['all_dead_pixels_markers'] = dict([('all_dead_pixels_markers',dead_pixels_markerstot)])
	output_dict['common_dead_pixels_markers'] = dict([('common_dead_pixels_markers',common_dead_pixels_markers)])
	np.savez(os.path.join(pathparam,'coeff_polynomial_deg'+str(n-1)+'int_time'+str(inttime)+'ms'),**output_dict)
	print('for a polinomial of degree '+str(n-1)+' the R^2 score is '+str(np.sum(score[n-2])))


def build_average_poly_coeff_multi_digitizer(temperaturehot,temperaturecold,fileshot,filescold,int,framerate,pathparam,n):
	# 08/10/2018 THIS MAKES THE AVERAGE OF COEFFICIENTS FROM MULTIPLE HOT>ROOM AND COLD >ROOM CYCLES THE COEFFICIENTS

	lengthhot=len(temperaturehot)
	lengthcold=len(temperaturecold)

	first=True
	for i in range(lengthhot):
		for j in range(lengthcold):
			path=pathparam+'/'+str(int)+'ms'+str(framerate)+'Hz'+'/'+'numcoeff'+str(n)+'/'+str(i+1)+'-'+str(j+1)
			full_saved_file_dict=np.load(os.path.join(path,'coeff_polynomial_deg'+str(n-1)+'int_time'+str(int)+'ms'+'.npz'))
			if first==True:
				shape=np.shape(full_saved_file_dict['coeff'])
				shape=np.concatenate(((lengthhot,lengthcold),shape))
				coeff=np.zeros(shape)
				shape=np.shape(full_saved_file_dict['errcoeff'])
				shape=np.concatenate(((lengthhot,lengthcold),shape))
				errcoeff=np.zeros(shape)
				shape=np.shape(full_saved_file_dict['score'])
				shape=np.concatenate(((lengthhot,lengthcold),shape))
				score=np.zeros(shape)
				first=False
			coeff[i,j]=full_saved_file_dict['coeff']
			errcoeff[i,j]=full_saved_file_dict['errcoeff']

	select = np.zeros((n,n)).astype(bool)
	np.fill_diagonal(select,True)
	meancoeff=np.sum(coeff/errcoeff[:,:,:,:,:,select],axis=(0,1))/np.sum(1/errcoeff[:,:,:,:,:,select],axis=(0,1))
	meanerrcoeff=(1/np.sum(1/errcoeff,axis=(0,1)))*1/((lengthhot*lengthcold)**0.5)
	meanerrcoeff[:,:,:,select] = (np.std(coeff,axis=(0,1))**2 + meanerrcoeff[:,:,:,select])
	meanscore = np.mean(score,axis=(0,1))

	path=pathparam+'/'+str(int)+'ms'+str(framerate)+'Hz'+'/'+'numcoeff'+str(n)+'/average'
	if not os.path.exists(path):
		os.makedirs(path)
	np.savez_compressed(os.path.join(path,'coeff_polynomial_deg'+str(n-1)+'int_time'+str(int)+'ms'),**dict([('coeff',meancoeff),('errcoeff',meanerrcoeff),('score',meanscore)]))

def count_to_temp_1D_homo_conversion(arg):
	out = np.sum(np.power(np.array([(arg[0]).tolist()]*arg[3]).T,np.arange(arg[3]-1,-1,-1))*correlated_values(arg[1],arg[2]),axis=1)
	out1 = nominal_values(out)
	out2 = std_devs(out)
	return [out1,out2]

def count_to_temp_0D_homo_conversion(arg):
	out = np.sum(np.power(np.array([(uarray(arg[0],arg[1])).tolist()]*arg[4]).T,np.arange(arg[4]-1,-1,-1))*correlated_values(arg[2],arg[3]),axis=0)
	out1 = nominal_values(out)
	out2 = std_devs(out)
	return [out1,out2]


def count_to_temp_poly_multi_digitizer_time_dependent(counts,params,errparams,reference_background,reference_background_std,reference_background_flat,digitizer_ID,number_cpu_available,n,parallelised=True,report=0):
	temperature = []
	temperature_std = []
	with cf.ProcessPoolExecutor(max_workers=number_cpu_available) as executor:
		for i in range(len(digitizer_ID)):
			counts_temp = np.array(counts[i])
			if False:
				temp1 = []
				temp2 = []
				if report>0:
					start_time = tm.time()
				for j in range(counts_temp.shape[1]):
					if report>0:
						start_time_1 = tm.time()
					if parallelised:	# parallel way
						arg = []
						for k in range(counts_temp.shape[2]):
							arg.append([counts_temp[:,j,k],params[i,j,k],errparams[i,j,k],n])
						if report>1:
							print(str(j) + ' , %.5gs , %.3gs' %(tm.time()-start_time,tm.time()-start_time_1))
						out = list(executor.map(count_to_temp_1D_homo_conversion,arg))
					else:	# non parallel way
						if report>1:
							print(str(j) + ' , %.5gs , %.3gs' %(tm.time()-start_time,tm.time()-start_time_1))
						out = []
						for k in range(counts_temp.shape[2]):
							out.append(count_to_temp_1D_homo_conversion([counts_temp[:,j,k],params[i,j,k],errparams[i,j,k],n]))

					if report>1:
						print(str(j) + ' , %.5gs , %.3gs' %(tm.time()-start_time,tm.time()-start_time_1))
					temp1.append([x for x,y in out])
					temp2.append([y for x,y in out])
					if report>0:
						print(str(j) + ' , %.5gs , %.3gs' %(tm.time()-start_time,tm.time()-start_time_1))
			else:	# method brutally simpler, just doing a matrix multiplication
				temp1 = params[i][:,:,2] + counts_temp*params[i][:,:,1] + (counts_temp**2)*params[i][:,:,0]
				if False:
					# this is approximate because it does not account properly for the error matrix but only the diagonal, but it's massively faster
					temp2 = (errparams[i][:,:,2,2] + (counts_temp**2)*errparams[i][:,:,1,1] + (counts_temp**4)*errparams[i][:,:,0,0])**0.5
				else:
					# unfortunately the correct method is necessary, otherwise the std is overestimated by 2 orders of magnitude
					# I use the the uncertainty on the counts as
					counts_temp_std = estimate_counts_std(counts_temp)
					# temp2 = (errparams[i][:,:,2,2] + counts_temp*(params[i][:,:,1]**2) + (counts_temp**2+counts_temp)*errparams[i][:,:,1,1] + (4*counts_temp**3+3*counts_temp**2)*(params[i][:,:,0]**2) + (counts_temp**4+6*counts_temp**3+3*counts_temp**2)*errparams[i][:,:,0,0] + 2*counts_temp*errparams[i][:,:,2,1] + 2*(counts_temp**2+counts_temp)*errparams[i][:,:,2,0] + 2*(counts_temp**3+counts_temp**2)*errparams[i][:,:,1,0])**0.5
					temp2 = (errparams[i][:,:,2,2] + (counts_temp_std**2)*(params[i][:,:,1]**2) + (counts_temp**2+counts_temp_std**2)*errparams[i][:,:,1,1] + (counts_temp_std**2)*(4*counts_temp**2+3*counts_temp_std**2)*(params[i][:,:,0]**2) + (counts_temp**4+6*(counts_temp**2)*(counts_temp_std**2)+3*counts_temp_std**4)*errparams[i][:,:,0,0] + 2*counts_temp*errparams[i][:,:,2,1] + 2*(counts_temp**2+counts_temp_std**2)*errparams[i][:,:,2,0] + 2*(counts_temp**3+counts_temp*(counts_temp_std**2))*errparams[i][:,:,1,0])**0.5
			# temperature.append(np.transpose(temp1,(2,0,1)))
			# temperature_std.append(np.transpose(temp2,(2,0,1)))
			temperature.append(temp1)
			temperature_std.append(temp2)

	return temperature,temperature_std

def count_to_temp_poly_multi_digitizer_stationary(counts,counts_std,params,errparams,digitizer_ID,number_cpu_available,n,parallelised=True,report=0):

	temperature = []
	temperature_std = []
	with cf.ProcessPoolExecutor(max_workers=number_cpu_available) as executor:
		# executor = cf.ProcessPoolExecutor()#max_workers=number_cpu_available)
		for i in range(len(digitizer_ID)):
			counts_temp = np.array(counts[i])
			counts_std_temp = np.array(counts_std[i])
			temp1 = []
			temp2 = []
			if report>0:
				start_time = tm.time()
			for j in range(counts_temp.shape[0]):
				if report>0:
					start_time_1 = tm.time()

				if parallelised:	# parallel way
					arg = []
					for k in range(counts_temp.shape[1]):
						arg.append([counts_temp[j,k],counts_std_temp[j,k],params[i,j,k],errparams[i,j,k],n])
					if report>1:
						print(str(j) + ' , %.5gs , %.3gs' %(tm.time()-start_time,tm.time()-start_time_1))
					out = list(executor.map(count_to_temp_0D_homo_conversion,arg))
				else:	# non parallel way
					if report>1:
						print(str(j) + ' , %.5gs , %.3gs' %(tm.time()-start_time,tm.time()-start_time_1))
					out = []
					for k in range(counts_temp.shape[1]):
						out.append(count_to_temp_0D_homo_conversion([counts_temp[j,k],counts_std_temp[j,k],params[i,j,k],errparams[i,j,k],n]))

				if report>1:
					print(str(j) + ' , %.5gs , %.3gs' %(tm.time()-start_time,tm.time()-start_time_1))
				temp1.append([x for x,y in out])
				temp2.append([y for x,y in out])
				if report>0:
					print(str(j) + ' , %.5gs , %.3gs' %(tm.time()-start_time,tm.time()-start_time_1))
			temperature.append(np.array(temp1))
			temperature_std.append(np.array(temp2))

	return temperature,temperature_std

def count_to_temp_poly_multi_digitizer(counts,params,errparams,digitizer_ID,number_cpu_available,counts_std=[0],reference_background=[0],reference_background_std=[0],reference_background_flat=0,parallelised=True,report=0):

	n = np.shape(params)[-1]
	shape = np.shape(counts)
	if len(shape)==3:
		if np.shape(counts)!=np.shape(counts_std):
			print("for steady state conversion counts std should be supplied, counts shape "+str(np.shape(counts))+" counts std "+str(np.shape(counts_std)))
			exit()
		return count_to_temp_poly_multi_digitizer_stationary(counts,counts_std,params,errparams,digitizer_ID,number_cpu_available,n,parallelised=parallelised,report=report)
	else:
		if (len(np.shape(reference_background))<2 or len(np.shape(reference_background_std))<2) and False:	# this is no longer necessary in count_to_temp_poly_multi_digitizer_time_dependent
			print("you didn't supply the appropriate background counts, requested "+str(np.shape(counts)[-2:]))
			exit()
		return count_to_temp_poly_multi_digitizer_time_dependent(counts,params,errparams,reference_background,reference_background_std,reference_background_flat,digitizer_ID,number_cpu_available,n,parallelised=parallelised,report=report)

def count_to_temp_BB_multi_digitizer(counts,params,errparams,digitizer_ID,counts_std=[0],reference_background=[0],reference_background_std=[0],ref_temperature=20,ref_temperature_std=0,wavewlength_top=5.1,wavelength_bottom=1.5,inttime=2):
	# I don't think that there is the need of a separate function for stationary and time dependent

	shape = np.shape(counts)
	if len(shape)==3:
		if np.shape(counts)!=np.shape(counts_std):
			print("for steady state conversion counts std should be supplied, counts shape "+str(np.shape(counts))+" counts std "+str(np.shape(counts_std)))
			exit()
		return count_to_temp_BB_multi_digitizer_int(counts,counts_std,params,errparams,digitizer_ID,reference_background,reference_background_std,ref_temperature=ref_temperature,ref_temperature_std=ref_temperature_std,wavewlength_top=wavewlength_top,wavelength_bottom=wavelength_bottom,inttime=inttime)
	else:
		if (len(np.shape(reference_background))<2 or len(np.shape(reference_background_std))<2) and False:	# this is no longer necessary in count_to_temp_poly_multi_digitizer_time_dependent
			print("you didn't supply the appropriate background counts, requested "+str(np.shape(counts)[-2:]))
			exit()
		if counts_std==[0]:
			counts_std = []
			for i in range(len(digitizer_ID)):
				counts_std.append(estimate_counts_std(counts[i],int_time=inttime))
		return count_to_temp_BB_multi_digitizer_int(counts,counts_std,params,errparams,digitizer_ID,reference_background,reference_background_std,ref_temperature=ref_temperature,ref_temperature_std=ref_temperature_std,wavewlength_top=wavewlength_top,wavelength_bottom=wavelength_bottom,inttime=inttime)


def calc_interpolators_BB(wavewlength_top=5.1,wavelength_bottom=1.5,inttime=2):
	# lambda_cam_x = np.linspace(wavelength_bottom,wavewlength_top,100)*1e-6	# m, Range of FLIR SC7500
	lambda_cam_x = np.arange(wavelength_bottom,wavewlength_top,0.02)*1e-6	# m, Range of FLIR SC7500
	def BB_rad_counts_to_delta_temp(trash,T_,lambda_cam_x=lambda_cam_x):
		temp1 = np.trapz(2*np.pi*scipy.constants.c/(lambda_cam_x**4) * 1/( np.exp(scipy.constants.h*scipy.constants.c/(lambda_cam_x*scipy.constants.k*T_)) -1) ,x=lambda_cam_x) * inttime/1000
		return temp1

	temperature_range = np.linspace(0,50,num=100)	# degC
	temperature_range += 273.15	# K
	photon_flux = []
	for T in temperature_range:
		photon_flux.append(BB_rad_counts_to_delta_temp(1,T))
	photon_flux = np.array(photon_flux)
	photon_flux_interpolator = interp1d(temperature_range-273.15,photon_flux,bounds_error=False,fill_value='extrapolate')	# in degC
	reverse_photon_flux_interpolator = interp1d(photon_flux,temperature_range-273.15,bounds_error=False,fill_value='extrapolate')	# in degC
	photon_flux_over_temperature = photon_flux/temperature_range	# flux / K
	photon_flux_over_temperature_interpolator = interp1d(temperature_range-273.15,photon_flux_over_temperature,bounds_error=False,fill_value='extrapolate')	# in degC, ,but the ratio is still flux / K
	photon_dict = dict([])
	photon_dict['photon_flux_interpolator'] = photon_flux_interpolator
	photon_dict['reverse_photon_flux_interpolator'] = reverse_photon_flux_interpolator	# equivalemt to alpha_r in the thesis
	photon_dict['temperature_range'] = temperature_range
	photon_dict['photon_flux'] = photon_flux
	photon_dict['photon_flux_over_temperature'] = photon_flux_over_temperature
	photon_dict['photon_flux_over_temperature_interpolator'] = photon_flux_over_temperature_interpolator	# equivalemt to alpha in the thesis
	return photon_dict

def calc_BB_coefficients_multi_digitizer(params,errparams,digitizer_ID,reference_background,reference_background_std,ref_temperature=20,ref_temperature_std=0,wavewlength_top=5.1,wavelength_bottom=1.5,inttime=2):

	params = np.array(params)
	errparams = np.array(errparams)
	photon_dict = calc_interpolators_BB(wavewlength_top=wavewlength_top,wavelength_bottom=wavelength_bottom,inttime=inttime)
	photon_flux_interpolator = photon_dict['photon_flux_interpolator']

	photon_flux_std = np.abs(photon_flux_interpolator(ref_temperature+ref_temperature_std)-photon_flux_interpolator(ref_temperature-ref_temperature_std))/2
	constant_offset = []
	constant_offset_std = []
	BB_proportional = []
	BB_proportional_std = []
	for i in range(len(digitizer_ID)):
		if len(params[0,0,0])==4:	# this is for the case when I fit both the data with window and without
			BB_proportional.append(params[i,:,:,0]*params[i,:,:,2])
			BB_proportional_std.append(((errparams[i,:,:,0,0]**0.5 * params[i,:,:,2])**2 + (errparams[i,:,:,2,2]**0.5 * params[i,:,:,0])**2 + 2*params[i,:,:,0]*params[i,:,:,2]*errparams[i,:,:,2,0])**0.5)
			# I think I had a different formula for the temperature when I wrote the sext 2 lines
			# constant_offset.append( reference_background[i]-BB_proportional[-1]*photon_flux_interpolator(ref_temperature) )
			# constant_offset_std.append( ((photon_flux_interpolator(ref_temperature)*BB_proportional_std[-1])**2 + (BB_proportional[-1]*photon_flux_std)**2 + reference_background_std[i]**2 )**0.5 )
			constant_offset.append( params[i,:,:,1]+params[i,:,:,3] )
			constant_offset_std.append( ( errparams[i,:,:,1,1]**2 + errparams[i,:,:,3,3]**2 + errparams[i,:,:,1,3] )**0.5 )
		elif len(params[0,0,0])==2:	# this is for the case when I fit only the data with window
			BB_proportional.append(params[i,:,:,0])
			BB_proportional_std.append(errparams[i,:,:,0,0])
			# I think I had a different formula for the temperature when I wrote the sext 2 lines
			# constant_offset.append( reference_background[i]-BB_proportional[-1]*photon_flux_interpolator(ref_temperature) )
			# constant_offset_std.append( ((photon_flux_interpolator(ref_temperature)*BB_proportional_std[-1])**2 + (BB_proportional[-1]*photon_flux_std)**2 + reference_background_std[i]**2 )**0.5 )
			constant_offset.append( params[i,:,:,1] )
			constant_offset_std.append( errparams[i,:,:,1,1] )
	BB_proportional = np.array(BB_proportional)
	BB_proportional_std = np.array(BB_proportional_std)
	constant_offset = np.array(constant_offset)
	constant_offset_std = np.array(constant_offset_std)

	return BB_proportional,BB_proportional_std,constant_offset,constant_offset_std,photon_dict


def count_to_temp_BB_multi_digitizer_int(counts,counts_std,params,errparams,digitizer_ID,reference_background,reference_background_std,ref_temperature=20,ref_temperature_std=0,wavewlength_top=5.1,wavelength_bottom=1.5,inttime=2):
	# by definition resetting the constant value such that it matches ref_temperature means that the reference temperature is a flat ref_temperature, so I'm not sure if this function will ever be usefull

	BB_proportional,BB_proportional_std,constant_offset,constant_offset_std,photon_dict = calc_BB_coefficients_multi_digitizer(params,errparams,digitizer_ID,reference_background,reference_background_std,ref_temperature=ref_temperature,ref_temperature_std=ref_temperature_std,wavewlength_top=wavewlength_top,wavelength_bottom=wavelength_bottom,inttime=inttime)
	photon_dict = calc_interpolators_BB(wavewlength_top=wavewlength_top,wavelength_bottom=wavelength_bottom,inttime=inttime)
	reverse_photon_flux_interpolator = photon_dict['reverse_photon_flux_interpolator']
	photon_flux_over_temperature_interpolator = photon_dict['photon_flux_over_temperature_interpolator']
	photon_flux_interpolator = photon_dict['photon_flux_interpolator']

	temperature = []
	temperature_std = []
	for i in range(len(digitizer_ID)):
		photon_flux = (counts[i] - reference_background[i])/BB_proportional[i] + photon_flux_interpolator(ref_temperature)
		temperature.append(reverse_photon_flux_interpolator(photon_flux))	# in degC
		photon_flux_over_temperature = photon_flux_over_temperature_interpolator(temperature[-1])
		if False:	# verified 2024/09/12 that this calculation is wrong, as it gives a much larger uncertainty than it should
			temperature_std.append( ( (counts_std[i]/(photon_flux_over_temperature*BB_proportional[i]))**2 + (reference_background_std[i]/(photon_flux_over_temperature_interpolator(ref_temperature)*BB_proportional[i]))**2 + (BB_proportional_std[i]*(counts[i] - reference_background[i])/((BB_proportional[i]**2)*photon_flux_over_temperature))**2 + (ref_temperature_std)**2 )**0.5 )	# in degC
		else:
			forward = reverse_photon_flux_interpolator((counts[i]+counts_std[i] - reference_background[i])/(BB_proportional[i]-BB_proportional_std[i]) + photon_flux_interpolator(ref_temperature))
			backward = reverse_photon_flux_interpolator((counts[i]-counts_std[i] - reference_background[i])/(BB_proportional[i]+BB_proportional_std[i]) + photon_flux_interpolator(ref_temperature))
			temperature_std_counts = (forward - backward)/2
			temperature_std.append((temperature_std_counts**2 + ref_temperature_std**2)**0.5)
	temperature = np.array(temperature)
	temperature_std = np.array(temperature_std)

	return temperature,temperature_std

def count_to_temp_BB_multi_digitizer_no_reference(counts,counts_std,params,errparams,digitizer_ID,wavewlength_top=5.1,wavelength_bottom=1.5,inttime=2):
	# by definition resetting the constant value such that it matches ref_temperature means that the reference temperature is a flat ref_temperature, so I'm not sure if this function will ever be usefull

	BB_proportional,BB_proportional_std,constant_offset,constant_offset_std,photon_dict = calc_BB_coefficients_multi_digitizer(params,errparams,digitizer_ID,0,0,wavewlength_top=wavewlength_top,wavelength_bottom=wavelength_bottom,inttime=inttime)
	photon_dict = calc_interpolators_BB(wavewlength_top=wavewlength_top,wavelength_bottom=wavelength_bottom,inttime=inttime)
	reverse_photon_flux_interpolator = photon_dict['reverse_photon_flux_interpolator']
	photon_flux_over_temperature_interpolator = photon_dict['photon_flux_over_temperature_interpolator']
	photon_flux_interpolator = photon_dict['photon_flux_interpolator']

	temperature = []
	temperature_std = []
	for i in range(len(digitizer_ID)):
		photon_flux = (counts[i] - constant_offset[i])/BB_proportional[i]
		temperature.append(reverse_photon_flux_interpolator(photon_flux))	# in degC
		photon_flux_over_temperature = photon_flux_over_temperature_interpolator(temperature[-1])
		temperature_std.append( ( (counts_std[i]/(photon_flux_over_temperature*BB_proportional[i]))**2 + (constant_offset_std[i]/(photon_flux_over_temperature*BB_proportional[i]))**2 + (BB_proportional_std[i]*(counts[i] - constant_offset[i])/((BB_proportional[i]**2)*photon_flux_over_temperature))**2 )**0.5 )	# in degC
	temperature = np.array(temperature)
	temperature_std = np.array(temperature_std)

	return temperature,temperature_std


##################################################################################################################################################################################################

def print_all_properties(obj):
	# created 29/09/2020 function that prints all properties of an object
	for attr in dir(obj):
		print("object.%s = %r" % (attr, getattr(obj, attr)))

####################################################################################################################################################################################################################################################

# -*- coding: utf-8 -*-
"""
Created on Fri Jul 02 19:15:37 2021

@author: Federici
"""

# Created from code from James Harrison
from matplotlib.figure import Figure
from matplotlib.backends.backend_tkagg import (FigureCanvasTkAgg, NavigationToolbar2Tk)

from . import efitData

from scipy.interpolate import RectBivariateSpline
from scipy.interpolate import interp1d
from scipy.optimize import bisect
from scipy import interpolate
from mastu_exhaust_analysis.read_efit import read_uda
import netCDF4
# import pyuda
# client=pyuda.Client()

def reset_connection(client):
	try:
		trash = client.get('/AMB/CTIME',45401)	# this is just to have a positive reading
	except Exception as e:
		print('the reading that should always work part of reset_connection failed')
		print('Error '+str(e))
		# exit()
	client.set_property("timeout", 0)
	try:
		_ = client.get('help::ping()','')
	except pyuda.UDAException:
		print("_ = client.get('help::ping()','') failed")
		pass
	try:
		client.set_property('timeout', 600)
	except pyuda.UDAException:
		print("client.set_property('timeout', 600) failed")
		pass
	print('client reset done')

client=pyuda.Client()
# exec(open("/home/ffederic/work/analysis_scripts/scripts/python_library/collect_and_eval/collect_and_eval/MASTU_structure.py").read())
reset_connection(client)
del client

class mclass:


	def __init__(self,path,pulse_ID=None):
		# from multiprocessing.dummy import Pool as ThreadPool
		from multiprocessing.pool import ThreadPool
		# from multiprocessing import Pool as ThreadPool
		import time as tm
		import netCDF4
		def abortable_worker(func,timeout, *arg):
			p = ThreadPool(1)
			res = p.apply_async(func, args=arg)
			print('Starting read ' + str(arg[0]))
			start = tm.time()
			try:
				output = res.get(timeout)  # Wait timeout seconds for func to complete.
				print("Succesful read of " +str(arg[0])+ " in %.3g min" %((tm.time() - start)/60))
				# p.close()
				# return out
				# except multiprocessing.TimeoutError:
			except Exception as e:
				if str(e)=='':
					print("External aborting due to timeout " +str(arg[0])+ " in %.3g s" %(tm.time() - start))
				else:
					print('Error '+str(e))
				output = False
			p.close()
			p.join()
			del p
			return output
		netCDF4_read_failed = True
		fdir = 'failed'
		try:
			fdir = uda_transfer(pulse_ID,'epm')
			if fdir!='failed':
				print('uda_transfer successfull')
			else:
				print('uda_transfer fail')
		except:
			pass
		if os.path.exists(path) or fdir!='failed':
			try:
				if fdir!='failed':
					f = abortable_worker(netCDF4.Dataset,10,fdir)	# 5 seconds timeout
					os.remove(fdir)
				else:
					f = abortable_worker(netCDF4.Dataset,10,path)	# 5 seconds timeout
				# f = netCDF4.Dataset(fdir)
				if f==False:
					print(path+' reading failed')
					sba=sgna	# I want this to generate error

				try:
					self.psidat = np.transpose(f['output']['profiles2D']['poloidalFlux'], (0, 2, 1))
					file_prefix = ''
				except IndexError:
					self.psidat = np.transpose(f['/epm/output']['profiles2D']['poloidalFlux'], (0, 2, 1))
					file_prefix = '/epm'

				self.q0 = f[file_prefix+'/output']['globalParameters']['q0'][:].data
				self.q95 = f[file_prefix+'/output']['globalParameters']['q95'][:].data

				try:
					self.r = f[file_prefix+'/output']['profiles2D']['r'][:].data
					self.z = f[file_prefix+'/output']['profiles2D']['z'][:].data
					self.R = self.r[0,:]
					self.Z = self.z[0,:]
					self.bVac = f[file_prefix+'/input']['bVacRadiusProduct']['values'][:].data
					self.r_axis = f[file_prefix+'/output']['globalParameters']['magneticAxis'][:]['R']
					self.z_axis = f[file_prefix+'/output']['globalParameters']['magneticAxis'][:]['Z']
					self.shotnumber = f.pulseNumber
					self.strikepointR = f[file_prefix+'/output']['separatrixGeometry']['strikepointCoords'][:]['R']
					self.strikepointZ = f[file_prefix+'/output']['separatrixGeometry']['strikepointCoords'][:]['Z']
				except IndexError:
					self.R = f[file_prefix+'/output']['profiles2D']['r'][:].data
					self.Z = f[file_prefix+'/output']['profiles2D']['z'][:].data
					self.bVac = f[file_prefix+'/input']['bVacRadiusProduct'][:].data
					self.r_axis = f[file_prefix+'/output']['globalParameters']['magneticAxis']['R'][:]
					self.z_axis = f[file_prefix+'/output']['globalParameters']['magneticAxis']['Z'][:]
					self.shotnumber = f.shot
					self.strikepointR = f[file_prefix+'/output']['separatrixGeometry']['strikepointR'][:].data
					self.strikepointZ = f[file_prefix+'/output']['separatrixGeometry']['strikepointZ'][:].data

				self.psi_bnd = f[file_prefix+'/output']['globalParameters']['psiBoundary'][:].data
				self.psi_axis = f[file_prefix+'/output']['globalParameters']['psiAxis'][:].data
				self.cpasma = f[file_prefix+'/output']['globalParameters']['plasmaCurrent'][:].data
				self.time = f[file_prefix+'/time'][:].data

				f.close()
				netCDF4_read_failed = False
			except:
				print('netCDF4_read_failed')
		if netCDF4_read_failed:
			# efit_reconstruction = read_uda(pulse_ID)
			from mastu_exhaust_analysis.read_efit import read_uda
			efit_reconstruction = abortable_worker(read_uda,10,pulse_ID)	# 10 seconds timeout
			if efit_reconstruction==False:
				sba=sgna	# I want this to generate error

			self.psidat = efit_reconstruction['psi']
			self.q0 = efit_reconstruction['q0']
			self.q95 = efit_reconstruction['q95']
			self.R = efit_reconstruction['r']
			self.Z = efit_reconstruction['z']
			self.bVac = efit_reconstruction['RBphi']
			self.r_axis = efit_reconstruction['r_axis']
			self.z_axis = efit_reconstruction['z_axis']
			self.shotnumber = efit_reconstruction['shot']
			# self.strikepointR = efit_reconstruction['strikepointR']
			# self.strikepointZ = efit_reconstruction['strikepointZ']
			client_int=pyuda.Client()
			self.strikepointR = np.array([efit_reconstruction['outer_strikepoint_upper_r'],efit_reconstruction['outer_strikepoint_lower_r'],client_int.get('epm/output/separatrixGeometry/dndXpoint2InnerStrikepointR',source=pulse_ID).data,client_int.get('epm/output/separatrixGeometry/dndXpoint1InnerStrikepointR',source=pulse_ID).data]).T
			self.strikepointR[np.isnan(self.strikepointR)] = 0
			self.strikepointZ = np.array([efit_reconstruction['outer_strikepoint_upper_z'],efit_reconstruction['outer_strikepoint_lower_z'],client_int.get('epm/output/separatrixGeometry/dndXpoint2InnerStrikepointZ',source=pulse_ID).data,client_int.get('epm/output/separatrixGeometry/dndXpoint1InnerStrikepointZ',source=pulse_ID).data]).T
			client_int.reset_connection()
			# reset_connection(client_int)
			# del client_int
			self.strikepointZ[np.isnan(self.strikepointZ)] = 0
			self.psi_bnd = efit_reconstruction['psi_boundary']
			self.psi_axis = efit_reconstruction['psi_axis']
			self.cpasma = efit_reconstruction['Ip']
			self.time = efit_reconstruction['t']

		# Calculate the time history of some useful quantites, i.e.
		# Inner and outer LCFS positions
		# Strike point positions
		# q0 at some point?

		print('Calculating equilibrium properties...')

		self.lower_xpoint_r = np.zeros(len(self.time))
		self.lower_xpoint_z = np.zeros(len(self.time))
		self.lower_xpoint_p = np.zeros(len(self.time))
		self.upper_xpoint_r = np.zeros(len(self.time))
		self.upper_xpoint_z = np.zeros(len(self.time))
		self.upper_xpoint_p = np.zeros(len(self.time))

		self.mag_axis_r = np.zeros(len(self.time))
		self.mag_axis_z = np.zeros(len(self.time))
		self.mag_axis_p = np.zeros(len(self.time))

		self.inner_sep_r = np.zeros(len(self.time))
		self.outer_sep_r = np.zeros(len(self.time))

		rr, zz = np.meshgrid(self.R, self.Z)

		if self.psidat is not None:
			for i in np.arange(len(self.time)):
				psiarr = np.array((self.psidat[i,:,:]))
				psi_interp = self.interp2d(self.R, self.Z, psiarr)

				if np.sum(np.isfinite(psiarr)) == np.size(psiarr):
					# Find the position of the xpoints
					opoint, xpoint = self.find_critical(rr.T,zz.T, psiarr.T)

					if len(xpoint) > 0:
						xpt1r = xpoint[0][0]
						xpt1z = xpoint[0][1]
						xpt1p = xpoint[0][2]

						if len(xpoint) > 1:
							xpt2r = xpoint[1][0]
							xpt2z = xpoint[1][1]
							xpt2p = xpoint[1][2]
						else:
							xpt2r = None
							xpt2z = None
							xpt2p = None

						self.mag_axis_r[i] = opoint[0][0]
						self.mag_axis_z[i] = opoint[0][1]
						self.mag_axis_p[i] = opoint[0][2]

						if xpt1z < 0:
							self.lower_xpoint_r[i] = xpt1r
							self.lower_xpoint_z[i] = xpt1z
							self.lower_xpoint_p[i] = xpt1p

						if xpt1z > 0:
							self.upper_xpoint_r[i] = xpt1r
							self.upper_xpoint_z[i] = xpt1z
							self.upper_xpoint_p[i] = xpt1p

						if xpt2z and  xpt2z < 0:
							self.lower_xpoint_r[i] = xpt2r
							self.lower_xpoint_z[i] = xpt2z
							self.lower_xpoint_p[i] = xpt2p

						if xpt2z and xpt2z > 0:
							self.upper_xpoint_r[i] = xpt2r
							self.upper_xpoint_z[i] = xpt2z
							self.upper_xpoint_p[i] = xpt2p

					mp_r_arr = np.linspace(np.min(self.R), np.max(self.R),500)
					mp_p_arr = mp_r_arr*0.0

					for j in np.arange(len(mp_p_arr)):
							mp_p_arr[j] = psi_interp(mp_r_arr[j], self.mag_axis_z[i])

					zcr = self.zcd(mp_p_arr-self.psi_bnd[i])

					# if len(zcr) > 2:	# it should literally have no effect
					# 	zcr = zcr[-2:]
					if len(zcr) < 1:
						zcr = [0,0]
					elif len(zcr) < 2:	# made to prevent the error when there is only one zero in (mp_p_arr-self.psi_bnd[i])
						zcr.append(zcr[0])

					self.inner_sep_r[i] = mp_r_arr[zcr[0]]
					self.outer_sep_r[i] = mp_r_arr[zcr[1]]


					# Calculate dr_sep

	def interp2d(self,R,Z,field):
		return RectBivariateSpline(R,Z,np.transpose(field))


	def find_critical(self,R, Z, psi, discard_xpoints=True):
		"""
		Find critical points

		Inputs
		------

		R - R(nr, nz) 2D array of major radii
		Z - Z(nr, nz) 2D array of heights
		psi - psi(nr, nz) 2D array of psi values

		Returns
		-------

		Two lists of critical points

		opoint, xpoint

		Each of these is a list of tuples with (R, Z, psi) points

		The first tuple is the primary O-point (magnetic axis)
		and primary X-point (separatrix)

		"""

		# Get a spline interpolation function
		f = interpolate.RectBivariateSpline(R[:, 0], Z[0, :], psi)

		# Find candidate locations, based on minimising Bp^2
		Bp2 = (f(R, Z, dx=1, grid=False) ** 2 + f(R, Z, dy=1, grid=False) ** 2) / R ** 2

		# Get grid resolution, which determines a reasonable tolerance
		# for the Newton iteration search area
		dR = R[1, 0] - R[0, 0]
		dZ = Z[0, 1] - Z[0, 0]
		radius_sq = 9 * (dR ** 2 + dZ ** 2)

		# Find local minima

		J = np.zeros([2, 2])

		xpoint = []
		opoint = []

		nx, ny = Bp2.shape
		for i in range(2, nx - 2):
			for j in range(2, ny - 2):
				if (
					(Bp2[i, j] < Bp2[i + 1, j + 1])
					and (Bp2[i, j] < Bp2[i + 1, j])
					and (Bp2[i, j] < Bp2[i + 1, j - 1])
					and (Bp2[i, j] < Bp2[i - 1, j + 1])
					and (Bp2[i, j] < Bp2[i - 1, j])
					and (Bp2[i, j] < Bp2[i - 1, j - 1])
					and (Bp2[i, j] < Bp2[i, j + 1])
					and (Bp2[i, j] < Bp2[i, j - 1])
				):

					# Found local minimum

					R0 = R[i, j]
					Z0 = Z[i, j]

					# Use Newton iterations to find where
					# both Br and Bz vanish
					R1 = R0
					Z1 = Z0

					count = 0
					while True:

						Br = -f(R1, Z1, dy=1, grid=False) / R1
						Bz = f(R1, Z1, dx=1, grid=False) / R1

						if Br ** 2 + Bz ** 2 < 1e-6:
							# Found a minimum. Classify as either
							# O-point or X-point

							dR = R[1, 0] - R[0, 0]
							dZ = Z[0, 1] - Z[0, 0]
							d2dr2 = (psi[i + 2, j] - 2.0 * psi[i, j] + psi[i - 2, j]) / (
								2.0 * dR
							) ** 2
							d2dz2 = (psi[i, j + 2] - 2.0 * psi[i, j] + psi[i, j - 2]) / (
								2.0 * dZ
							) ** 2
							d2drdz = (
								(psi[i + 2, j + 2] - psi[i + 2, j - 2]) / (4.0 * dZ)
								- (psi[i - 2, j + 2] - psi[i - 2, j - 2]) / (4.0 * dZ)
							) / (4.0 * dR)
							D = d2dr2 * d2dz2 - d2drdz ** 2

							if D < 0.0:
								# Found X-point
								xpoint.append((R1, Z1, f(R1, Z1)[0][0]))
							else:
								# Found O-point
								opoint.append((R1, Z1, f(R1, Z1)[0][0]))
							break

						# Jacobian matrix
						# J = ( dBr/dR, dBr/dZ )
						#	 ( dBz/dR, dBz/dZ )

						J[0, 0] = -Br / R1 - f(R1, Z1, dy=1, dx=1)[0][0] / R1
						J[0, 1] = -f(R1, Z1, dy=2)[0][0] / R1
						J[1, 0] = -Bz / R1 + f(R1, Z1, dx=2) / R1
						J[1, 1] = f(R1, Z1, dx=1, dy=1)[0][0] / R1

						d = np.dot(np.linalg.inv(J), [Br, Bz])

						R1 = R1 - d[0]
						Z1 = Z1 - d[1]

						count += 1
						# If (R1,Z1) is too far from (R0,Z0) then discard
						# or if we've taken too many iterations
						if ((R1 - R0) ** 2 + (Z1 - Z0) ** 2 > radius_sq) or (count > 100):
							# Discard this point
							break

		# Remove duplicates
		def remove_dup(points):
			result = []
			for n, p in enumerate(points):
				dup = False
				for p2 in result:
					if (p[0] - p2[0]) ** 2 + (p[1] - p2[1]) ** 2 < 1e-5:
						dup = True  # Duplicate
						break
				if not dup:
					result.append(p)  # Add to the list
			return result

		xpoint = remove_dup(xpoint)
		opoint = remove_dup(opoint)

		if len(opoint) == 0:
			# Can't order primary O-point, X-point so return
			print("Warning: No O points found")
			return opoint, xpoint

		# Find primary O-point by sorting by distance from middle of domain
		Rmid = 0.5 * (R[-1, 0] + R[0, 0])
		Zmid = 0.5 * (Z[0, -1] + Z[0, 0])
		opoint.sort(key=lambda x: (x[0] - Rmid) ** 2 + (x[1] - Zmid) ** 2)

		# Draw a line from the O-point to each X-point. Psi should be
		# monotonic; discard those which are not

		if discard_xpoints:
			Ro, Zo, Po = opoint[0]  # The primary O-point
			xpt_keep = []
			for xpt in xpoint:
				Rx, Zx, Px = xpt

				rline = np.linspace(Ro, Rx, num=50)
				zline = np.linspace(Zo, Zx, num=50)

				pline = f(rline, zline, grid=False)

				if Px < Po:
					pline *= -1.0  # Reverse, so pline is maximum at X-point

				# Now check that pline is monotonic
				# Tried finding maximum (argmax) and testing
				# how far that is from the X-point. This can go
				# wrong because psi can be quite flat near the X-point
				# Instead here look for the difference in psi
				# rather than the distance in space

				maxp = np.amax(pline)
				if (maxp - pline[-1]) / (maxp - pline[0]) > 0.001:
					# More than 0.1% drop in psi from maximum to X-point
					# -> Discard
					continue

				ind = np.argmin(pline)  # Should be at O-point
				if (rline[ind] - Ro) ** 2 + (zline[ind] - Zo) ** 2 > 1e-4:
					# Too far, discard
					continue
				xpt_keep.append(xpt)
			xpoint = xpt_keep

		# Sort X-points by distance to primary O-point in psi space
		psi_axis = opoint[0][2]
		xpoint.sort(key=lambda x: (x[2] - psi_axis) ** 2)

		return opoint, xpoint


	def zcd(self, data):
		sign_array=np.sign(data)
		out=[]
		for i in np.arange(1,len(sign_array)):
			if sign_array[i] != sign_array[i-1]:
				out.append(i)
		return out

############################################################################################################################################################################################################################################################################
# from https://gist.github.com/derricw/95eab740e1b08b78c03f
def bin_ndarray(ndarray, new_shape, operation='sum',running_time=False):
	from scipy.ndimage import generic_filter
	"""
	Bins an ndarray in all axes based on the target shape, by summing or
		averaging.

	Number of output dimensions must match number of input dimensions and
		new axes must divide old ones.

	Example
	-------
	>>> m = np.arange(0,100,1).reshape((10,10))
	>>> n = bin_ndarray(m, new_shape=(5,5), operation='sum')
	>>> print(n)

	[[ 22  30  38  46  54]
	 [102 110 118 126 134]
	 [182 190 198 206 214]
	 [262 270 278 286 294]
	 [342 350 358 366 374]]

	"""
	# if operation=='np.nanmean':
	# 	operation='mean'
	# if operation=='np.nansum':
	# 	operation='sum'

	operation = operation.lower()
	if not operation in ['sum', 'mean','np.nansum','np.nanmean','np.nanstd']:
		raise ValueError("Operation not supported.")
	if ndarray.ndim != len(new_shape):
		raise ValueError("Shape mismatch: {} -> {}".format(ndarray.shape,new_shape))

	if running_time:
		real_time_binning = np.ceil(np.shape(ndarray)[0]/new_shape[0]).astype(int)
		new_shape[0] = np.shape(ndarray)[0]
	compression_pairs = [(d, c//d) for d,c in zip(new_shape,ndarray.shape)]
	flattened = [l for p in compression_pairs for l in p]
	ndarray = ndarray.reshape(flattened)
	for i in range(len(new_shape)):
		if (not running_time) or i!=len(new_shape)-1:
			if operation in ['mean','sum']:
				op = getattr(ndarray, operation)
				ndarray = op(-1*(i+1))
			elif operation in ['np.nansum','np.nanmean','np.nanstd']:
				ndarray = eval(operation)(ndarray, axis=-1*(i+1) )
		else:
			ndarray = ndarray[:,0]
			ndarray = generic_filter(ndarray,eval(operation),size=[real_time_binning]+np.ones((len(new_shape)-1)).astype(int).tolist(),mode='nearest')
	return ndarray

def proper_homo_binning_t_2D(data,shrink_factor_t,shrink_factor_x,type='np.nanmean',running_time=False):
	old_shape = np.array(np.shape(data))
	new_shape=np.array([int(np.ceil(old_shape[0]/shrink_factor_t)),int(np.ceil(old_shape[1]/shrink_factor_x)),int(np.ceil(old_shape[2]/shrink_factor_x))]).astype(int)
	if running_time:
		to_pad=np.array([0,(shrink_factor_x-old_shape[1]%shrink_factor_x)*(old_shape[1]%shrink_factor_x>0),(shrink_factor_x-old_shape[2]%shrink_factor_x)*(old_shape[2]%shrink_factor_x>0)]).astype(int)
	else:
		to_pad=np.array([(shrink_factor_t-old_shape[0]%shrink_factor_t)*(old_shape[0]%shrink_factor_t>0),(shrink_factor_x-old_shape[1]%shrink_factor_x)*(old_shape[1]%shrink_factor_x>0),(shrink_factor_x-old_shape[2]%shrink_factor_x)*(old_shape[2]%shrink_factor_x>0)]).astype(int)
	to_pad_right = to_pad//2
	to_pad_left = to_pad - to_pad_right
	to_pad = np.array([to_pad_left,to_pad_right]).T
	# data_binned = np.pad(data,to_pad,mode='mean',stat_length=((max(1,shrink_factor_t//2),max(1,shrink_factor_t//2)),(max(1,shrink_factor_x//2),max(1,shrink_factor_x//2)),(max(1,shrink_factor_x//2),max(1,shrink_factor_x//2))))
	data_binned = np.pad(np.array(data).astype(float),to_pad,mode='constant',constant_values=np.nan)
	data_binned = bin_ndarray(data_binned, new_shape=new_shape, operation=type,running_time=running_time)
	nan_ROI_mask = np.isfinite(np.nanmedian(data_binned[:10],axis=0))
	return data_binned,nan_ROI_mask

def proper_homo_binning_2D(data,shrink_factor_x,type='np.nanmean',shrink_factor_x2=0):
	if shrink_factor_x2==0:
		shrink_factor_x2 = shrink_factor_x
	old_shape = np.array(np.shape(data))
	new_shape=np.array([int(np.ceil(old_shape[0]/shrink_factor_x)),int(np.ceil(old_shape[1]/shrink_factor_x2))]).astype(int)
	to_pad=np.array([(shrink_factor_x-old_shape[0]%shrink_factor_x)*(old_shape[0]%shrink_factor_x>0),(shrink_factor_x2-old_shape[1]%shrink_factor_x2)*(old_shape[1]%shrink_factor_x2>0)]).astype(int)
	to_pad_right = to_pad//2
	to_pad_left = to_pad - to_pad_right
	to_pad = np.array([to_pad_left,to_pad_right]).T
	# data_binned = np.pad(data,to_pad,mode='mean',stat_length=((max(1,shrink_factor_x//2),max(1,shrink_factor_x//2)),(max(1,shrink_factor_x//2),max(1,shrink_factor_x//2))))
	data_binned = np.pad(np.array(data).astype(float),to_pad,mode='constant',constant_values=np.nan)
	data_binned = bin_ndarray(data_binned, new_shape=new_shape, operation=type)
	return data_binned

def proper_homo_binning_1D_1D_1D(data,shrink_factor_x_1,shrink_factor_x_2,shrink_factor_v,type='np.nanmean'):	# v stands for voxel, because this will be used for the sensitivity matrix
	old_shape = np.array(np.shape(data))
	new_shape=np.array([int(np.ceil(old_shape[0]/shrink_factor_x_1)),int(np.ceil(old_shape[1]/shrink_factor_x_2)),int(np.ceil(old_shape[2]/shrink_factor_v))]).astype(int)
	to_pad=np.array([(shrink_factor_x_1-old_shape[0]%shrink_factor_x_1)*(old_shape[0]%shrink_factor_x_1>0),(shrink_factor_x_2-old_shape[1]%shrink_factor_x_2)*(old_shape[1]%shrink_factor_x_2>0),(shrink_factor_v-old_shape[2]%shrink_factor_v)*(old_shape[2]%shrink_factor_v>0)]).astype(int)
	to_pad_right = to_pad//2
	to_pad_left = to_pad - to_pad_right
	to_pad = np.array([to_pad_left,to_pad_right]).T
	# data_binned = np.pad(data,to_pad,mode='mean',stat_length=((max(1,shrink_factor_x_1//2),max(1,shrink_factor_x_1//2)),(max(1,shrink_factor_x_2//2),max(1,shrink_factor_x_2//2)),(max(1,shrink_factor_v//2),max(1,shrink_factor_v//2))))
	data_binned = np.pad(np.array(data).astype(float),to_pad,mode='constant',constant_values=np.nan)
	data_binned = bin_ndarray(data_binned, new_shape=new_shape, operation=type)
	return data_binned

def proper_homo_binning_t(time,shrink_factor_t,type='np.nanmean',running_time=False):
	old_shape = np.array(np.shape(time))
	new_shape=np.array([int(np.ceil(old_shape[0]/shrink_factor_t))]).astype(int)
	if running_time:
		to_pad=np.array([0]).astype(int)
	else:
		to_pad=np.array([(shrink_factor_t-old_shape[0]%shrink_factor_t)*(old_shape[0]%shrink_factor_t>0)]).astype(int)
	to_pad_right = to_pad//2
	to_pad_left = to_pad - to_pad_right
	to_pad = np.array([to_pad_left,to_pad_right]).T
	# time_binned = np.pad(time,to_pad,mode='mean',stat_length=((max(1,shrink_factor_t//2),max(1,shrink_factor_t//2))))
	time_binned = np.pad(np.array(time).astype(float),to_pad,mode='constant',constant_values=np.nan)
	time_binned = bin_ndarray(time_binned, new_shape=new_shape, operation=type,running_time=running_time)
	# time_binned[0] = time_binned[1] - np.median(np.diff(time_binned[1:-1]))	# I'm not sure if it's proper to leave this lines, because with the binning it can actually be right that all dt are not the same. I remove it for now
	# time_binned[-1] = time_binned[-2] + np.median(np.diff(time_binned[1:-1]))
	return time_binned

def efit_reconstruction_to_separatrix_on_foil(efit_reconstruction,refinement=1000):
	import time as tm
	from scipy.signal import find_peaks, peak_prominences as get_proms
	from scipy.interpolate import interp2d
	import mastu_exhaust_analysis.fluxsurface_tracer as ft
	from mastu_exhaust_analysis.read_efit import read_epm

	start_efit_reconstruction_to_separatrix_on_foil = tm.time()
	fdir = uda_transfer(efit_reconstruction.shotnumber,'epm')
	efit_data = read_epm(fdir,calc_bfield=True)
	os.remove(fdir)

	all_time_sep_r = []
	all_time_sep_z = []
	# r_fine = np.unique(np.linspace(efit_reconstruction.R.min(),efit_reconstruction.R.max(),refinement).tolist() + np.linspace(R_centre_column-0.01,R_centre_column+0.08,refinement).tolist())
	r_fine = np.unique(np.linspace(efit_reconstruction.R.min(),efit_reconstruction.R.max(),refinement).tolist() + np.linspace(R_centre_column-0.01,R_centre_column+0.08,refinement).tolist() + np.linspace(1,1.7,refinement).tolist() + np.linspace(0.5,0.7,refinement).tolist())
	r_fine = r_fine[r_fine>=R_centre_column-0.01]	# this is to avoid ambiguity around the x-point
	# z_fine = np.linspace(efit_reconstruction.Z.min(),efit_reconstruction.Z.max(),refinement)
	z_fine = np.unique(np.linspace(efit_reconstruction.Z.min(),efit_reconstruction.Z.max(),refinement).tolist() + np.linspace(-1.,-2,refinement).tolist() + np.linspace(1.,2,refinement).tolist())
	interp1 = interp1d([1.1,1.5],[-1.5,-1.75],fill_value="extrapolate",bounds_error=False)
	interp2 = interp1d([1.1,1.5],[-1.5,-1.2],fill_value="extrapolate",bounds_error=False)
	for time in range(len(efit_reconstruction.time)):
		try:	# let's do the sensible thing: use already mabe and more trustworthy SW
			if False:
				sdfsdaf = asdfsdf	# I want to generate an error and use the older process
			margin_away_from_x_points = 0.02
			temp_max = 0.2
			temp = 0.03
			trace_flux_surface = ft.trace_flux_surface(efit_data, time,efit_reconstruction.lower_xpoint_r[time]-temp,efit_reconstruction.lower_xpoint_z[time])
			# the following loop is to avoid that I start too close to the x-point and the trace is inside the separatrix rather than outside
			while (np.nanmin(trace_flux_surface.z)>efit_reconstruction.lower_xpoint_z[time]-margin_away_from_x_points) and temp<temp_max:
				temp += 0.01
				trace_flux_surface = ft.trace_flux_surface(efit_data, time,efit_reconstruction.lower_xpoint_r[time]-temp,efit_reconstruction.lower_xpoint_z[time])
				# print('separatrix left_low moved time %.3g, %.3g' %(efit_reconstruction.time[time],temp))
			left_low = np.array(trace_flux_surface.r)
			left_low_z = np.array(trace_flux_surface.z)
			left_low,left_low_z = linear_interpolation_2D(left_low,left_low_z,5)	# refinement stage create to help my functions like track_outer_leg_radiation later
			left_low = np.round(np.interp(left_low, r_fine,np.arange(len(r_fine)))).astype(int)
			left_low_z = np.round(np.interp(left_low_z, z_fine,np.arange(len(z_fine)))).astype(int)
			temp = 0.03
			trace_flux_surface = ft.trace_flux_surface(efit_data, time,efit_reconstruction.lower_xpoint_r[time]+temp,efit_reconstruction.lower_xpoint_z[time])
			while (np.nanmin(trace_flux_surface.z)>efit_reconstruction.lower_xpoint_z[time]-margin_away_from_x_points) and temp<temp_max:
				temp += 0.01
				trace_flux_surface = ft.trace_flux_surface(efit_data, time,efit_reconstruction.lower_xpoint_r[time]+temp,efit_reconstruction.lower_xpoint_z[time])
				# print('separatrix right_low moved time %.3g, %.3g' %(efit_reconstruction.time[time],temp))
			right_low = np.array(trace_flux_surface.r)
			right_low_z = np.array(trace_flux_surface.z)
			right_low,right_low_z = linear_interpolation_2D(right_low,right_low_z,5)	# refinement stage create to help my functions like track_outer_leg_radiation later
			right_low = np.round(np.interp(right_low, r_fine,np.arange(len(r_fine)))).astype(int)
			right_low_z = np.round(np.interp(right_low_z, z_fine,np.arange(len(z_fine)))).astype(int)
			temp = 0.03
			trace_flux_surface = ft.trace_flux_surface(efit_data, time,efit_reconstruction.upper_xpoint_r[time]-temp,efit_reconstruction.upper_xpoint_z[time])
			while (np.nanmax(trace_flux_surface.z)<efit_reconstruction.upper_xpoint_z[time]+margin_away_from_x_points) and temp<temp_max:
				temp += 0.01
				trace_flux_surface = ft.trace_flux_surface(efit_data, time,efit_reconstruction.upper_xpoint_r[time]-temp,efit_reconstruction.upper_xpoint_z[time])
				# print('separatrix left_up moved time %.3g, %.3g' %(efit_reconstruction.time[time],temp))
			left_up = np.array(trace_flux_surface.r)
			left_up_z = np.array(trace_flux_surface.z)
			left_up,left_up_z = linear_interpolation_2D(left_up,left_up_z,5)	# refinement stage create to help my functions like track_outer_leg_radiation later
			left_up = np.round(np.interp(left_up, r_fine,np.arange(len(r_fine)))).astype(int)
			left_up_z = np.round(np.interp(left_up_z, z_fine,np.arange(len(z_fine)))).astype(int)
			temp = 0.03
			trace_flux_surface = ft.trace_flux_surface(efit_data, time,efit_reconstruction.upper_xpoint_r[time]+temp,efit_reconstruction.upper_xpoint_z[time])
			while (np.nanmax(trace_flux_surface.z)<efit_reconstruction.upper_xpoint_z[time]+margin_away_from_x_points) and temp<temp_max:
				temp += 0.01
				trace_flux_surface = ft.trace_flux_surface(efit_data, time,efit_reconstruction.upper_xpoint_r[time]+temp,efit_reconstruction.upper_xpoint_z[time])
				# print('separatrix right_up moved time %.3g, %.3g' %(efit_reconstruction.time[time],temp))
			right_up = np.array(trace_flux_surface.r)
			right_up_z = np.array(trace_flux_surface.z)
			right_up,right_up_z = linear_interpolation_2D(right_up,right_up_z,5)	# refinement stage create to help my functions like track_outer_leg_radiation later
			right_up = np.round(np.interp(right_up, r_fine,np.arange(len(r_fine)))).astype(int)
			right_up_z = np.round(np.interp(right_up_z, z_fine,np.arange(len(z_fine)))).astype(int)
		except:
			gna = efit_reconstruction.psidat[time]
			sep_up = efit_reconstruction.upper_xpoint_p[time]
			sep_low = efit_reconstruction.lower_xpoint_p[time]
			x_point_z_proximity = np.abs(np.nanmin([efit_reconstruction.upper_xpoint_z[time],efit_reconstruction.lower_xpoint_z[time],-0.573-0.2]))-0.2	# -0.573 is an arbitrary treshold in case both are nan

			# this scans psi for each z and finds radius of the peaks
			psi_interpolator = interp2d(efit_reconstruction.R,efit_reconstruction.Z,gna,kind='cubic')
			psi = psi_interpolator(r_fine,z_fine)
			psi_up = -np.abs(psi-sep_up)
			psi_low = -np.abs(psi-sep_low)
			all_peaks_up = []
			all_z_up = []
			all_peaks_low = []
			all_z_low = []
			z_array = np.arange(len(z_fine))
			# z_array = np.array([np.concatenate((z_array[z_fine>=0],np.flip(z_array[z_fine<0],axis=0))),np.concatenate((z_fine[z_fine>=0],np.flip(z_fine[z_fine<0],axis=0)))]).T	# I'm not sure why I was rearranginf z like this
			z_array = np.array([z_array,z_fine]).T
			# for i_z,z in enumerate(z_fine):
			for i_z,z in z_array:
				i_z = int(i_z)
				# psi_z = psi[i_z]
				peaks = find_peaks(psi_up[i_z])[0]
				all_peaks_up.append(peaks)
				all_z_up.append([i_z]*len(peaks))
				peaks = find_peaks(psi_low[i_z])[0]
				all_peaks_low.append(peaks)
				all_z_low.append([i_z]*len(peaks))
			all_peaks_up = np.concatenate(all_peaks_up).astype(int)
			all_z_up = np.concatenate(all_z_up).astype(int)
			found_psi_up = np.abs(psi_up[all_z_up,all_peaks_up])
			all_peaks_low = np.concatenate(all_peaks_low).astype(int)
			all_z_low = np.concatenate(all_z_low).astype(int)
			found_psi_low = np.abs(psi_low[all_z_low,all_peaks_low])

			# this is to remove small peaks found
			# plt.figure()
			# plt.plot(z_fine[all_z_up[found_psi_up<(gna.max()-gna.min())/500]],r_fine[all_peaks_up[found_psi_up<(gna.max()-gna.min())/500]],'+b')
			# plt.plot(z_fine[all_z_low[found_psi_low<(gna.max()-gna.min())/500]],r_fine[all_peaks_low[found_psi_low<(gna.max()-gna.min())/500]],'+r')
			all_peaks_up = all_peaks_up[found_psi_up<(gna.max()-gna.min())/500]
			all_z_up = all_z_up[found_psi_up<(gna.max()-gna.min())/500]
			all_peaks_low = all_peaks_low[found_psi_low<(gna.max()-gna.min())/500]
			all_z_low = all_z_low[found_psi_low<(gna.max()-gna.min())/500]

			# I add the x-points
			all_peaks_up = np.concatenate([all_peaks_up,[np.abs(r_fine-efit_reconstruction.lower_xpoint_r[time]).argmin()]]).astype(int)
			all_z_up = np.concatenate([all_z_up,[z_array[:,0][np.abs(z_array[:,1]-efit_reconstruction.lower_xpoint_z[time]).argmin()]]]).astype(int)
			all_peaks_low = np.concatenate([all_peaks_low,[np.abs(r_fine-efit_reconstruction.upper_xpoint_r[time]).argmin()]]).astype(int)
			all_z_low = np.concatenate([all_z_low,[z_array[:,0][np.abs(z_array[:,1]-efit_reconstruction.upper_xpoint_z[time]).argmin()]]]).astype(int)


			# this cuts the sections inside the nose. dunno why it is needed, now
			# plt.figure()
			# plt.plot(z_fine[all_z_up],r_fine[all_peaks_up],'+b')
			# plt.plot(z_fine[all_z_low],r_fine[all_peaks_low],'+r')
			# plt.pause(0.01)
			select = np.logical_or( interp1(r_fine[all_peaks_up])>z_fine[all_z_up] , interp2(r_fine[all_peaks_up])<z_fine[all_z_up] )
			all_peaks_up = all_peaks_up[select]
			all_z_up = all_z_up[select]
			select = np.logical_or( interp1(r_fine[all_peaks_low])>z_fine[all_z_low] , interp2(r_fine[all_peaks_low])<z_fine[all_z_low] )
			all_peaks_low = all_peaks_low[select]
			all_z_low = all_z_low[select]

			if True:	# this takes the scanned sections in z and picks the innermost one for inner separatrix and outermost as outer separatrix
				left_up = []
				right_up = []
				left_up_z = []
				right_up_z = []
				left_low = []
				right_low = []
				left_low_z = []
				right_low_z = []
				# for i_z,z in enumerate(z_fine):
				for i_z,z in z_array:
					i_z = int(i_z)
					if i_z in all_z_up:
						temp = all_peaks_up[all_z_up==i_z]
						if len(temp) == 1:
							right_up.append(temp[0])
							right_up_z.append(i_z)
						elif len(temp) == 2:
							# # if r_fine[temp.min()]>R_centre_column or np.abs(z)<x_point_z_proximity:
							# if r_fine[temp.min()]>R_centre_column_interpolator(-np.abs(z)):
							if np.abs(z)>1.5 and np.sum(r_fine[temp]<0.8)==0:	# this identifies the condition when the outer separatrix curles up and it gets split in inner and outer
								right_up.append(temp[np.abs(temp - right_up[-1]).argmin()])
								right_up_z.append(i_z)
							else:
								left_up.append(temp.min())
								left_up_z.append(i_z)
								right_up.append(temp.max())
								right_up_z.append(i_z)
						elif len(temp) == 3:
							if np.abs(z)>1.5 and np.sum(r_fine[temp]<0.8)==0:	# this identifies the condition when the outer separatrix curles up and it gets split in inner and outer
								right_up.append(temp[np.abs(temp - right_up[-1]).argmin()])
								right_up_z.append(i_z)
							else:
								left_up.append(np.sort(temp)[1])
								left_up_z.append(i_z)
								right_up.append(temp.max())
								right_up_z.append(i_z)
						elif len(temp) == 4:
							left_up.append(np.sort(temp)[1])
							left_up_z.append(i_z)
							right_up.append(np.sort(temp)[2])
							right_up_z.append(i_z)
					if i_z in all_z_low:
						temp = all_peaks_low[all_z_low==i_z]
						if len(temp) == 1:
							right_low.append(temp[0])
							right_low_z.append(i_z)
						elif len(temp) == 2:
							# # if r_fine[temp.min()]>R_centre_column or np.abs(z)<x_point_z_proximity:
							# if r_fine[temp.min()]>R_centre_column_interpolator(-np.abs(z)):
							if np.abs(z)>1.5 and np.sum(r_fine[temp]<0.8)==0:	# this identifies the condition when the outer separatrix curles up and it gets split in inner and outer
								right_low.append(temp[np.abs(temp - right_low[-1]).argmin()])
								right_low_z.append(i_z)
							else:
								left_low.append(temp.min())
								left_low_z.append(i_z)
								right_low.append(temp.max())
								right_low_z.append(i_z)
						elif len(temp) == 3:
							if np.abs(z)>1.5 and np.sum(r_fine[temp]<0.8)==0:	# this identifies the condition when the outer separatrix curles up and it gets split in inner and outer
								right_low.append(temp[np.abs(temp - right_low[-1]).argmin()])
								right_low_z.append(i_z)
							else:
								left_low.append(np.sort(temp)[1])
								left_low_z.append(i_z)
								right_low.append(temp.max())
								right_low_z.append(i_z)
						elif len(temp) == 4:
							left_low.append(np.sort(temp)[1])
							left_low_z.append(i_z)
							right_low.append(np.sort(temp)[2])
							right_low_z.append(i_z)
				# sep_r = [left_up,right_up,left_low,right_low]
				# sep_z = [left_up_z,right_up_z,left_low_z,right_low_z]
				left_up = np.array([y for _, y in sorted(zip(left_up_z, left_up))])
				left_up_z = np.sort(left_up_z)
				right_up = np.array([y for _, y in sorted(zip(right_up_z, right_up))])
				right_up_z = np.sort(right_up_z)
				left_low = np.array([y for _, y in sorted(zip(left_low_z, left_low))])
				left_low_z = np.sort(left_low_z)
				right_low = np.array([y for _, y in sorted(zip(right_low_z, right_low))])
				right_low_z = np.sort(right_low_z)
			elif False:	# something I can do is to build a crowler that starts from the HFS or LFS and finds the closer point sticking to the previous that stays on the left or right
				# inner side
				last_point = np.abs((z_fine[all_z_up]-0)**2 + (r_fine[all_peaks_up]-R_centre_column)**2).argmin()	# point closer to midplane
				# z is in increasing direction
				# I crawl first up. the point is to stay always on the left
				radious_of_search = 0.05	# m
				next_point = ((z_fine[all_z_up]-z_fine[all_z_up][last_point])**2 + (r_fine[all_peaks_up]-r_fine[all_peaks_up][last_point])**2 + (z_fine[all_z_up]<=z_fine[all_z_up][last_point])*1e3*radious_of_search).argmin()
				left_up = [r_fine[all_peaks_up][last_point],r_fine[all_peaks_up][next_point]]
				left_up_z = [z_fine[all_z_up][last_point],z_fine[all_z_up][next_point]]
				points_used = [last_point,next_point]
				points_masked = [last_point,next_point]
				points_masked.extend(np.arange(len(z_fine[all_z_up]))[z_fine[all_z_up]<z_fine[all_z_up][last_point]])
				select = ((z_fine[all_z_up]-z_fine[all_z_up][next_point])**2 + (r_fine[all_peaks_up]-r_fine[all_peaks_up][next_point])**2) <= radious_of_search**2
				select[points_masked] = False
				while np.sum(select)>0:
					last_angle = np.arctan2(z_fine[all_z_up][last_point]-z_fine[all_z_up][next_point],r_fine[all_peaks_up][last_point]-r_fine[all_peaks_up][next_point])*180/np.pi
					if last_angle<0:
						last_angle = 360+last_angle
					next_angle = []
					next_radius = []
					for i_ in range(np.sum(select)):
						temp = np.arctan2(z_fine[all_z_up][select][i_]-z_fine[all_z_up][next_point],r_fine[all_peaks_up][select][i_]-r_fine[all_peaks_up][next_point])*180/np.pi
						if temp<0:
							temp = 360+temp
						next_angle.append(temp)
						next_radius.append(((z_fine[all_z_up][select][i_]-z_fine[all_z_up][next_point])**2 + (r_fine[all_peaks_up][select][i_]-r_fine[all_peaks_up][next_point])**2)**0.5)
					# next_angle = np.array(next_angle)
					# next_angle[next_angle<0] = 360-next_angle[next_angle<0]
					angle_difference = last_angle-np.array(next_angle)
					angle_difference[angle_difference<0] = 360+angle_difference[angle_difference<0]
					next_perimeter = angle_difference* np.array(next_radius)**2
					last_point = cp.deepcopy(next_point)
					next_point = np.arange(len(z_fine[all_z_up]))[select][angle_difference>20][next_perimeter[angle_difference>20].argmin()]
					# next_point = np.arange(len(z_fine[all_z_up]))[select][next_perimeter.argmin()]
					points_masked.append(next_point)
					points_used.append(next_point)
					left_up.append(r_fine[all_peaks_up][next_point])
					left_up_z.append(z_fine[all_z_up][next_point])
					select = ((z_fine[all_z_up]-z_fine[all_z_up][next_point])**2 + (r_fine[all_peaks_up]-r_fine[all_peaks_up][next_point])**2) <= radious_of_search**2
					select[points_masked] = False

				left_up = np.flip(left_up,axis=0).tolist()
				left_up_z = np.flip(left_up_z,axis=0).tolist()
				points_used = np.flip(points_used,axis=0).tolist()
				# now I go down. the point is to stay alwaus on the right
				last_point = cp.deepcopy(points_used[-2])
				next_point = cp.deepcopy(points_used[-1])
				points_masked = cp.deepcopy(points_used)
				points_masked.extend(np.arange(len(z_fine[all_z_up]))[z_fine[all_z_up]>z_fine[all_z_up][last_point]])
				select = ((z_fine[all_z_up]-z_fine[all_z_up][next_point])**2 + (r_fine[all_peaks_up]-r_fine[all_peaks_up][next_point])**2) <= radious_of_search**2
				select[points_masked] = False
				while np.sum(select)>0:
					last_angle = np.arctan2(z_fine[all_z_up][last_point]-z_fine[all_z_up][next_point],r_fine[all_peaks_up][last_point]-r_fine[all_peaks_up][next_point])*180/np.pi
					if last_angle<0:
						last_angle = 360+last_angle
					next_angle = []
					next_radius = []
					for i_ in range(np.sum(select)):
						temp = np.arctan2(z_fine[all_z_up][select][i_]-z_fine[all_z_up][next_point],r_fine[all_peaks_up][select][i_]-r_fine[all_peaks_up][next_point])*180/np.pi
						if temp<0:
							temp = 360+temp
						next_angle.append(temp)
						next_radius.append(((z_fine[all_z_up][select][i_]-z_fine[all_z_up][next_point])**2 + (r_fine[all_peaks_up][select][i_]-r_fine[all_peaks_up][next_point])**2)**0.5)
					# next_angle = np.array(next_angle)
					# next_angle[next_angle<0] = 360-next_angle[next_angle<0]
					angle_difference = np.array(next_angle) - last_angle	# this way going down it stays to the right
					angle_difference[angle_difference<0] = 360+angle_difference[angle_difference<0]
					next_perimeter = angle_difference* np.array(next_radius)**2
					last_point = cp.deepcopy(next_point)
					next_point = np.arange(len(z_fine[all_z_up]))[select][angle_difference>20][next_perimeter[angle_difference>20].argmin()]
					# next_point = np.arange(len(z_fine[all_z_up]))[select][next_perimeter.argmin()]
					points_masked.append(next_point)
					points_used.append(next_point)
					left_up.append(r_fine[all_peaks_up][next_point])
					left_up_z.append(z_fine[all_z_up][next_point])
					select = ((z_fine[all_z_up]-z_fine[all_z_up][next_point])**2 + (r_fine[all_peaks_up]-r_fine[all_peaks_up][next_point])**2) <= radious_of_search**2
					select[points_masked] = False


				# now I do the outer one
				last_point = np.abs((z_fine[all_z_up]-0)**2 + (r_fine[all_peaks_up]-1.4)**2).argmin()	# point closer to midplane and to an arbitrary 1.4 radious
				# z is in increasing direction
				# I crawl first up. the point is to stay always on the right
				radious_of_search = 0.05	# m
				next_point = ((z_fine[all_z_up]-z_fine[all_z_up][last_point])**2 + (r_fine[all_peaks_up]-r_fine[all_peaks_up][last_point])**2 + (z_fine[all_z_up]<=z_fine[all_z_up][last_point])*1e3*radious_of_search).argmin()
				right_up = [r_fine[all_peaks_up][last_point],r_fine[all_peaks_up][next_point]]
				right_up_z = [z_fine[all_z_up][last_point],z_fine[all_z_up][next_point]]
				points_used = [last_point,next_point]
				points_masked = [last_point,next_point]
				points_masked.extend(np.arange(len(z_fine[all_z_up]))[z_fine[all_z_up]<z_fine[all_z_up][last_point]])
				select = ((z_fine[all_z_up]-z_fine[all_z_up][next_point])**2 + (r_fine[all_peaks_up]-r_fine[all_peaks_up][next_point])**2) <= radious_of_search**2
				select[points_masked] = False
				while np.sum(select)>0:
					last_angle = np.arctan2(z_fine[all_z_up][last_point]-z_fine[all_z_up][next_point],r_fine[all_peaks_up][last_point]-r_fine[all_peaks_up][next_point])*180/np.pi
					if last_angle<0:
						last_angle = 360+last_angle
					next_angle = []
					next_radius = []
					for i_ in range(np.sum(select)):
						temp = np.arctan2(z_fine[all_z_up][select][i_]-z_fine[all_z_up][next_point],r_fine[all_peaks_up][select][i_]-r_fine[all_peaks_up][next_point])*180/np.pi
						if temp<0:
							temp = 360+temp
						next_angle.append(temp)
						next_radius.append(((z_fine[all_z_up][select][i_]-z_fine[all_z_up][next_point])**2 + (r_fine[all_peaks_up][select][i_]-r_fine[all_peaks_up][next_point])**2)**0.5)
					# next_angle = np.array(next_angle)
					# next_angle[next_angle<0] = 360-next_angle[next_angle<0]
					angle_difference = np.array(next_angle) - last_angle	# this way going up it stays to the right
					angle_difference[angle_difference<0] = 360+angle_difference[angle_difference<0]
					next_perimeter = angle_difference* np.array(next_radius)**2
					last_point = cp.deepcopy(next_point)
					next_point = np.arange(len(z_fine[all_z_up]))[select][angle_difference>20][next_perimeter[angle_difference>20].argmin()]
					# next_point = np.arange(len(z_fine[all_z_up]))[select][next_perimeter.argmin()]
					points_masked.append(next_point)
					points_used.append(next_point)
					right_up.append(r_fine[all_peaks_up][next_point])
					right_up_z.append(z_fine[all_z_up][next_point])
					select = ((z_fine[all_z_up]-z_fine[all_z_up][next_point])**2 + (r_fine[all_peaks_up]-r_fine[all_peaks_up][next_point])**2) <= radious_of_search**2
					select[points_masked] = False

				right_up = np.flip(right_up,axis=0).tolist()
				right_up_z = np.flip(right_up_z,axis=0).tolist()
				points_used = np.flip(points_used,axis=0).tolist()
				# now I go down. the point is to stay alwaus on the right
				last_point = cp.deepcopy(points_used[-2])
				next_point = cp.deepcopy(points_used[-1])
				points_masked = cp.deepcopy(points_used)
				points_masked.extend(np.arange(len(z_fine[all_z_up]))[z_fine[all_z_up]>z_fine[all_z_up][last_point]])
				select = ((z_fine[all_z_up]-z_fine[all_z_up][next_point])**2 + (r_fine[all_peaks_up]-r_fine[all_peaks_up][next_point])**2) <= radious_of_search**2
				select[points_masked] = False
				while np.sum(select)>0:
					last_angle = np.arctan2(z_fine[all_z_up][last_point]-z_fine[all_z_up][next_point],r_fine[all_peaks_up][last_point]-r_fine[all_peaks_up][next_point])*180/np.pi
					if last_angle<0:
						last_angle = 360+last_angle
					next_angle = []
					next_radius = []
					for i_ in range(np.sum(select)):
						temp = np.arctan2(z_fine[all_z_up][select][i_]-z_fine[all_z_up][next_point],r_fine[all_peaks_up][select][i_]-r_fine[all_peaks_up][next_point])*180/np.pi
						if temp<0:
							temp = 360+temp
						next_angle.append(temp)
						next_radius.append(((z_fine[all_z_up][select][i_]-z_fine[all_z_up][next_point])**2 + (r_fine[all_peaks_up][select][i_]-r_fine[all_peaks_up][next_point])**2)**0.5)
					# next_angle = np.array(next_angle)
					# next_angle[next_angle<0] = 360-next_angle[next_angle<0]
					angle_difference = last_angle - np.array(next_angle)	# this way going down it stays to the left
					angle_difference[angle_difference<0] = 360+angle_difference[angle_difference<0]
					next_perimeter = angle_difference* np.array(next_radius)**2
					last_point = cp.deepcopy(next_point)
					next_point = np.arange(len(z_fine[all_z_up]))[select][angle_difference>20][next_perimeter[angle_difference>20].argmin()]
					# next_point = np.arange(len(z_fine[all_z_up]))[select][next_perimeter.argmin()]
					points_masked.append(next_point)
					points_used.append(next_point)
					right_up.append(r_fine[all_peaks_up][next_point])
					right_up_z.append(z_fine[all_z_up][next_point])
					select = ((z_fine[all_z_up]-z_fine[all_z_up][next_point])**2 + (r_fine[all_peaks_up]-r_fine[all_peaks_up][next_point])**2) <= radious_of_search**2
					select[points_masked] = False
				# forget about it, it's way too fiddly


		# plt.figure()
		# plt.plot(z_fine[left_up_z],r_fine[left_up],'-b')
		# plt.plot(z_fine[right_up_z],r_fine[right_up],'-r')
		# plt.plot(z_fine[left_low_z],r_fine[left_low],'-b')
		# plt.plot(z_fine[right_low_z],r_fine[right_low],'-r')
		# plt.pause(0.01)
		all_time_sep_r.append([left_up,right_up,left_low,right_low])
		all_time_sep_z.append([left_up_z,right_up_z,left_low_z,right_low_z])
	print('efit_reconstruction_to_separatrix_on_foil done in %.3gmin' %((tm.time()-start_efit_reconstruction_to_separatrix_on_foil)/60))
	return all_time_sep_r,all_time_sep_z,r_fine,z_fine


##########################################################################################################################################################################

def get_rotation_crop_parameters(testrot,foil_position_dict,laser_to_analyse,plasma_data_array,plasma_data_array_time,foilhorizw=0.09,foilvertw=0.07):
	# Created 2021/07/23 only to obtain the rotation / cropping parameters
	from scipy.ndimage import generic_filter
	from scipy.ndimage import median_filter
	from scipy import ndimage


	# foil_position_dict = dict([('angle',1.0),('foilcenter',[158,136]),('foilhorizw',0.09),('foilvertw',0.07),('foilhorizwpixel',246)])	# identified 2022-11-08 for MU02

	rotangle = foil_position_dict['angle'] #in degrees
	foilrot=rotangle*2*np.pi/360
	foilrotdeg=rotangle
	foilcenter = foil_position_dict['foilcenter']
	foilhorizwpixel = foil_position_dict['foilhorizwpixel']
	# foilvertwpixel=int((foilhorizwpixel*foilvertw)//foilhorizw)
	foilvertwpixel=int(round((foilhorizwpixel*foilvertw)/foilhorizw))
	r=((foilhorizwpixel**2+foilvertwpixel**2)**0.5)/2  # HALF DIAGONAL
	a=foilvertwpixel/np.cos(foilrot)
	tgalpha=np.tan(foilrot)
	delta=-(a**2)/4+(1+tgalpha**2)*(r**2)
	foilx=np.add(foilcenter[0],[(-0.5*a*tgalpha+delta**0.5)/(1+tgalpha**2),(-0.5*a*tgalpha-delta**0.5)/(1+tgalpha**2),(0.5*a*tgalpha-delta**0.5)/(1+tgalpha**2),(0.5*a*tgalpha+delta**0.5)/(1+tgalpha**2),(-0.5*a*tgalpha+delta**0.5)/(1+tgalpha**2)])
	foily=np.add(foilcenter[1]-tgalpha*foilcenter[0],[tgalpha*foilx[0]+a/2,tgalpha*foilx[1]+a/2,tgalpha*foilx[2]-a/2,tgalpha*foilx[3]-a/2,tgalpha*foilx[0]+a/2])
	foilxint=(np.rint(foilx)).astype('int')
	foilyint=(np.rint(foily)).astype('int')


	sx = ndimage.sobel(testrot, axis=0, mode='constant')
	sy = ndimage.sobel(testrot, axis=1, mode='constant')
	sob = np.hypot(sx, sy)
	# testrot_filtered = testrot - generic_filter((testrot),np.mean,size=[7,7])

	plt.figure(figsize=(20, 10))
	plt.rcParams.update({'font.size': 20})
	plt.title('Foil search in '+laser_to_analyse+'\nFoil center '+str(foilcenter)+', foil rot '+str(foilrotdeg)+'deg, foil size '+str([foilhorizwpixel,foilvertwpixel])+'pixel\n')
	plt.imshow(testrot,'rainbow',origin='lower')
	plt.ylabel('Horizontal axis [pixles]')
	plt.xlabel('Vertical axis [pixles]')
	temp = np.sort(testrot[testrot>0])
	plt.clim(vmin=np.nanmean(temp[:max(20,int(len(temp)/20))]), vmax=np.nanmean(temp[-max(20,int(len(temp)/20)):]))
	# plt.clim(vmin=median_filter(testrot_filtered,size=[31,31]).min(), vmax=median_filter(testrot_filtered,size=[31,31]).max())
	# plt.clim(vmax=median_filter(sob,size=[51,51]).max()/2)
	# plt.clim(vmax=27.1,vmin=26.8)
	# plt.clim(vmin=np.nanmin(testrot[testrot>0]), vmax=np.nanmax(testrot))
	plt.colorbar().set_label('counts [au]')
	plt.plot(foilx-1,foily-1,'k',alpha=0.5,dashes=[6, 6])
	plt.plot(foilcenter[0],foilcenter[1],'k+',markersize=30)
	plt.savefig(laser_to_analyse[:-4]+'_foil_fit.eps')#, bbox_inches='tight')
	plt.close('all')
	plt.figure(figsize=(20, 10))
	plt.title('Foil search in '+laser_to_analyse+'\nFoil center '+str(foilcenter)+', foil rot '+str(foilrotdeg)+'deg, foil size '+str([foilhorizwpixel,foilvertwpixel])+'pixel\n sobel filter\n')
	plt.imshow(sob,'rainbow',origin='lower')
	plt.ylabel('Horizontal axis [pixles]')
	plt.xlabel('Vertical axis [pixles]')
	temp = np.sort(testrot[testrot>0])
	# plt.clim(vmin=np.nanmean(temp[:max(20,int(len(temp)/20))]), vmax=np.nanmean(temp[-max(20,int(len(temp)/20)):]))
	# plt.clim(vmin=median_filter(testrot_filtered,size=[31,31]).min(), vmax=median_filter(testrot_filtered,size=[31,31]).max())
	plt.clim(vmax=median_filter(sob,size=[51,51]).max()/2)
	# plt.clim(vmax=27.1,vmin=26.8)
	# plt.clim(vmin=np.nanmin(testrot[testrot>0]), vmax=np.nanmax(testrot))
	plt.colorbar().set_label('counts [au]')
	plt.plot(foilx-1,foily-1,'k',alpha=0.5,dashes=[6, 6])
	plt.plot(foilcenter[0],foilcenter[1],'k+',markersize=30)
	plt.savefig(laser_to_analyse[:-4]+'_foil_fit_sobel.eps')#, bbox_inches='tight')
	plt.close('all')

	plt.figure(figsize=(20, 10))
	plt.rcParams.update({'font.size': 20})
	temp = np.max(plasma_data_array[np.logical_and(plasma_data_array_time>plasma_data_array_time.min()+0.3,plasma_data_array_time<plasma_data_array_time.max()-0.3),:,:200],axis=(-1,-2)).argmax()+(np.abs(plasma_data_array_time-(plasma_data_array_time.min()+0.3))).argmin()	# the plasma_data_array_time.min()+0.3 is there because I want to avoid that temp-20 is negative
	temp2 = np.nanmax(plasma_data_array[temp-int(0.1/np.mean(np.diff(plasma_data_array_time))):temp+int(0.1/np.mean(np.diff(plasma_data_array_time)))],axis=0)
	plt.title('Foil search in '+laser_to_analyse+'\nFoil center '+str(foilcenter)+', foil rot '+str(foilrotdeg)+'deg, foil size '+str([foilhorizwpixel,foilvertwpixel])+'pixel\nhottest frame at %.4gsec +/-100ms' %(plasma_data_array_time[temp]))
	plt.imshow(temp2,'rainbow',origin='lower',vmin=0,vmax=np.max(temp2[:,:200])/2)
	plt.ylabel('Horizontal axis [pixles]')
	plt.xlabel('Vertical axis [pixles]')
	plt.clim(vmin=0,vmax=np.max(temp2[:,:200])/2)
	plt.colorbar().set_label('deviation from running average [au]')
	plt.plot(foilx-1,foily-1,'r',dashes=[6, 6])
	plt.plot(foilcenter[0],foilcenter[1],'k+',markersize=30)
	plt.savefig(laser_to_analyse[:-4]+'_foil_fit_plasma.eps')#, bbox_inches='tight')
	plt.close('all')

	testrotback=rotate(testrot,foilrotdeg,axes=(-1,-2))
	precisionincrease=25
	dummy=np.ones(np.multiply(np.shape(testrot),precisionincrease))
	dummy[foilcenter[1]*precisionincrease,foilcenter[0]*precisionincrease]=2
	dummy[int(np.rint(foily[0]*precisionincrease)),int(np.rint(foilx[0]*precisionincrease))]=3
	dummy[int(np.rint(foily[1]*precisionincrease)),int(np.rint(foilx[1]*precisionincrease))]=4
	dummy[int(np.rint(foily[2]*precisionincrease)),int(np.rint(foilx[2]*precisionincrease))]=5
	dummy[int(np.rint(foily[3]*precisionincrease)),int(np.rint(foilx[3]*precisionincrease))]=6
	dummy2=rotate(dummy,foilrotdeg,axes=(-1,-2),order=0)
	foilcenterrot=(np.rint([np.where(dummy2==2)[1][0]/precisionincrease,np.where(dummy2==2)[0][0]/precisionincrease])).astype('int')
	foilxrot=(np.rint([np.where(dummy2==3)[1][0]/precisionincrease,np.where(dummy2==4)[1][0]/precisionincrease,np.where(dummy2==5)[1][0]/precisionincrease,np.where(dummy2==6)[1][0]/precisionincrease,np.where(dummy2==3)[1][0]/precisionincrease])).astype('int')
	foilyrot=(np.rint([np.where(dummy2==3)[0][0]/precisionincrease,np.where(dummy2==4)[0][0]/precisionincrease,np.where(dummy2==5)[0][0]/precisionincrease,np.where(dummy2==6)[0][0]/precisionincrease,np.where(dummy2==3)[0][0]/precisionincrease])).astype('int')
	# plt.plot(foilcenterrot[0],foilcenterrot[1],'k+',markersize=30)
	# plt.plot(foilxrot,foilyrot,'r')
	# plt.title('Foil center '+str(foilcenterrot)+', foil rot '+str(0)+'deg, foil size '+str([foilhorizwpixel,foilvertwpixel])+'pixel',size=9)
	# plt.colorbar().set_label('counts [au]')
	# plt.pause(0.01)

	foillx=min(foilxrot)
	foilrx=max(foilxrot)
	foilhorizwpixel=foilrx-foillx
	foildw=min(foilyrot)
	foilup=max(foilyrot)
	foilvertwpixel=foilup-foildw

	out_of_ROI_mask = np.ones_like(testrotback)
	out_of_ROI_mask[testrotback<np.nanmin(testrot[testrot>0])]=np.nan
	out_of_ROI_mask[testrotback>np.nanmax(testrot[testrot>0])]=np.nan
	a = generic_filter((testrotback),np.std,size=[19,19])
	out_of_ROI_mask[a>np.mean(a)]=np.nan

	if True:
		height,width = np.shape(testrot)
		max_ROI = [[0,height-1],[0,width-1]]
		testrotback_crop = rotate_and_crop_3D([testrot],foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw-10,foilup+10,foillx-10,foilrx+10)[0]
		# temp = np.max(plasma_data_array[:,:,:200],axis=(-1,-2)).argmax()
		# testrotback_crop = coleval.rotate_and_crop_3D([plasma_data_array[temp]],foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw-10,foilup+10,foillx-10,foilrx+10)[0]
		plt.figure(figsize=(20, 10))
		plt.rcParams.update({'font.size': 20})
		plt.title('Foil search in '+laser_to_analyse+'\nFoil center '+str(foilcenter)+', foil rot '+str(foilrotdeg)+'deg, foil size '+str([foilhorizwpixel,foilvertwpixel])+'pixel\nreal foil fit')
		plt.imshow(testrotback_crop,'rainbow',origin='lower')
		plt.ylabel('Horizontal axis [pixles]')
		plt.xlabel('Vertical axis [pixles]')
		plt.plot([10-0.5,testrotback_crop.shape[1]-10-0.5,testrotback_crop.shape[1]-10-0.5,10-0.5,10-0.5],[10-0.5,10-0.5,testrotback_crop.shape[0]-10-0.5,testrotback_crop.shape[0]-10-0.5,10-0.5],'k',dashes=[6, 6])
		plt.colorbar().set_label('counts [au]')
		plt.savefig(laser_to_analyse[:-4]+'_foil_fit_2.eps')#, bbox_inches='tight')
		plt.close('all')

		plt.figure(figsize=(20, 10))
		plt.rcParams.update({'font.size': 20})
		temp = np.max(plasma_data_array[np.logical_and(plasma_data_array_time>plasma_data_array_time.min()+0.3,plasma_data_array_time<plasma_data_array_time.max()-0.3),:,:200],axis=(-1,-2)).argmax()+(np.abs(plasma_data_array_time-(plasma_data_array_time.min()+0.3))).argmin()	# the plasma_data_array_time.min()+0.3 is there because I want to avoid that temp-20 is negative
		temp2 = np.nanmax(plasma_data_array[temp-int(0.1/np.mean(np.diff(plasma_data_array_time))):temp+int(0.1/np.mean(np.diff(plasma_data_array_time)))],axis=0)
		testrotback_crop = rotate_and_crop_3D([temp2],foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw-10,foilup+10,foillx-10,foilrx+10)[0]
		plt.title('Foil search in '+laser_to_analyse+'\nFoil center '+str(foilcenter)+', foil rot '+str(foilrotdeg)+'deg, foil size '+str([foilhorizwpixel,foilvertwpixel])+'pixel\nhottest frame at %.4gsec +/-100ms' %(plasma_data_array_time[temp]))
		plt.imshow(testrotback_crop,'rainbow',origin='lower',vmin=0,vmax=np.max(temp2[:,:200])/2)
		plt.clim(vmin=0,vmax=np.max(temp2[:,:200])/2)
		plt.ylabel('Horizontal axis [pixles]')
		plt.xlabel('Vertical axis [pixles]')
		plt.plot([10-0.5,testrotback_crop.shape[1]-10-0.5,testrotback_crop.shape[1]-10-0.5,10-0.5,10-0.5],[10-0.5,10-0.5,testrotback_crop.shape[0]-10-0.5,testrotback_crop.shape[0]-10-0.5,10-0.5],'k',dashes=[6, 6])
		plt.colorbar().set_label('deviation from running average [au]')
		plt.savefig(laser_to_analyse[:-4]+'_foil_fit_plasma_2.eps')#, bbox_inches='tight')
		plt.close('all')

		plt.figure(figsize=(20, 10))
		plt.rcParams.update({'font.size': 20})
		temp = np.max(plasma_data_array[np.logical_and(plasma_data_array_time>plasma_data_array_time.min()+0.3,plasma_data_array_time<plasma_data_array_time.max()-0.3),:,:200],axis=(-1,-2)).argmax()+(np.abs(plasma_data_array_time-(plasma_data_array_time.min()+0.3))).argmin()	# the plasma_data_array_time.min()+0.3 is there because I want to avoid that temp-20 is negative
		temp2 = np.nanmax(plasma_data_array[temp-int(0.1/np.mean(np.diff(plasma_data_array_time))):temp+int(0.1/np.mean(np.diff(plasma_data_array_time)))],axis=0)
		testrotback_crop = rotate_and_crop_3D([temp2],foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx)[0]
		plt.title('Foil search in '+laser_to_analyse+'\nFoil center '+str(foilcenter)+', foil rot '+str(foilrotdeg)+'deg, foil size '+str([foilhorizwpixel,foilvertwpixel])+'pixel\nhottest frame at %.4gsec +/-100ms' %(plasma_data_array_time[temp]))
		plt.imshow(testrotback_crop,'rainbow',origin='lower',vmin=0,vmax=np.max(temp2[:,:200])/2)
		plt.clim(vmin=0,vmax=np.max(temp2[:,:200])/2)
		plt.ylabel('Horizontal axis [pixles]')
		plt.xlabel('Vertical axis [pixles]')
		plt.colorbar().set_label('deviation from running average [au]')
		plt.savefig(laser_to_analyse[:-4]+'_foil_fit_plasma_3.eps')#, bbox_inches='tight')
		plt.close('all')

	return foilrotdeg,out_of_ROI_mask,foildw,foilup,foillx,foilrx


def MASTU_pulse_process_FAST(laser_counts_corrected,time_of_experiment_digitizer_ID,time_of_experiment,external_clock_marker,aggregated_correction_coefficients,laser_framerate,laser_digitizer_ID,laser_int_time,seconds_for_reference_frame,start_time_of_pulse,laser_to_analyse,height,width,flag_use_of_first_frames_as_reference):
	# created 2021/07/23
	# Here I do a fast analysis of the unfiltered data so I can look at it during experiments
	max_ROI = [[0,255],[0,319]]
	# foil_position_dict = dict([('angle',0.5),('foilcenter',[158,136]),('foilhorizwpixel',241)])	# fixed orientation, for now, this is from 2021-06-04/44168
	foil_position_dict = dict([('angle',0.7),('foilcenter',[157,136]),('foilhorizwpixel',240)])	# modified 2021/09/21 to match sensitivity matrix
	temp_ref_counts = []
	temp_counts_minus_background = []
	time_partial = []
	timesteps = np.inf
	for i in range(len(laser_digitizer_ID)):
		time_of_experiment_digitizer_ID_seconds = (time_of_experiment_digitizer_ID[i]-time_of_experiment[0])*1e-6-start_time_of_pulse
		if external_clock_marker:
			time_of_experiment_digitizer_ID_seconds = time_of_experiment_digitizer_ID_seconds-np.mean(aggregated_correction_coefficients[:,4])	# I use the mean of the coefficients because I want to avoid small unpredictable differences between the digitisers
		if flag_use_of_first_frames_as_reference:
			# temp_ref_counts.append(np.mean(laser_counts_corrected[i][time_of_experiment_digitizer_ID_seconds<0],axis=0))
			temp_ref_counts.append(np.mean(laser_counts_corrected[i][np.logical_and(time_of_experiment_digitizer_ID_seconds<0,time_of_experiment_digitizer_ID_seconds>-0.5)],axis=0))
		else:
			temp_ref_counts.append(np.mean(laser_counts_corrected[i][-int(seconds_for_reference_frame*laser_framerate/len(laser_digitizer_ID)):],axis=0))
		select_time = np.logical_and(time_of_experiment_digitizer_ID_seconds>=0,time_of_experiment_digitizer_ID_seconds<=1.5)
		temp_counts_minus_background.append(laser_counts_corrected[i][select_time]-temp_ref_counts[-1])
		time_partial.append(time_of_experiment_digitizer_ID_seconds[select_time])
		timesteps = min(timesteps,len(temp_counts_minus_background[-1]))

	for i in range(len(laser_digitizer_ID)):
		temp_counts_minus_background[i] = temp_counts_minus_background[i][:timesteps]
		time_partial[i] = time_partial[i][:timesteps]
	temp_counts_minus_background = np.nanmean(temp_counts_minus_background,axis=0)
	temp_ref_counts = np.nanmean(temp_ref_counts,axis=0)
	FAST_counts_minus_background_crop_time = np.nanmean(time_partial,axis=0)

	# I'm going to use the reference frames for foil position
	foilrotdeg,out_of_ROI_mask,foildw,foilup,foillx,foilrx = get_rotation_crop_parameters(temp_ref_counts,foil_position_dict,laser_to_analyse,temp_counts_minus_background,FAST_counts_minus_background_crop_time)

	# rotation and crop
	temp_counts_minus_background_rot=rotate(temp_counts_minus_background,foilrotdeg,axes=(-1,-2))
	if not (height==max_ROI[0][1]+1 and width==max_ROI[1][1]+1):
		temp_counts_minus_background_rot*=out_of_ROI_mask
		temp_counts_minus_background_rot[np.logical_and(temp_counts_minus_background_rot<np.nanmin(temp_counts_minus_background[i]),temp_counts_minus_background_rot>np.nanmax(temp_counts_minus_background[i]))]=0
	FAST_counts_minus_background_crop = temp_counts_minus_background_rot[:,foildw:foilup,foillx:foilrx]

	temp = FAST_counts_minus_background_crop[:,:,:int(np.shape(FAST_counts_minus_background_crop)[2]*0.75)]
	temp = np.sort(temp[np.max(temp,axis=(1,2)).argmax()].flatten())
	ani = movie_from_data(np.array([np.flip(np.transpose(FAST_counts_minus_background_crop,(0,2,1)),axis=2)]), laser_framerate/len(laser_digitizer_ID),integration=laser_int_time/1000,time_offset=FAST_counts_minus_background_crop_time[0],extvmin=0,extvmax=np.nanmean(temp[-len(temp)//60:]),xlabel='horizontal coord [pixels]', ylabel='vertical coord [pixels]',barlabel='Count increase [au]', prelude='shot ' + laser_to_analyse[-9:-4]+'\n',overlay_structure=True,include_EFIT=True,pulse_ID=laser_to_analyse[-9:-4],overlay_x_point=True,overlay_mag_axis=True,overlay_strike_points=True,overlay_separatrix=True)
	ani.save(laser_to_analyse[:-4]+ '_FAST_count_increase.mp4', fps=5*laser_framerate/len(laser_digitizer_ID)/383, writer='ffmpeg',codec='mpeg4')
	plt.close('all')

	print('completed FAST rotating/cropping ' + laser_to_analyse)

	return foilrotdeg,out_of_ROI_mask,foildw,foilup,foillx,foilrx,FAST_counts_minus_background_crop,FAST_counts_minus_background_crop_time

#######################################################################################################################################################################################################


def MASTU_pulse_process_FAST2(laser_counts_corrected,time_of_experiment_digitizer_ID,time_of_experiment,external_clock_marker,aggregated_correction_coefficients,laser_framerate,laser_digitizer_ID,laser_int_time,seconds_for_reference_frame,start_time_of_pulse,laser_to_analyse,height,width,flag_use_of_first_frames_as_reference,params,foil_position_dict):
	# created 2021/09/10
	# created modifying MASTU_pulse_process_FAST in order t have a quick power on the foil with a primitive filtering

	from scipy.ndimage import generic_filter

	max_ROI = [[0,255],[0,319]]
	# foil_position_dict = dict([('angle',0.5),('foilcenter',[158,136]),('foilhorizw',0.09),('foilvertw',0.07),('foilhorizwpixel',241)])	# fixed orientation, for now, this is from 2021-06-04/44168
	temp_ref_counts = []
	temp_counts_minus_background = []
	time_partial = []
	timesteps = np.inf
	for i in range(len(laser_digitizer_ID)):
		time_of_experiment_digitizer_ID_seconds = (time_of_experiment_digitizer_ID[i]-time_of_experiment[0])*1e-6-start_time_of_pulse
		if external_clock_marker:
			time_of_experiment_digitizer_ID_seconds = time_of_experiment_digitizer_ID_seconds-np.mean(aggregated_correction_coefficients[:,4])	# I use the mean of the coefficients because I want to avoid small unpredictable differences between the digitisers

		# basic smoothing
		spectra_orig=np.fft.fft(np.mean(laser_counts_corrected[i],axis=(-1,-2)))
		magnitude = 2 * np.abs(spectra_orig) / len(spectra_orig)
		freq = np.fft.fftfreq(len(magnitude), d=np.mean(np.diff(time_of_experiment_digitizer_ID_seconds)))
		magnitude = np.array([y for _, y in sorted(zip(freq, magnitude))])
		freq = np.sort(freq)
		magnitude_smooth = generic_filter(np.log(magnitude),np.median,size=[7])
		peak_oscillation = (magnitude-np.exp(magnitude_smooth))[np.logical_and(freq>10,freq<50)].argmax()
		peak_oscillation_freq = freq[np.logical_and(freq>10,freq<50)][peak_oscillation]
		frames_to_average = 1/peak_oscillation_freq/np.mean(np.diff(time_of_experiment_digitizer_ID_seconds))
		laser_counts_corrected_filtered = real_mean_filter_agent(laser_counts_corrected[i],frames_to_average)

		if flag_use_of_first_frames_as_reference:
			# temp_ref_counts.append(np.mean(laser_counts_corrected_filtered[time_of_experiment_digitizer_ID_seconds<0],axis=0))
			temp_ref_counts.append(np.mean(laser_counts_corrected_filtered[np.logical_and(time_of_experiment_digitizer_ID_seconds<0,time_of_experiment_digitizer_ID_seconds>-0.5)],axis=0))
		else:
			temp_ref_counts.append(np.mean(laser_counts_corrected_filtered[-int(seconds_for_reference_frame*laser_framerate/len(laser_digitizer_ID)):],axis=0))
		select_time = np.logical_and(time_of_experiment_digitizer_ID_seconds>=0,time_of_experiment_digitizer_ID_seconds<=1.5)
		temp_counts_minus_background.append(laser_counts_corrected_filtered[select_time]-temp_ref_counts[-1])
		time_partial.append(time_of_experiment_digitizer_ID_seconds[select_time])
		timesteps = min(timesteps,len(temp_counts_minus_background[-1]))

	for i in range(len(laser_digitizer_ID)):
		temp_counts_minus_background[i] = temp_counts_minus_background[i][:timesteps]
		time_partial[i] = time_partial[i][:timesteps]
	temp_counts_minus_background = np.nanmean(temp_counts_minus_background,axis=0)
	temp_ref_counts = np.nanmean(temp_ref_counts,axis=0)
	FAST_counts_minus_background_crop_time = np.nanmean(time_partial,axis=0)

	# I'm going to use the reference frames for foil position
	foilrotdeg,out_of_ROI_mask,foildw,foilup,foillx,foilrx = get_rotation_crop_parameters(temp_ref_counts,foil_position_dict,laser_to_analyse,temp_counts_minus_background,FAST_counts_minus_background_crop_time)

	# rotation and crop
	temp_counts_minus_background_rot=rotate(temp_counts_minus_background,foilrotdeg,axes=(-1,-2))
	if not (height==max_ROI[0][1]+1 and width==max_ROI[1][1]+1):
		temp_counts_minus_background_rot*=out_of_ROI_mask
		temp_counts_minus_background_rot[np.logical_and(temp_counts_minus_background_rot<np.nanmin(temp_counts_minus_background),temp_counts_minus_background_rot>np.nanmax(temp_counts_minus_background))]=0
	FAST_counts_minus_background_crop = temp_counts_minus_background_rot[:,foildw:foilup,foillx:foilrx]

	temp = FAST_counts_minus_background_crop[:,:,:int(np.shape(FAST_counts_minus_background_crop)[2]*0.75)]
	temp = np.sort(temp[np.max(temp,axis=(1,2)).argmax()].flatten())
	ani = movie_from_data(np.array([np.flip(np.transpose(FAST_counts_minus_background_crop,(0,2,1)),axis=2)]), laser_framerate/len(laser_digitizer_ID),timesteps=FAST_counts_minus_background_crop_time,integration=laser_int_time/1000,time_offset=FAST_counts_minus_background_crop_time[0],extvmin=0,extvmax=np.nanmean(temp[-len(temp)//60:]),xlabel='horizontal coord [pixels]', ylabel='vertical coord [pixels]',barlabel='Count increase [au]', prelude='shot ' + laser_to_analyse[-9:-4]+'\n',overlay_structure=True,include_EFIT=True,pulse_ID=laser_to_analyse[-9:-4],overlay_x_point=True,overlay_mag_axis=True,overlay_strike_points=True,overlay_separatrix=True)
	ani.save(laser_to_analyse[:-4]+ '_FAST_count_increase.mp4', fps=5*laser_framerate/len(laser_digitizer_ID)/383, writer='ffmpeg',codec='mpeg4')
	plt.close('all')

	print('completed FAST count rotating/cropping ' + laser_to_analyse)

	params = np.mean(params,axis=(0))
	temperature = params[:,:,-1] + params[:,:,-2] * (temp_counts_minus_background+temp_ref_counts) + params[:,:,-3] * ((temp_counts_minus_background+temp_ref_counts)**2)
	temperature_ref = params[:,:,-1] + params[:,:,-2] * temp_ref_counts + params[:,:,-3] * (temp_ref_counts**2)

	# rotation and crop
	temperature_rot=rotate(temperature,foilrotdeg,axes=(-1,-2))
	if not (height==max_ROI[0][1]+1 and width==max_ROI[1][1]+1):
		temperature_rot*=out_of_ROI_mask
		temperature_rot[np.logical_and(temperature_rot<np.nanmin(temperature),temperature_rot>np.nanmax(temperature))]=0
	temperature_crop = temperature_rot[:,foildw:foilup,foillx:foilrx]

	# rotation and crop
	temperature_ref_rot=rotate(temperature_ref,foilrotdeg,axes=(-1,-2))
	if not (height==max_ROI[0][1]+1 and width==max_ROI[1][1]+1):
		temperature_ref_rot*=out_of_ROI_mask
		temperature_ref_rot[np.logical_and(temperature_ref_rot<np.nanmin(temperature_ref),temperature_ref_rot>np.nanmax(temperature_ref))]=0
	temperature_ref_crop = temperature_ref_rot[foildw:foilup,foillx:foilrx]

	temperature_minus_background_crop = temperature_crop-temperature_ref_crop

	shrink_factor_t = int(round(frames_to_average))
	shrink_factor_x = 3	# with large time averaging this should be enough
	binning_type = 'bin' + str(shrink_factor_t) + 'x' + str(shrink_factor_x) + 'x' + str(shrink_factor_x)

	FAST_counts_minus_background_crop_binned,nan_ROI_mask = proper_homo_binning_t_2D(FAST_counts_minus_background_crop,shrink_factor_t,shrink_factor_x)
	temperature_crop_binned,nan_ROI_mask = proper_homo_binning_t_2D(temperature_crop,shrink_factor_t,shrink_factor_x)
	temperature_minus_background_crop_binned,nan_ROI_mask = proper_homo_binning_t_2D(temperature_minus_background_crop,shrink_factor_t,shrink_factor_x)
	temperature_ref_crop_binned = proper_homo_binning_2D(temperature_ref_crop,shrink_factor_x)
	time_binned = proper_homo_binning_t(FAST_counts_minus_background_crop_time,shrink_factor_t)

	# reference foil properties
	# thickness = 1.4859095354482858e-06
	# emissivity = 0.9884061389741369
	# diffusivity = 1.045900223180454e-05
	# from 2021/09/17, Laser_data_analysis3_3.py
	thickness = 2.0531473351462095e-06
	emissivity = 0.9999999999999
	diffusivity = 1.0283685197530968e-05
	Ptthermalconductivity=71.6 #[W/(m·K)]
	zeroC=273.15 #K / C
	sigmaSB=5.6704e-08 #[W/(m2 K4)]

	foilemissivityscaled=emissivity*np.ones(np.array(temperature_ref_crop_binned.shape)-2)
	foilthicknessscaled=thickness*np.ones(np.array(temperature_ref_crop_binned.shape)-2)
	conductivityscaled=Ptthermalconductivity*np.ones(np.array(temperature_ref_crop_binned.shape)-2)
	reciprdiffusivityscaled=(1/diffusivity)*np.ones(np.array(temperature_ref_crop_binned.shape)-2)

	dt = time_binned[2:]-time_binned[:-2]
	dx=foil_position_dict['foilhorizw']/foil_position_dict['foilhorizwpixel']*shrink_factor_x
	dTdt=np.divide((temperature_crop_binned[2:,1:-1,1:-1]-temperature_crop_binned[:-2,1:-1,1:-1]).T,dt).T.astype(np.float32)
	d2Tdx2=np.divide(temperature_minus_background_crop_binned[1:-1,1:-1,2:]-np.multiply(2,temperature_minus_background_crop_binned[1:-1,1:-1,1:-1])+temperature_minus_background_crop_binned[1:-1,1:-1,:-2],dx**2).astype(np.float32)
	d2Tdy2=np.divide(temperature_minus_background_crop_binned[1:-1,2:,1:-1]-np.multiply(2,temperature_minus_background_crop_binned[1:-1,1:-1,1:-1])+temperature_minus_background_crop_binned[1:-1,:-2,1:-1],dx**2).astype(np.float32)
	d2Tdxy = np.ones_like(dTdt).astype(np.float32)*np.nan
	d2Tdxy[:,nan_ROI_mask[1:-1,1:-1]]=np.add(d2Tdx2[:,nan_ROI_mask[1:-1,1:-1]],d2Tdy2[:,nan_ROI_mask[1:-1,1:-1]])
	del d2Tdx2,d2Tdy2
	negd2Tdxy=np.multiply(-1,d2Tdxy)
	T4=(temperature_minus_background_crop_binned[1:-1,1:-1,1:-1]+np.nanmean(temperature_ref_crop_binned)+zeroC)**4
	T04=(np.nanmean(temperature_ref_crop_binned)+zeroC)**4 *np.ones_like(temperature_minus_background_crop_binned[1:-1,1:-1,1:-1])
	T4_T04 = np.ones_like(dTdt).astype(np.float32)*np.nan
	T4_T04[:,nan_ROI_mask[1:-1,1:-1]] = (T4[:,nan_ROI_mask[1:-1,1:-1]]-T04[:,nan_ROI_mask[1:-1,1:-1]]).astype(np.float32)

	BBrad = np.ones_like(dTdt).astype(np.float32)*np.nan
	BBrad[:,nan_ROI_mask[1:-1,1:-1]] = (2*sigmaSB*T4_T04[:,nan_ROI_mask[1:-1,1:-1]] * foilemissivityscaled[nan_ROI_mask[1:-1,1:-1]]).astype(np.float32)
	diffusion = np.ones_like(dTdt).astype(np.float32)*np.nan
	diffusion[:,nan_ROI_mask[1:-1,1:-1]] = (Ptthermalconductivity*negd2Tdxy[:,nan_ROI_mask[1:-1,1:-1]]*foilthicknessscaled[nan_ROI_mask[1:-1,1:-1]]).astype(np.float32)
	timevariation = np.ones_like(dTdt).astype(np.float32)*np.nan
	timevariation[:,nan_ROI_mask[1:-1,1:-1]] = (Ptthermalconductivity*dTdt[:,nan_ROI_mask[1:-1,1:-1]]*foilthicknessscaled[nan_ROI_mask[1:-1,1:-1]]*reciprdiffusivityscaled[nan_ROI_mask[1:-1,1:-1]]).astype(np.float32)
	powernoback = (diffusion + timevariation + BBrad).astype(np.float32)

	horizontal_coord = np.arange(np.shape(powernoback[0])[1])
	vertical_coord = np.arange(np.shape(powernoback[0])[0])
	horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
	horizontal_coord = (horizontal_coord+1+0.5)*dx	# +1 because in the process of calculating the power I eliminate the first and last pixel in spatial coordinates, +0.5 do be the centre of the pixel
	vertical_coord = (vertical_coord+1+0.5)*dx
	horizontal_coord -= foil_position_dict['foilhorizw']*0.5+0.0198
	vertical_coord -= foil_position_dict['foilvertw']*0.5-0.0198
	distance_from_vertical = (horizontal_coord**2+vertical_coord**2)**0.5
	pinhole_to_foil_vertical = 0.008 + 0.003 + 0.002 + 0.045	# pinhole holder, washer, foil holder, stand_off
	pinhole_to_pixel_distance = (pinhole_to_foil_vertical**2 + distance_from_vertical**2)**0.5

	etendue = np.ones_like(powernoback[0]) * (np.pi*(0.002**2)) / (pinhole_to_pixel_distance**2)	# I should include also the area of the pixel, but that is already in the w/m2 power
	etendue *= (pinhole_to_foil_vertical/pinhole_to_pixel_distance)**2	 # cos(a)*cos(b). for pixels not directly under the pinhole both pinhole and pixel are tilted respect to the vertical, with same angle.
	brightness = 4*np.pi*powernoback/etendue

	temp = brightness[:,:,:int(np.shape(brightness)[2]*0.75)]
	temp = np.sort(temp[np.max(temp,axis=(1,2)).argmax()].flatten())
	ani = movie_from_data(np.array([np.flip(np.transpose(brightness,(0,2,1)),axis=2)]), 1/np.median(np.diff(time_binned)),timesteps=time_binned[1:-1],integration=laser_int_time/1000,time_offset=time_binned[0],extvmin=0,extvmax=np.nanmean(temp[-len(temp)//60:]),xlabel='horizontal coord [pixels]', ylabel='vertical coord [pixels]',barlabel='brightness [W/m2]', prelude='shot ' + laser_to_analyse[-9:-4]+'\n'+binning_type+'\n',overlay_structure=True,include_EFIT=True,pulse_ID=laser_to_analyse[-9:-4],overlay_x_point=True,overlay_mag_axis=True,overlay_strike_points=True,overlay_separatrix=True)
	ani.save('/home/ffederic/work/irvb/MAST-U/FAST_results/'+os.path.split(laser_to_analyse[:-4])[1]+ '_FAST_brightness.mp4', fps=5*(1/np.mean(np.diff(time_binned)))/383, writer='ffmpeg',codec='mpeg4')
	plt.close('all')

	print('completed FAST power calculation ' + laser_to_analyse)

	return foilrotdeg,out_of_ROI_mask,foildw,foilup,foillx,foilrx,FAST_counts_minus_background_crop_binned,time_binned,powernoback,brightness,binning_type

#######################################################################################################################################################################################################

def find_radiator_location(inverted_data,inversion_R,inversion_Z,time_full_binned_crop,efit_reconstruction,min_distance_between_peaks=0.5,radious_around_xpoint_for_radiator=0.2):
	from scipy.interpolate.fitpack2 import RectBivariateSpline
	# I want to filter points too close to each other
	# min_distance_between_peaks = 0.5	# m
	a,b = np.meshgrid(inversion_Z[1:-1],inversion_R[1:-1])
	dr = np.median(np.diff(inversion_R))
	dz = np.median(np.diff(inversion_Z))
	a_flat = a.flatten()
	b_flat = b.flatten()

	radiator_xpoint_distance_all = []
	radiator_above_xpoint_all = []
	radiator_magnetic_radious_all = []
	radiator_position_all = []
	radiator_baricentre_magnetic_radious_all = []
	radiator_baricentre_position_all = []
	radiator_baricentre_above_xpoint_all = []
	for i_t in range(len(time_full_binned_crop)):
		try:
			i_efit_time = np.abs(efit_reconstruction.time-time_full_binned_crop[i_t]).argmin()

			central_voxel = cp.deepcopy(inverted_data[i_t,1:-1,1:-1])
			central_voxel_flat = central_voxel.flatten()
			highest_surrounding_voxel = np.nanmax([inverted_data[i_t,1:-1,:-2],inverted_data[i_t,:-2,:-2],inverted_data[i_t,:-2,1:-1],inverted_data[i_t,:-2,2:],inverted_data[i_t,1:-1,2:],inverted_data[i_t,2:,2:],inverted_data[i_t,2:,1:-1],inverted_data[i_t,2:,:-2]],axis=0)
			peaks = np.logical_and(central_voxel>highest_surrounding_voxel,central_voxel>0)

			# filtering by distance
			peaks_location = peaks.flatten()
			peaks_location = peaks_location*np.arange(len(peaks_location))
			peaks_location = peaks_location[peaks_location>0]
			new_peaks = np.zeros_like(peaks.flatten()).astype(int)
			for i_ in range(len(peaks_location)):
				distance = ((a_flat-a_flat[peaks_location[i_]])**2 + (b_flat-b_flat[peaks_location[i_]])**2)**0.5
				if np.sum(central_voxel_flat[np.logical_and(np.logical_and(distance < min_distance_between_peaks,distance >0),peaks.flatten())] > central_voxel_flat[peaks_location[i_]])==0:
					# print(peaks_location[i_])
					new_peaks[peaks_location[i_]] = peaks_location[i_]

			if False:
				plt.figure()
				plt.imshow(np.flip(np.transpose(central_voxel,(1,0)),axis=0),extent=[b_flat.min()-dr/2,b_flat.max()+dr/2,a_flat.min()-dz/2,a_flat.max()+dz/2])
				plt.plot(_MASTU_CORE_GRID_POLYGON[:, 0], _MASTU_CORE_GRID_POLYGON[:, 1], 'k')
				plt.plot(b_flat[new_peaks>0],a_flat[new_peaks>0],'+r')
				temp = np.abs(efit_reconstruction.time-time_full_binned_crop[i_t]).argmin()
				for i in range(len(all_time_sep_r[temp])):
					plt.plot(r_fine[all_time_sep_r[temp][i]],z_fine[all_time_sep_z[temp][i]],'--b')
				plt.pause(0.01)

			closer_peak = ((efit_reconstruction.lower_xpoint_z[i_efit_time] - a_flat[new_peaks>0])**2 +  + (efit_reconstruction.lower_xpoint_r[i_efit_time] - b_flat[new_peaks>0])**2).argmin()
			closer_peak = new_peaks[new_peaks>0][closer_peak]
		except:
			closer_peak = 0
		radiator_xpoint_distance = ((efit_reconstruction.lower_xpoint_z[i_efit_time] - a_flat[closer_peak])**2 + (efit_reconstruction.lower_xpoint_r[i_efit_time] - b_flat[closer_peak])**2)**0.5
		radiator_above_xpoint = a_flat[closer_peak]-efit_reconstruction.lower_xpoint_z[i_efit_time]
		interpolator = RectBivariateSpline(efit_reconstruction.R,efit_reconstruction.Z,efit_reconstruction.psidat[i_efit_time].T)
		radiator_magnetic_radious = (efit_reconstruction.psi_axis[i_efit_time] - interpolator(b_flat[closer_peak],a_flat[closer_peak])[0,0])/(efit_reconstruction.psi_axis[i_efit_time] - efit_reconstruction.psi_bnd[i_efit_time])
		radiator_xpoint_distance_all.append(radiator_xpoint_distance)
		radiator_above_xpoint_all.append(radiator_above_xpoint)
		radiator_magnetic_radious_all.append(radiator_magnetic_radious)
		radiator_position_all.append([b_flat[closer_peak],a_flat[closer_peak]])

		# bericentre of the emission. i don't take the baricentre of the radiated power on purpose
		# i want to see where the emissivity is from, not where emission dominates
		distance = ((a-efit_reconstruction.lower_xpoint_z[i_efit_time])**2 + (b-efit_reconstruction.lower_xpoint_r[i_efit_time])**2)**0.5
		select = distance < radious_around_xpoint_for_radiator
		z_baricentre = np.sum(a[select]*inverted_data[i_t,1:-1,1:-1][select])/np.sum(inverted_data[i_t,1:-1,1:-1][select])
		r_baricentre = np.sum(b[select]*inverted_data[i_t,1:-1,1:-1][select])/np.sum(inverted_data[i_t,1:-1,1:-1][select])
		radiator_baricentre_magnetic_radious = (efit_reconstruction.psi_axis[i_efit_time] - interpolator(r_baricentre,z_baricentre)[0,0])/(efit_reconstruction.psi_axis[i_efit_time] - efit_reconstruction.psi_bnd[i_efit_time])
		radiator_baricentre_magnetic_radious_all.append(radiator_baricentre_magnetic_radious)
		radiator_baricentre_position_all.append([r_baricentre,z_baricentre])
		radiator_baricentre_above_xpoint_all.append(z_baricentre-efit_reconstruction.lower_xpoint_z[i_efit_time])

	radiator_xpoint_distance_all = np.array(radiator_xpoint_distance_all)
	radiator_above_xpoint_all = np.array(radiator_above_xpoint_all)
	radiator_magnetic_radious_all = np.array(radiator_magnetic_radious_all)
	additional_points_dict = dict([])
	additional_points_dict['time'] = time_full_binned_crop
	additional_points_dict['0'] = np.array(radiator_position_all)
	additional_points_dict['1'] = np.array(radiator_baricentre_position_all)
	additional_points_dict['number_of_points'] = 2
	additional_points_dict['marker'] = ['xk','xb']
	return additional_points_dict,radiator_xpoint_distance_all,radiator_above_xpoint_all,radiator_magnetic_radious_all,radiator_baricentre_magnetic_radious_all,radiator_baricentre_above_xpoint_all

def track_outer_leg_radiation(inverted_data,inversion_R,inversion_Z,time_full_binned_crop,efit_reconstruction,leg_resolution = 0.1,starting_distance_find_separatrix=0.001,sideways_leg_resolution=0.1,type='leg_only'):
	from shapely.geometry import Point
	from shapely.geometry.polygon import Polygon
	from scipy.ndimage import generic_filter
	import mastu_exhaust_analysis.fluxsurface_tracer as ft
	from mastu_exhaust_analysis.read_efit import read_epm

	all_time_sep_r,all_time_sep_z,r_fine,z_fine = efit_reconstruction_to_separatrix_on_foil(efit_reconstruction)	# the separatrix are ordered as left_of_up_xpoints,right_of_up_xpoints,left_of_low_xpoints,right_of_low_xpoints
	all_time_strike_points_location = return_all_time_strike_points_location_radial(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine)

	fdir = uda_transfer(efit_reconstruction.shotnumber,'epm')
	efit_data = read_epm(fdir,calc_bfield=True)
	os.remove(fdir)

	a,b = np.meshgrid(inversion_Z,inversion_R)
	a_flat = a.flatten()
	b_flat = b.flatten()
	grid_resolution = np.median(np.concatenate([np.diff(inversion_Z),np.diff(inversion_R)]))
	# leg_resolution = 0.1	# m

	data_length = 0
	local_mean_emis_all = []
	local_power_all = []
	leg_length_all = []
	leg_length_interval_all = []
	for i_t in range(len(time_full_binned_crop)):
		print('starting t=%.4gms' %(time_full_binned_crop[i_t]*1e3))
		try:
			i_efit_time = np.abs(efit_reconstruction.time-time_full_binned_crop[i_t]).argmin()
			try:
				distance_find_separatrix = cp.deepcopy(starting_distance_find_separatrix)
				r_coord_smooth_int = np.array([0,0])
				while np.sum(r_coord_smooth_int<efit_data['lower_xpoint_r'][i_efit_time])>0 and distance_find_separatrix<0.2:
					trace_flux_surface = ft.trace_flux_surface(efit_data, i_efit_time,efit_data['lower_xpoint_r'][i_efit_time]+distance_find_separatrix,efit_data['lower_xpoint_z'][i_efit_time])
					r_coord_smooth = np.array(trace_flux_surface.r)
					z_coord_smooth = np.array(trace_flux_surface.z)
					r_coord_smooth_int = r_coord_smooth[z_coord_smooth<=efit_reconstruction.mag_axis_z[i_efit_time]]	# I add this because it could get confusing for signficant dr_sep
					# print(distance_find_separatrix)
					distance_find_separatrix += 0.001
				if False:
					plt.plot(trace_flux_surface.r,trace_flux_surface.z,'+')
				r_coord_smooth_int = scipy.signal.savgol_filter(np.array(trace_flux_surface.r),7,2)
				z_coord_smooth_int = scipy.signal.savgol_filter(np.array(trace_flux_surface.z),7,2)
				# to restore previous behaviour, I cut the separatrix above the x-point
				distance = (r_coord_smooth_int-efit_reconstruction.lower_xpoint_r[i_efit_time])**2 + (z_coord_smooth_int-efit_reconstruction.lower_xpoint_z[i_efit_time])**2
				r_coord_smooth = r_coord_smooth_int[np.abs(distance).argmin()+1:]
				z_coord_smooth = z_coord_smooth_int[np.abs(distance).argmin()+1:]
				r_coord_smooth = np.concatenate(([[efit_reconstruction.lower_xpoint_r[i_efit_time]]],[r_coord_smooth]),axis=1)[0]
				z_coord_smooth = np.concatenate(([[efit_reconstruction.lower_xpoint_z[i_efit_time]]],[z_coord_smooth]),axis=1)[0]
				if type=='separatrix':
					r_coord_smooth = np.concatenate(([ r_coord_smooth_int[:np.abs(distance).argmin()][z_coord_smooth_int[:np.abs(distance).argmin()]<=efit_reconstruction.mag_axis_z[i_efit_time]] ],[r_coord_smooth]),axis=1)[0]
					z_coord_smooth = np.concatenate(([ z_coord_smooth_int[:np.abs(distance).argmin()][z_coord_smooth_int[:np.abs(distance).argmin()]<=efit_reconstruction.mag_axis_z[i_efit_time]] ],[z_coord_smooth]),axis=1)[0]
				r_coord_smooth = np.flip(r_coord_smooth,axis=0)
				z_coord_smooth = np.flip(z_coord_smooth,axis=0)
			except:
				if len(all_time_sep_r[i_efit_time][1])==0 and len(all_time_sep_r[i_efit_time][3])>0:
					i_closer_separatrix_to_x_point = 3
				elif len(all_time_sep_r[i_efit_time][3])==0 and len(all_time_sep_r[i_efit_time][1])>0:
					i_closer_separatrix_to_x_point = 1
				elif len(all_time_sep_r[i_efit_time][3])==0 and len(all_time_sep_r[i_efit_time][1])==0:
					local_mean_emis_all.append([])
					local_power_all.append([])
					leg_length_all.append(0)
					leg_length_interval_all.append([])
					continue
				elif np.abs((r_fine[all_time_sep_r[i_efit_time][1]]- efit_reconstruction.lower_xpoint_r[i_efit_time])**2 + (z_fine[all_time_sep_z[i_efit_time][1]]- efit_reconstruction.lower_xpoint_z[i_efit_time])**2 ).min() < np.abs((r_fine[all_time_sep_r[i_efit_time][3]]- efit_reconstruction.lower_xpoint_r[i_efit_time])**2 + (z_fine[all_time_sep_z[i_efit_time][3]]- efit_reconstruction.lower_xpoint_z[i_efit_time])**2 ).min():
					i_closer_separatrix_to_x_point = 1
				else:
					i_closer_separatrix_to_x_point = 3
				i_where_x_point_is = np.abs((r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]]- efit_reconstruction.lower_xpoint_r[i_efit_time])**2 + (z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]]- efit_reconstruction.lower_xpoint_z[i_efit_time])**2 ).argmin()

				temp = np.array([((all_time_strike_points_location[i_efit_time][0][i] - r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][:i_where_x_point_is])**2 + (all_time_strike_points_location[i_efit_time][1][i] - z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][:i_where_x_point_is])**2).min() for i in range(len(all_time_strike_points_location[i_efit_time][1]))])
				temp[np.isnan(temp)] = np.inf
				i_which_strike_point_is = temp.argmin()
				i_where_strike_point_is = ((all_time_strike_points_location[i_efit_time][0][i_which_strike_point_is] - r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][:i_where_x_point_is])**2 + (all_time_strike_points_location[i_efit_time][1][i_which_strike_point_is] - z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][:i_where_x_point_is])**2).argmin()
				# plt.figure()
				# plt.plot(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is],z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is])
				# plt.pause(0.001)
				# r_coord_smooth = generic_filter(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is],np.mean,size=11)
				# z_coord_smooth = generic_filter(z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is],np.mean,size=11)
				r_coord_smooth = scipy.signal.savgol_filter(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is],max(11,int(len(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is])/10//2*2+1)),2)
				z_coord_smooth = scipy.signal.savgol_filter(z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is],max(11,int(len(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is])/10//2*2+1)),2)

			leg_length = np.sum((np.diff(r_coord_smooth)**2 + np.diff(z_coord_smooth)**2)**0.5)
			# I arbitrarily decide to cut the leg in 10cm pieces
			if False:	# this seems a very weird way of doing it
				leg_length_interval = [0]
				target_length = 0 + leg_resolution
				i_ref_points = [0]
				ref_points = [[r_coord_smooth[0],z_coord_smooth[0]]]
				while target_length < leg_length + leg_resolution:
					# print(target_length)
					temp = np.abs(np.cumsum((np.diff(r_coord_smooth)**2 + np.diff(z_coord_smooth)**2)**0.5) - target_length).argmin()
					leg_length_interval.append(np.cumsum((np.diff(r_coord_smooth)**2 + np.diff(z_coord_smooth)**2)**0.5)[temp] - np.sum(leg_length_interval))
					i_ref_points.append(temp+1)
					ref_points.append([r_coord_smooth[temp+1],z_coord_smooth[temp+1]])
					target_length += leg_resolution
				ref_points = np.array(ref_points)
				leg_length_interval = leg_length_interval[1:]
				# I want to eliminate doubles
				i_ref_points = np.concatenate([[i_ref_points[0]],np.array(i_ref_points[1:])[np.abs(np.diff(i_ref_points))>0]])
				ref_points = np.concatenate([[ref_points[0]],ref_points[1:][np.abs(np.diff(ref_points[:,0]))+np.abs(np.diff(ref_points[:,1]))>0]])
				leg_length_interval = np.array(leg_length_interval)[np.array(leg_length_interval)>0].tolist()

				ref_points_1 = expand_line_sideways(r_coord_smooth,z_coord_smooth,i_ref_points,-sideways_leg_resolution)
				ref_points_2 = expand_line_sideways(r_coord_smooth,z_coord_smooth,i_ref_points,+sideways_leg_resolution)
			else:	# I redo it a bit more proper
				intervals = (np.diff(r_coord_smooth)**2 + np.diff(z_coord_smooth)**2)**0.5
				intervals = np.concatenate(([[0]],[intervals]),axis=1)[0]
				leg_length = np.cumsum(intervals)
				n_intervals = int(np.ceil(leg_length[-1]/leg_resolution))
				real_longitudinal_leg_resolution = leg_length[-1]/n_intervals
				ref_points = [np.interp(np.arange(n_intervals+1)*real_longitudinal_leg_resolution,leg_length,r_coord_smooth) , np.interp(np.arange(n_intervals+1)*real_longitudinal_leg_resolution,leg_length,z_coord_smooth)]
				ref_points = np.array(ref_points).T
				leg_length = leg_length[-1]
				leg_length_interval = ((np.diff(ref_points[:,0])**2 + np.diff(ref_points[:,1])**2)**0.5).tolist()

				ref_points_1 = expand_line_sideways(ref_points[:,0],ref_points[:,1],np.arange(n_intervals+1),-sideways_leg_resolution)
				ref_points_2 = expand_line_sideways(ref_points[:,0],ref_points[:,1],np.arange(n_intervals+1),+sideways_leg_resolution)
			# ref_points_1 = []
			# ref_points_2 = []
			# try:
			# 	m = -1/((z_coord_smooth[i_ref_points[0]]-z_coord_smooth[i_ref_points[0]+1])/(r_coord_smooth[i_ref_points[0]]-r_coord_smooth[i_ref_points[0]+1]))
			# 	ref_points_1.append([r_coord_smooth[i_ref_points[0]] - leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[0]] - m*leg_resolution/((1+m**2)**0.5)])
			# 	ref_points_2.append([r_coord_smooth[i_ref_points[0]] + leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[0]] + m*leg_resolution/((1+m**2)**0.5)])
			# except:
			# 	pass
			# for i_ref_point in range(1,len(i_ref_points)):
			# 	try:
			# 		m = -1/((z_coord_smooth[i_ref_points[i_ref_point]-1]-z_coord_smooth[i_ref_points[i_ref_point]+1])/(r_coord_smooth[i_ref_points[i_ref_point]-1]-r_coord_smooth[i_ref_points[i_ref_point]+1]))
			# 		ref_points_1.append([r_coord_smooth[i_ref_points[i_ref_point]] - leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[i_ref_point]] - m*leg_resolution/((1+m**2)**0.5)])
			# 		ref_points_2.append([r_coord_smooth[i_ref_points[i_ref_point]] + leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[i_ref_point]] + m*leg_resolution/((1+m**2)**0.5)])
			# 	except:
			# 		pass
			# try:
			# 	m = -1/((z_coord_smooth[i_ref_points[-1]-1]-z_coord_smooth[i_ref_points[-1]])/(r_coord_smooth[i_ref_points[-1]-1]-r_coord_smooth[i_ref_points[-1]]))
			# 	ref_points_1.append([r_coord_smooth[i_ref_points[-1]] - leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[-1]] - m*leg_resolution/((1+m**2)**0.5)])
			# 	ref_points_2.append([r_coord_smooth[i_ref_points[-1]] + leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[-1]] + m*leg_resolution/((1+m**2)**0.5)])
			# except:
			# 	pass
			# ref_points_1 = np.array(ref_points_1)
			# ref_points_2 = np.array(ref_points_2)

			# plt.figure()
			# plt.plot(r_coord_smooth,z_coord_smooth)
			# plt.plot(r_coord_smooth,z_coord_smooth,'+')
			# plt.plot(ref_points[:,0],ref_points[:,1],'+')
			# plt.plot(ref_points_1[:,0],ref_points_1[:,1],'o')
			# plt.plot(ref_points_2[:,0],ref_points_2[:,1],'+')
			# plt.pause(0.001)

			local_mean_emis = []
			local_power = []
			emissivity_flat = cp.deepcopy(inverted_data[i_t].flatten())
			num_cells=[]
			for i_ref_point in range(1,len(ref_points)):
				# print(i_ref_point)
				polygon = Polygon([ref_points_1[i_ref_point-1], ref_points_1[i_ref_point], ref_points_2[i_ref_point], ref_points_2[i_ref_point-1]])
				# select = []
				# for i_e in range(len(emissivity_flat)):
				# 	point = Point((b_flat[i_e],a_flat[i_e]))
				# 	select.append(polygon.contains(point))
				select = select_cells_inside_polygon(polygon,[inversion_R,inversion_Z],center_line=ref_points).flatten()
				num_cells.append(np.sum(select))
				local_mean_emis.append(np.nanmean(emissivity_flat[select]))
				local_power.append(2*np.pi*np.nansum(emissivity_flat[select]*b_flat[select]*(grid_resolution**2)))
			# local_mean_emis = np.array(local_mean_emis)
			local_power = np.array(local_power)
			num_cells = np.array(num_cells)
			local_power = (local_power/num_cells * np.mean(num_cells)).tolist()
			# local_mean_emis = local_mean_emis[np.logical_not(np.isnan(local_mean_emis))].tolist()
			# local_power = local_power[np.logical_not(np.isnan(local_power))].tolist()
			local_mean_emis_all.append(local_mean_emis)
			local_power_all.append(local_power)
			data_length = max(data_length,len(local_power))
			leg_length_all.append(leg_length)
			leg_length_interval_all.append(leg_length_interval)
		except Exception as e:
			# logging.exception('with error: ' + str(e))
			local_mean_emis_all.append([])
			local_power_all.append([])
			leg_length_interval_all.append([])
			leg_length_all.append(0)

	for i_t in range(len(time_full_binned_crop)):
		if len(local_mean_emis_all[i_t])<data_length:
			local_mean_emis_all[i_t].extend([0]*(data_length-len(local_mean_emis_all[i_t])))
			local_power_all[i_t].extend([0]*(data_length-len(local_power_all[i_t])))
			leg_length_interval_all[i_t].extend([0]*(data_length-len(leg_length_interval_all[i_t])))
	average_leg_resolution = np.nanmean(np.array(leg_length_interval_all)[np.array(leg_length_interval_all)>0])

	return local_mean_emis_all,local_power_all,leg_length_interval_all,leg_length_all,data_length,average_leg_resolution

def track_inner_leg_radiation(inverted_data,inversion_R,inversion_Z,time_full_binned_crop,efit_reconstruction,leg_resolution = 0.05,starting_distance_find_separatrix=0.001,sideways_leg_resolution=0.05,type='leg_only'):
	from shapely.geometry import Point
	from shapely.geometry.polygon import Polygon
	from scipy.ndimage import generic_filter
	import mastu_exhaust_analysis.fluxsurface_tracer as ft
	from mastu_exhaust_analysis.read_efit import read_epm

	all_time_sep_r,all_time_sep_z,r_fine,z_fine = efit_reconstruction_to_separatrix_on_foil(efit_reconstruction)	# the separatrix are ordered as left_of_up_xpoints,right_of_up_xpoints,left_of_low_xpoints,right_of_low_xpoints
	all_time_strike_points_location = return_all_time_strike_points_location_radial(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine)

	fdir = uda_transfer(efit_reconstruction.shotnumber,'epm')
	efit_data = read_epm(fdir,calc_bfield=True)
	os.remove(fdir)

	a,b = np.meshgrid(inversion_Z,inversion_R)
	a_flat = a.flatten()
	b_flat = b.flatten()
	grid_resolution = np.median(np.concatenate([np.diff(inversion_Z),np.diff(inversion_R)]))
	# leg_resolution = 0.1	# m

	data_length = 0
	local_mean_emis_all = []
	local_power_all = []
	leg_length_all = []
	leg_length_interval_all = []
	for i_t in range(len(time_full_binned_crop)):
		print('starting t=%.4gms' %(time_full_binned_crop[i_t]*1e3))
		try:
			i_efit_time = np.abs(efit_reconstruction.time-time_full_binned_crop[i_t]).argmin()
			try:
				distance_find_separatrix = cp.deepcopy(starting_distance_find_separatrix)
				r_coord_smooth_int = np.array([2,2])
				while np.sum(r_coord_smooth_int>efit_data['lower_xpoint_r'][i_efit_time])>0 and distance_find_separatrix<0.2:
					trace_flux_surface = ft.trace_flux_surface(efit_data, i_efit_time,efit_data['lower_xpoint_r'][i_efit_time]-distance_find_separatrix,efit_data['lower_xpoint_z'][i_efit_time])
					r_coord_smooth = np.array(trace_flux_surface.r)
					z_coord_smooth = np.array(trace_flux_surface.z)
					r_coord_smooth_int = r_coord_smooth[z_coord_smooth<=efit_reconstruction.mag_axis_z[i_efit_time]]	# I add this because it could get confusing for signficant dr_sep
					# print(distance_find_separatrix)
					distance_find_separatrix += 0.001
				if False:
					plt.plot(trace_flux_surface.r,trace_flux_surface.z,'+')
				r_coord_smooth_int = scipy.signal.savgol_filter(np.array(trace_flux_surface.r),7,2)
				z_coord_smooth_int = scipy.signal.savgol_filter(np.array(trace_flux_surface.z),7,2)
				# to restore previous behaviour, I cut the separatrix above the x-point
				distance = (r_coord_smooth_int-efit_reconstruction.lower_xpoint_r[i_efit_time])**2 + (z_coord_smooth_int-efit_reconstruction.lower_xpoint_z[i_efit_time])**2
				r_coord_smooth = r_coord_smooth_int[:np.abs(distance).argmin()]
				z_coord_smooth = z_coord_smooth_int[:np.abs(distance).argmin()]
				r_coord_smooth = np.concatenate(([r_coord_smooth],[[efit_reconstruction.lower_xpoint_r[i_efit_time]]]),axis=1)[0]
				z_coord_smooth = np.concatenate(([z_coord_smooth],[[efit_reconstruction.lower_xpoint_z[i_efit_time]]]),axis=1)[0]
				if type=='separatrix':
					r_coord_smooth = np.concatenate(([r_coord_smooth],[ r_coord_smooth_int[np.abs(distance).argmin()+1:][z_coord_smooth_int[np.abs(distance).argmin()+1:]<=efit_reconstruction.mag_axis_z[i_efit_time]] ]),axis=1)[0]
					z_coord_smooth = np.concatenate(([z_coord_smooth],[ z_coord_smooth_int[np.abs(distance).argmin()+1:][z_coord_smooth_int[np.abs(distance).argmin()+1:]<=efit_reconstruction.mag_axis_z[i_efit_time]] ]),axis=1)[0]
			except:
				if len(all_time_sep_r[i_efit_time][1])==0 and len(all_time_sep_r[i_efit_time][3])>0:
					i_closer_separatrix_to_x_point = 3
				elif len(all_time_sep_r[i_efit_time][3])==0 and len(all_time_sep_r[i_efit_time][1])>0:
					i_closer_separatrix_to_x_point = 1
				elif len(all_time_sep_r[i_efit_time][3])==0 and len(all_time_sep_r[i_efit_time][1])==0:
					local_mean_emis_all.append([])
					local_power_all.append([])
					leg_length_all.append(0)
					leg_length_interval_all.append([])
					continue
				elif np.abs((r_fine[all_time_sep_r[i_efit_time][0]]- efit_reconstruction.lower_xpoint_r[i_efit_time])**2 + (z_fine[all_time_sep_z[i_efit_time][0]]- efit_reconstruction.lower_xpoint_z[i_efit_time])**2 ).min() < np.abs((r_fine[all_time_sep_r[i_efit_time][2]]- efit_reconstruction.lower_xpoint_r[i_efit_time])**2 + (z_fine[all_time_sep_z[i_efit_time][2]]- efit_reconstruction.lower_xpoint_z[i_efit_time])**2 ).min():
					i_closer_separatrix_to_x_point = 0
				else:
					i_closer_separatrix_to_x_point = 2
				i_where_x_point_is = np.abs((r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]]- efit_reconstruction.lower_xpoint_r[i_efit_time])**2 + (z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]]- efit_reconstruction.lower_xpoint_z[i_efit_time])**2 ).argmin()

				temp = np.array([((all_time_strike_points_location[i_efit_time][0][i] - r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][:i_where_x_point_is])**2 + (all_time_strike_points_location[i_efit_time][1][i] - z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][:i_where_x_point_is])**2).min() for i in range(len(all_time_strike_points_location[i_efit_time][1]))])
				temp[np.isnan(temp)] = np.inf
				i_which_strike_point_is = temp.argmin()
				i_where_strike_point_is = ((all_time_strike_points_location[i_efit_time][0][i_which_strike_point_is] - r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][:i_where_x_point_is])**2 + (all_time_strike_points_location[i_efit_time][1][i_which_strike_point_is] - z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][:i_where_x_point_is])**2).argmin()
				# plt.figure()
				# plt.plot(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is],z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is])
				# plt.pause(0.001)
				# r_coord_smooth = generic_filter(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is],np.mean,size=11)
				# z_coord_smooth = generic_filter(z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is],np.mean,size=11)
				r_coord_smooth = scipy.signal.savgol_filter(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is],max(9,int(len(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is])/3//2*2+1)),2)
				z_coord_smooth = scipy.signal.savgol_filter(z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is],max(9,int(len(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is])/3//2*2+1)),2)

			leg_length = np.sum((np.diff(r_coord_smooth)**2 + np.diff(z_coord_smooth)**2)**0.5)
			if False:	# this seems a very weird way of doing it
				# I arbitrarily decide to cut the leg in 10cm pieces
				leg_length_interval = [0]
				target_length = 0 + leg_resolution
				i_ref_points = [0]
				ref_points = [[r_coord_smooth[0],z_coord_smooth[0]]]
				while target_length < leg_length + leg_resolution:
					# print(target_length)
					temp = np.abs(np.cumsum((np.diff(r_coord_smooth)**2 + np.diff(z_coord_smooth)**2)**0.5) - target_length).argmin()
					leg_length_interval.append(np.cumsum((np.diff(r_coord_smooth)**2 + np.diff(z_coord_smooth)**2)**0.5)[temp] - np.sum(leg_length_interval))
					i_ref_points.append(temp+1)
					ref_points.append([r_coord_smooth[temp+1],z_coord_smooth[temp+1]])
					target_length += leg_resolution
				ref_points = np.array(ref_points)
				leg_length_interval = leg_length_interval[1:]
				# I want to eliminate doubles
				i_ref_points = np.concatenate([[i_ref_points[0]],np.array(i_ref_points[1:])[np.abs(np.diff(i_ref_points))>0]])
				ref_points = np.concatenate([[ref_points[0]],ref_points[1:][np.abs(np.diff(ref_points[:,0]))+np.abs(np.diff(ref_points[:,1]))>0]])
				leg_length_interval = np.array(leg_length_interval)[np.array(leg_length_interval)>0].tolist()

				ref_points_1 = expand_line_sideways(r_coord_smooth,z_coord_smooth,i_ref_points,-sideways_leg_resolution)
				ref_points_2 = expand_line_sideways(r_coord_smooth,z_coord_smooth,i_ref_points,+sideways_leg_resolution)
			else:	# I redo it a bit more proper
				intervals = (np.diff(r_coord_smooth)**2 + np.diff(z_coord_smooth)**2)**0.5
				intervals = np.concatenate(([[0]],[intervals]),axis=1)[0]
				leg_length = np.cumsum(intervals)
				n_intervals = int(np.ceil(leg_length[-1]/leg_resolution))
				real_longitudinal_leg_resolution = leg_length[-1]/n_intervals
				ref_points = [np.interp(np.arange(n_intervals+1)*real_longitudinal_leg_resolution,leg_length,r_coord_smooth) , np.interp(np.arange(n_intervals+1)*real_longitudinal_leg_resolution,leg_length,z_coord_smooth)]
				ref_points = np.array(ref_points).T
				leg_length = leg_length[-1]
				leg_length_interval = ((np.diff(ref_points[:,0])**2 + np.diff(ref_points[:,1])**2)**0.5).tolist()

				ref_points_1 = expand_line_sideways(ref_points[:,0],ref_points[:,1],np.arange(n_intervals+1),-sideways_leg_resolution)
				ref_points_2 = expand_line_sideways(ref_points[:,0],ref_points[:,1],np.arange(n_intervals+1),+sideways_leg_resolution)


			# ref_points_1 = []
			# ref_points_2 = []
			# try:
			# 	m = -1/((z_coord_smooth[i_ref_points[0]]-z_coord_smooth[i_ref_points[0]+1])/(r_coord_smooth[i_ref_points[0]]-r_coord_smooth[i_ref_points[0]+1]))
			# 	ref_points_1.append([r_coord_smooth[i_ref_points[0]] - leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[0]] - m*leg_resolution/((1+m**2)**0.5)])
			# 	ref_points_2.append([r_coord_smooth[i_ref_points[0]] + leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[0]] + m*leg_resolution/((1+m**2)**0.5)])
			# except:
			# 	pass
			# for i_ref_point in range(1,len(i_ref_points)):
			# 	try:
			# 		m = -1/((z_coord_smooth[i_ref_points[i_ref_point]-1]-z_coord_smooth[i_ref_points[i_ref_point]+1])/(r_coord_smooth[i_ref_points[i_ref_point]-1]-r_coord_smooth[i_ref_points[i_ref_point]+1]))
			# 		ref_points_1.append([r_coord_smooth[i_ref_points[i_ref_point]] - leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[i_ref_point]] - m*leg_resolution/((1+m**2)**0.5)])
			# 		ref_points_2.append([r_coord_smooth[i_ref_points[i_ref_point]] + leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[i_ref_point]] + m*leg_resolution/((1+m**2)**0.5)])
			# 	except:
			# 		pass
			# try:
			# 	m = -1/((z_coord_smooth[i_ref_points[-1]-1]-z_coord_smooth[i_ref_points[-1]])/(r_coord_smooth[i_ref_points[-1]-1]-r_coord_smooth[i_ref_points[-1]]))
			# 	ref_points_1.append([r_coord_smooth[i_ref_points[-1]] - leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[-1]] - m*leg_resolution/((1+m**2)**0.5)])
			# 	ref_points_2.append([r_coord_smooth[i_ref_points[-1]] + leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[-1]] + m*leg_resolution/((1+m**2)**0.5)])
			# except:
			# 	pass
			# ref_points_1 = np.array(ref_points_1)
			# ref_points_2 = np.array(ref_points_2)

			# plt.figure()
			# plt.plot(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is],z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is])
			# plt.plot(r_coord_smooth,z_coord_smooth)
			# plt.plot(r_coord_smooth,z_coord_smooth,'+')
			# plt.plot(ref_points[:,0],ref_points[:,1],'x')
			# plt.plot(ref_points_1[:,0],ref_points_1[:,1],'o')
			# plt.plot(ref_points_2[:,0],ref_points_2[:,1],'+')
			# plt.pause(0.001)

			local_mean_emis = []
			local_power = []
			emissivity_flat = cp.deepcopy(inverted_data[i_t].flatten())
			num_cells=[]
			for i_ref_point in range(1,len(ref_points)):
				# print(i_ref_point)
				polygon = Polygon([ref_points_1[i_ref_point-1], ref_points_1[i_ref_point], ref_points_2[i_ref_point], ref_points_2[i_ref_point-1]])
				# select = []
				# for i_e in range(len(emissivity_flat)):
				# 	point = Point((b_flat[i_e],a_flat[i_e]))
				# 	select.append(polygon.contains(point))
				select = select_cells_inside_polygon(polygon,[inversion_R,inversion_Z],center_line=ref_points).flatten()
				num_cells.append(np.sum(select))
				local_mean_emis.append(np.nanmean(emissivity_flat[select]))
				local_power.append(2*np.pi*np.nansum(emissivity_flat[select]*b_flat[select]*(grid_resolution**2)))
			# local_mean_emis = np.array(local_mean_emis)
			local_power = np.array(local_power)
			num_cells = np.array(num_cells)
			local_power = (local_power/num_cells * np.mean(num_cells)).tolist()
			# local_mean_emis = local_mean_emis[np.logical_not(np.isnan(local_mean_emis))].tolist()
			# local_power = local_power[np.logical_not(np.isnan(local_power))].tolist()
			local_mean_emis_all.append(local_mean_emis)
			local_power_all.append(local_power)
			data_length = max(data_length,len(local_power))
			leg_length_all.append(leg_length)
			leg_length_interval_all.append(leg_length_interval)
		except Exception as e:
			# logging.exception('with error: ' + str(e))
			local_mean_emis_all.append([])
			local_power_all.append([])
			leg_length_interval_all.append([])
			leg_length_all.append(0)

	for i_t in range(len(time_full_binned_crop)):
		if len(local_mean_emis_all[i_t])<data_length:
			local_mean_emis_all[i_t].extend([0]*(data_length-len(local_mean_emis_all[i_t])))
			local_power_all[i_t].extend([0]*(data_length-len(local_power_all[i_t])))
			leg_length_interval_all[i_t].extend([0]*(data_length-len(leg_length_interval_all[i_t])))
	average_leg_resolution = np.nanmean(np.array(leg_length_interval_all)[np.array(leg_length_interval_all)>0])

	return local_mean_emis_all,local_power_all,leg_length_interval_all,leg_length_all,data_length,average_leg_resolution

def plot_leg_radiation_tracking(inverted_data,inversion_R,inversion_Z,time_full_binned_crop,local_mean_emis_all,local_power_all,leg_length_interval_all,leg_length_all,data_length,leg_resolution,filename_root,filename_root_add,laser_to_analyse,scenario,which_leg='',which_part_of_separatrix='leg',x_point_L_pol=[]):
	from scipy.ndimage import generic_filter
	from scipy.ndimage import median_filter

	if which_leg=='':
		print('please specify whick leg is processed in plot_leg_radiation_tracking')

	fig, ax = plt.subplots( 1,2,figsize=(15, 10), squeeze=False,sharey=True)
	temp = np.array(local_power_all)
	temp[np.isnan(temp)] = 0
	im1 = ax[0,0].imshow(temp,'rainbow',origin='lower',extent=[(0)*leg_resolution,(data_length)*leg_resolution,time_full_binned_crop[0]-np.diff(time_full_binned_crop)[0]/2,time_full_binned_crop[-1]+np.diff(time_full_binned_crop)[-1]/2],aspect=10,vmin=np.min(temp[:-4]),vmax=generic_filter(temp,np.mean,size=[max(5,temp.shape[0]//10*2),max(5,temp.shape[1]//10*2)]).max())#,vmax=np.max(temp[:-4]))
	ax[0,0].plot(leg_length_all,time_full_binned_crop,'--k')
	ax[0,0].set_aspect(3)
	temp = np.array(local_mean_emis_all)
	temp[np.isnan(temp)] = 0
	im2 = ax[0,1].imshow(temp,'rainbow',origin='lower',extent=[(0)*leg_resolution,(data_length)*leg_resolution,time_full_binned_crop[0]-np.diff(time_full_binned_crop)[0]/2,time_full_binned_crop[-1]+np.diff(time_full_binned_crop)[-1]/2],aspect=10,vmin=np.min(temp[:-4]),vmax=generic_filter(temp,np.mean,size=[max(5,temp.shape[0]//10*2),max(5,temp.shape[1]//10*2)]).max())#,vmax=np.max(temp[:-4]))
	ax[0,1].plot(leg_length_all,time_full_binned_crop,'--k')
	peak = [np.nanargmax(gna) for gna in local_mean_emis_all]
	peak_location = [np.sum(leng[:val])+leng[val]/2 for val,leng in zip(peak,leg_length_interval_all)]
	midpoint_location = []
	for i in range(len(local_mean_emis_all)):
		midpoint_location.append(np.interp(np.nanmax(local_mean_emis_all[i])/2,local_mean_emis_all[i][:peak[i]+1],np.concatenate(([0],np.cumsum(leg_length_interval_all[i])[:peak[i]])),left=0,right=np.inf))
	ax[0,1].plot(peak_location,time_full_binned_crop,'ok')
	ax[0,1].plot(midpoint_location,time_full_binned_crop,'xk')
	ax[0,1].set_aspect(3)
	fig.suptitle('shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\ntracking radiation on the '+which_leg+' '+which_part_of_separatrix+'\naveraged/summed %.3gcm above and below the separatrix'%(leg_resolution*100))
	ax[0,0].set_xlabel('distance from the strike point [m]')
	ax[0,0].grid()
	ax[0,0].set_ylabel('time [s]')
	plt.colorbar(im1,ax=ax[0,0]).set_label('Integrated power [W]')
	# ax[0,0].colorbar().set_label('Integrated power [W]')
	ax[0,1].set_xlabel('distance from the strike point [m]')
	ax[0,1].grid()
	# ax[0,1].colorbar().set_label('Emissivity [W/m3]')
	ax[0,0].set_xlim(right=median_filter(leg_length_all,size=[max(5,len(leg_length_all)//5*2)]).max())
	ax[0,1].set_xlim(right=median_filter(leg_length_all,size=[max(5,len(leg_length_all)//5*2)]).max())
	plt.colorbar(im2,ax=ax[0,1]).set_label('Averaged emissivity [W/m3] o=peak, x=mid')
	if len(x_point_L_pol)>0:
		ax[0,0].plot(x_point_L_pol,time_full_binned_crop,'-k')
		ax[0,1].plot(x_point_L_pol,time_full_binned_crop,'-k')
	plt.savefig(filename_root+filename_root_add+'_'+which_leg+'_'+which_part_of_separatrix+'_radiation_tracking.eps')
	plt.close()

	number_of_curves_to_plot = 12
	alpha = 0.9
	# colors_smooth = np.array([np.linspace(0,1,num=number_of_curves_to_plot),np.linspace(1,0,num=number_of_curves_to_plot),[0]*number_of_curves_to_plot]).T
	colors_smooth = np.linspace(0,0.9,num=int(np.ceil(number_of_curves_to_plot/3))).astype(str)
	select = np.unique(np.round(np.linspace(0,len(local_mean_emis_all)-1,num=number_of_curves_to_plot))).astype(int)
	fig, ax = plt.subplots( 2,1,figsize=(10, 20), squeeze=False,sharex=True)
	fig.suptitle('shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\ntracking radiation on the '+which_leg+' leg')
	# plt.figure()
	for i_i_t,i_t in enumerate(select):
		# ax[0,0].plot(np.cumsum(leg_length_interval_all[i_t]),local_mean_emis_all[i_t],color=colors_smooth[i_i_t],label = '%.3gs' %(time_full_binned_crop[i_t]),linewidth=3)
		# ax[1,0].plot(np.cumsum(leg_length_interval_all[i_t]),local_power_all[i_t],color=colors_smooth[i_i_t],linewidth=3)
		to_plot_x = np.array(leg_length_interval_all[i_t])
		to_plot_y1 = np.array(local_mean_emis_all[i_t])
		to_plot_y1 = to_plot_y1[to_plot_x>0]
		to_plot_y2 = np.array(local_power_all[i_t])
		to_plot_y2 = to_plot_y2[to_plot_x>0]
		to_plot_x = to_plot_x[to_plot_x>0]
		to_plot_x = np.flip(np.sum(to_plot_x)-(np.cumsum(to_plot_x)-np.array(to_plot_x)/2),axis=0)
		to_plot_y1 = np.flip(to_plot_y1,axis=0)
		to_plot_y2 = np.flip(to_plot_y2,axis=0)
		if i_i_t%3==0:
			ax[0,0].plot(to_plot_x,to_plot_y1,'-',color=colors_smooth[i_i_t//3],label = '%.3gs' %(time_full_binned_crop[i_t]),linewidth=2,alpha=alpha)
			ax[1,0].plot(to_plot_x,to_plot_y2,'-',color=colors_smooth[i_i_t//3],linewidth=2,alpha=alpha)
		elif i_i_t%3==1:
			ax[0,0].plot(to_plot_x,to_plot_y1,'-.',color=colors_smooth[i_i_t//3],label = '%.3gs' %(time_full_binned_crop[i_t]),linewidth=2,alpha=alpha)
			ax[1,0].plot(to_plot_x,to_plot_y2,'-.',color=colors_smooth[i_i_t//3],linewidth=2,alpha=alpha)
		elif i_i_t%3==2:
			ax[0,0].plot(to_plot_x,to_plot_y1,':',color=colors_smooth[i_i_t//3],label = '%.3gs' %(time_full_binned_crop[i_t]),linewidth=2,alpha=alpha)
			ax[1,0].plot(to_plot_x,to_plot_y2,':',color=colors_smooth[i_i_t//3],linewidth=2,alpha=alpha)
	ax[0,0].legend(loc='best', fontsize='x-small')
	ax[0,0].set_ylabel('average emissivity [W/m3]')
	ax[1,0].set_ylabel('local radiated power [W]')
	# ax[1,0].set_xlabel('distance from target [m]')
	ax[1,0].set_xlabel('distance from x-point [m]')
	ax[0,0].grid()
	ax[1,0].grid()
	plt.savefig(filename_root+filename_root_add+'_'+which_leg+'_'+which_part_of_separatrix+'_radiation_tracking_2.png')
	plt.close()

	return peak_location,midpoint_location


def MASTU_pulse_process_FAST3(laser_counts_corrected,time_of_experiment_digitizer_ID,time_of_experiment,external_clock_marker,aggregated_correction_coefficients,laser_framerate,laser_digitizer_ID,laser_int_time,seconds_for_reference_frame,start_time_of_pulse,laser_to_analyse,height,width,flag_use_of_first_frames_as_reference,params,errparams,foil_position_dict):
	# created 2021/10/07
	# created modifying MASTU_pulse_process_FAST2 in order to include the bayesian inversion

	from scipy.ndimage import generic_filter

	max_ROI = [[0,255],[0,319]]
	# foil_position_dict = dict([('angle',0.5),('foilcenter',[158,136]),('foilhorizw',0.09),('foilvertw',0.07),('foilhorizwpixel',241)])	# fixed orientation, for now, this is from 2021-06-04/44168
	temp_ref_counts = []
	temp_counts_minus_background = []
	time_partial = []
	timesteps = np.inf
	for i in range(len(laser_digitizer_ID)):
		time_of_experiment_digitizer_ID_seconds = (time_of_experiment_digitizer_ID[i]-time_of_experiment[0])*1e-6-start_time_of_pulse
		if external_clock_marker:
			time_of_experiment_digitizer_ID_seconds = time_of_experiment_digitizer_ID_seconds-np.mean(aggregated_correction_coefficients[:,4])	# I use the mean of the coefficients because I want to avoid small unpredictable differences between the digitisers

		# basic smoothing
		spectra_orig=np.fft.fft(np.mean(laser_counts_corrected[i],axis=(-1,-2)))
		magnitude = 2 * np.abs(spectra_orig) / len(spectra_orig)
		freq = np.fft.fftfreq(len(magnitude), d=np.mean(np.diff(time_of_experiment_digitizer_ID_seconds)))
		magnitude = np.array([y for _, y in sorted(zip(freq, magnitude))])
		freq = np.sort(freq)
		magnitude_smooth = generic_filter(np.log(magnitude),np.median,size=[7])
		peak_oscillation = (magnitude-np.exp(magnitude_smooth))[np.logical_and(freq>10,freq<50)].argmax()
		peak_oscillation_freq = freq[np.logical_and(freq>10,freq<50)][peak_oscillation]
		frames_to_average = 1/peak_oscillation_freq/np.mean(np.diff(time_of_experiment_digitizer_ID_seconds))
		laser_counts_corrected_filtered = real_mean_filter_agent(laser_counts_corrected[i],frames_to_average)

		if flag_use_of_first_frames_as_reference:
			# temp_ref_counts.append(np.mean(laser_counts_corrected_filtered[time_of_experiment_digitizer_ID_seconds<0],axis=0))
			temp_ref_counts.append(np.mean(laser_counts_corrected_filtered[np.logical_and(time_of_experiment_digitizer_ID_seconds<0,time_of_experiment_digitizer_ID_seconds>-0.5)],axis=0))
		else:
			temp_ref_counts.append(np.mean(laser_counts_corrected_filtered[-int(seconds_for_reference_frame*laser_framerate/len(laser_digitizer_ID)):],axis=0))
		select_time = np.logical_and(time_of_experiment_digitizer_ID_seconds>=0,time_of_experiment_digitizer_ID_seconds<=1.5)
		temp_counts_minus_background.append(laser_counts_corrected_filtered[select_time]-temp_ref_counts[-1])
		time_partial.append(time_of_experiment_digitizer_ID_seconds[select_time])
		timesteps = min(timesteps,len(temp_counts_minus_background[-1]))

	for i in range(len(laser_digitizer_ID)):
		temp_counts_minus_background[i] = temp_counts_minus_background[i][:timesteps]
		time_partial[i] = time_partial[i][:timesteps]
	temp_counts_minus_background = np.nanmean(temp_counts_minus_background,axis=0)
	temp_ref_counts = np.nanmean(temp_ref_counts,axis=0)
	FAST_counts_minus_background_crop_time = np.nanmean(time_partial,axis=0)

	# I'm going to use the reference frames for foil position
	foilrotdeg,out_of_ROI_mask,foildw,foilup,foillx,foilrx = get_rotation_crop_parameters(temp_ref_counts,foil_position_dict,laser_to_analyse,temp_counts_minus_background,FAST_counts_minus_background_crop_time)

	# rotation and crop
	temp_counts_minus_background_rot=rotate(temp_counts_minus_background,foilrotdeg,axes=(-1,-2))
	if not (height==max_ROI[0][1]+1 and width==max_ROI[1][1]+1):
		temp_counts_minus_background_rot*=out_of_ROI_mask
		temp_counts_minus_background_rot[np.logical_and(temp_counts_minus_background_rot<np.nanmin(temp_counts_minus_background),temp_counts_minus_background_rot>np.nanmax(temp_counts_minus_background))]=0
	FAST_counts_minus_background_crop = temp_counts_minus_background_rot[:,foildw:foilup,foillx:foilrx]

	# I drop this to save memory
	# temp = FAST_counts_minus_background_crop[:,:,:int(np.shape(FAST_counts_minus_background_crop)[2]*0.75)]
	# temp = np.sort(temp[np.max(temp,axis=(1,2)).argmax()].flatten())
	# ani,efit_reconstruction = movie_from_data(np.array([np.flip(np.transpose(FAST_counts_minus_background_crop,(0,2,1)),axis=2)]), laser_framerate/len(laser_digitizer_ID),timesteps=FAST_counts_minus_background_crop_time,integration=laser_int_time/1000,time_offset=FAST_counts_minus_background_crop_time[0],extvmin=0,extvmax=np.nanmean(temp[-len(temp)//60:]),xlabel='horizontal coord [pixels]', ylabel='vertical coord [pixels]',barlabel='Count increase [au]', prelude='shot ' + laser_to_analyse[-9:-4]+'\n',overlay_structure=True,include_EFIT=True,pulse_ID=laser_to_analyse[-9:-4],overlay_x_point=True,overlay_mag_axis=True,overlay_strike_points=True,overlay_separatrix=True,EFIT_output_requested=True)
	# ani.save(laser_to_analyse[:-4]+ '_FAST_count_increase.mp4', fps=5*laser_framerate/len(laser_digitizer_ID)/383, writer='ffmpeg',codec='mpeg4')
	# plt.close('all')

	print('completed FAST count rotating/cropping ' + laser_to_analyse)

	averaged_params = np.mean(params,axis=(0))
	averaged_errparams = np.mean(errparams,axis=(0))
	counts = temp_counts_minus_background+temp_ref_counts
	temperature = averaged_params[:,:,-1] + averaged_params[:,:,-2] * counts + averaged_params[:,:,-3] * (counts**2)
	counts_std = estimate_counts_std(counts,int_time=laser_int_time/1000)
	temperature_std = (averaged_errparams[:,:,2,2] + (counts_std**2)*(averaged_params[:,:,1]**2) + (counts**2+counts_std**2)*averaged_errparams[:,:,1,1] + (counts_std**2)*(4*counts**2+3*counts_std**2)*(averaged_params[:,:,0]**2) + (counts**4+6*(counts**2)*(counts_std**2)+3*counts_std**4)*averaged_errparams[:,:,0,0] + 2*counts*averaged_errparams[:,:,2,1] + 2*(counts**2+counts_std**2)*averaged_errparams[:,:,2,0] + 2*(counts**3+counts*(counts_std**2))*averaged_errparams[:,:,1,0])**0.5

	temperature_ref = averaged_params[:,:,-1] + averaged_params[:,:,-2] * temp_ref_counts + averaged_params[:,:,-3] * (temp_ref_counts**2)

	# rotation and crop
	temperature_rot=rotate(temperature,foilrotdeg,axes=(-1,-2))
	temperature_std_rot=rotate(temperature_std,foilrotdeg,axes=(-1,-2))
	counts_rot=rotate(counts,foilrotdeg,axes=(-1,-2))
	if not (height==max_ROI[0][1]+1 and width==max_ROI[1][1]+1):
		temperature_rot*=out_of_ROI_mask
		temperature_rot[np.logical_and(temperature_rot<np.nanmin(temperature),temperature_rot>np.nanmax(temperature))]=0
		temperature_std_rot*=out_of_ROI_mask
		temperature_std_rot[np.logical_and(temperature_std_rot<np.nanmin(temperature),temperature_std_rot>np.nanmax(temperature))]=0
		counts_rot*=out_of_ROI_mask
		counts_rot[np.logical_and(counts_rot<np.nanmin(temperature),counts_rot>np.nanmax(temperature))]=0
	temperature_crop = temperature_rot[:,foildw:foilup,foillx:foilrx]
	temperature_std_crop = temperature_std_rot[:,foildw:foilup,foillx:foilrx]
	counts_crop = counts_rot[:,foildw:foilup,foillx:foilrx]

	# rotation and crop
	temperature_ref_rot=rotate(temperature_ref,foilrotdeg,axes=(-1,-2))
	if not (height==max_ROI[0][1]+1 and width==max_ROI[1][1]+1):
		temperature_ref_rot*=out_of_ROI_mask
		temperature_ref_rot[np.logical_and(temperature_ref_rot<np.nanmin(temperature_ref),temperature_ref_rot>np.nanmax(temperature_ref))]=0
	temperature_ref_crop = temperature_ref_rot[foildw:foilup,foillx:foilrx]

	temperature_minus_background_crop = temperature_crop-temperature_ref_crop

	shrink_factor_t = int(round(frames_to_average))
	shrink_factor_x = 3	# with large time averaging this should be enough
	binning_type = 'bin' + str(shrink_factor_t) + 'x' + str(shrink_factor_x) + 'x' + str(shrink_factor_x)

	FAST_counts_minus_background_crop_binned,nan_ROI_mask = proper_homo_binning_t_2D(FAST_counts_minus_background_crop,shrink_factor_t,shrink_factor_x)
	temperature_crop_binned,nan_ROI_mask = proper_homo_binning_t_2D(temperature_crop,shrink_factor_t,shrink_factor_x)
	temperature_std_crop_binned = 1/(shrink_factor_t*shrink_factor_x**2)*(proper_homo_binning_t_2D(temperature_std_crop**2,shrink_factor_t,shrink_factor_x,type='np.nansum')[0]**0.5)
	counts_crop_binned,trash = proper_homo_binning_t_2D(counts_crop,shrink_factor_t,shrink_factor_x)
	temperature_minus_background_crop_binned,nan_ROI_mask = proper_homo_binning_t_2D(temperature_minus_background_crop,shrink_factor_t,shrink_factor_x)
	temperature_ref_crop_binned = proper_homo_binning_2D(temperature_ref_crop,shrink_factor_x)
	time_binned = proper_homo_binning_t(FAST_counts_minus_background_crop_time,shrink_factor_t)

	averaged_params = np.mean(averaged_params,axis=(0,1))
	averaged_errparams = np.mean(averaged_errparams,axis=(0,1))

	# reference foil properties
	# thickness = 1.4859095354482858e-06
	# emissivity = 0.9884061389741369
	# diffusivity = 1.045900223180454e-05
	# from 2021/09/17, Laser_data_analysis3_3.py
	thickness = 2.0531473351462095e-06
	emissivity = 0.9999999999999
	diffusivity = 1.0283685197530968e-05
	Ptthermalconductivity=71.6 #[W/(m·K)]
	zeroC=273.15 #K / C
	sigmaSB=5.6704e-08 #[W/(m2 K4)]

	foilemissivityscaled=emissivity*np.ones(np.array(temperature_ref_crop_binned.shape)-2)
	foilthicknessscaled=thickness*np.ones(np.array(temperature_ref_crop_binned.shape)-2)
	conductivityscaled=Ptthermalconductivity*np.ones(np.array(temperature_ref_crop_binned.shape)-2)
	reciprdiffusivityscaled=(1/diffusivity)*np.ones(np.array(temperature_ref_crop_binned.shape)-2)

	dt = time_binned[2:]-time_binned[:-2]
	dx=foil_position_dict['foilhorizw']/foil_position_dict['foilhorizwpixel']*shrink_factor_x
	dTdt,dTdt_std,d2Tdxy,d2Tdxy_std,negd2Tdxy,negd2Tdxy_std,T4_T04,T4_T04_std = calc_temp_to_power_1(dx,dt,averaged_params,counts_crop_binned,averaged_errparams,shrink_factor_t,shrink_factor_x,temperature_std_crop_binned,temperature_minus_background_crop_binned,laser_int_time,nan_ROI_mask,temperature_ref_crop_binned)
	# dTdt=np.divide((temperature_crop_binned[2:,1:-1,1:-1]-temperature_crop_binned[:-2,1:-1,1:-1]).T,dt).T.astype(np.float32)
	# temp = averaged_errparams[2,2] + counts_crop_binned[2:,1:-1,1:-1]*counts_crop_binned[:-2,1:-1,1:-1]*averaged_errparams[1,1] + (counts_crop_binned[2:,1:-1,1:-1]**2)*(counts_crop_binned[:-2,1:-1,1:-1]**2)*averaged_errparams[0,0] + (estimate_counts_std(counts_crop_binned[2:,1:-1,1:-1])**2)*(estimate_counts_std(counts_crop_binned[:-2,1:-1,1:-1])**2)*(averaged_params[0]**2)/((shrink_factor_t*shrink_factor_x**2)**2) + (counts_crop_binned[2:,1:-1,1:-1]+counts_crop_binned[:-2,1:-1,1:-1])*averaged_errparams[1,2] + (counts_crop_binned[2:,1:-1,1:-1]**2 + counts_crop_binned[:-2,1:-1,1:-1]**2)*averaged_errparams[0,2] + (counts_crop_binned[2:,1:-1,1:-1]*(counts_crop_binned[:-2,1:-1,1:-1]**2)+counts_crop_binned[:-2,1:-1,1:-1]*(counts_crop_binned[2:,1:-1,1:-1]**2))*averaged_errparams[0,1]
	# dTdt_std=np.divide((temperature_std_crop_binned[2:,1:-1,1:-1]**2 + temperature_std_crop_binned[:-2,1:-1,1:-1]**2 - 2*temp).T**0.5,dt).T.astype(np.float32)
	# d2Tdx2=np.divide(temperature_minus_background_crop_binned[1:-1,1:-1,2:]-np.multiply(2,temperature_minus_background_crop_binned[1:-1,1:-1,1:-1])+temperature_minus_background_crop_binned[1:-1,1:-1,:-2],dx**2).astype(np.float32)
	# d2Tdy2=np.divide(temperature_minus_background_crop_binned[1:-1,2:,1:-1]-np.multiply(2,temperature_minus_background_crop_binned[1:-1,1:-1,1:-1])+temperature_minus_background_crop_binned[1:-1,:-2,1:-1],dx**2).astype(np.float32)
	# d2Tdxy = np.ones_like(dTdt).astype(np.float32)*np.nan
	# d2Tdxy[:,nan_ROI_mask[1:-1,1:-1]]=np.add(d2Tdx2[:,nan_ROI_mask[1:-1,1:-1]],d2Tdy2[:,nan_ROI_mask[1:-1,1:-1]])
	# del d2Tdx2,d2Tdy2
	# d2Tdx2_std=np.divide((temperature_std_crop_binned[1:-1,1:-1,2:]**2+np.multiply(2**2,temperature_std_crop_binned[1:-1,1:-1,1:-1])**2+temperature_std_crop_binned[1:-1,1:-1,:-2]**2)**0.5,dx**2).astype(np.float32)
	# d2Tdy2_std=np.divide((temperature_std_crop_binned[1:-1,2:,1:-1]**2+np.multiply(2**2,temperature_std_crop_binned[1:-1,1:-1,1:-1])**2+temperature_std_crop_binned[1:-1,:-2,1:-1]**2)**0.5,dx**2).astype(np.float32)
	# d2Tdxy_std = np.ones_like(dTdt).astype(np.float32)*np.nan
	# temp1 = averaged_errparams[2,2] + counts_crop_binned[1:-1,1:-1,2:]*counts_crop_binned[1:-1,1:-1,:-2]*averaged_errparams[1,1] + (counts_crop_binned[1:-1,1:-1,2:]**2)*(counts_crop_binned[1:-1,1:-1,:-2]**2)*averaged_errparams[0,0] + (estimate_counts_std(counts_crop_binned[1:-1,1:-1,2:],int_time=laser_int_time/1000)**2)*(estimate_counts_std(counts_crop_binned[1:-1,1:-1,:-2],int_time=laser_int_time/1000)**2)*(averaged_params[0]**2)/((shrink_factor_t*shrink_factor_x**2)**2) + (counts_crop_binned[1:-1,1:-1,2:]+counts_crop_binned[1:-1,1:-1,:-2])*averaged_errparams[1,2] + (counts_crop_binned[1:-1,1:-1,2:]**2+counts_crop_binned[1:-1,1:-1,:-2]**2)*averaged_errparams[0,2] + (counts_crop_binned[1:-1,1:-1,2:]*(counts_crop_binned[1:-1,1:-1,:-2]**2)+counts_crop_binned[1:-1,1:-1,:-2]*(counts_crop_binned[1:-1,1:-1,2:]**2))*averaged_errparams[0,1]
	# temp2 = averaged_errparams[2,2] + counts_crop_binned[1:-1,1:-1,2:]*counts_crop_binned[1:-1,1:-1,1:-1]*averaged_errparams[1,1] + (counts_crop_binned[1:-1,1:-1,2:]**2)*(counts_crop_binned[1:-1,1:-1,1:-1]**2)*averaged_errparams[0,0] + (estimate_counts_std(counts_crop_binned[1:-1,1:-1,2:],int_time=laser_int_time/1000)**2)*(estimate_counts_std(counts_crop_binned[1:-1,1:-1,1:-1],int_time=laser_int_time/1000)**2)*(averaged_params[0]**2)/((shrink_factor_t*shrink_factor_x**2)**2) + (counts_crop_binned[1:-1,1:-1,2:]+counts_crop_binned[1:-1,1:-1,1:-1])*averaged_errparams[1,2] + (counts_crop_binned[1:-1,1:-1,2:]**2+counts_crop_binned[1:-1,1:-1,1:-1]**2)*averaged_errparams[0,2] + (counts_crop_binned[1:-1,1:-1,2:]*(counts_crop_binned[1:-1,1:-1,1:-1]**2)+counts_crop_binned[1:-1,1:-1,1:-1]*(counts_crop_binned[1:-1,1:-1,2:]**2))*averaged_errparams[0,1]
	# temp3 = averaged_errparams[2,2] + counts_crop_binned[1:-1,1:-1,1:-1]*counts_crop_binned[1:-1,1:-1,:-2]*averaged_errparams[1,1] + (counts_crop_binned[1:-1,1:-1,1:-1]**2)*(counts_crop_binned[1:-1,1:-1,:-2]**2)*averaged_errparams[0,0] + (estimate_counts_std(counts_crop_binned[1:-1,1:-1,1:-1],int_time=laser_int_time/1000)**2)*(estimate_counts_std(counts_crop_binned[1:-1,1:-1,:-2],int_time=laser_int_time/1000)**2)*(averaged_params[0]**2)/((shrink_factor_t*shrink_factor_x**2)**2) + (counts_crop_binned[1:-1,1:-1,1:-1]+counts_crop_binned[1:-1,1:-1,:-2])*averaged_errparams[1,2] + (counts_crop_binned[1:-1,1:-1,1:-1]**2+counts_crop_binned[1:-1,1:-1,:-2]**2)*averaged_errparams[0,2] + (counts_crop_binned[1:-1,1:-1,1:-1]*(counts_crop_binned[1:-1,1:-1,:-2]**2)+counts_crop_binned[1:-1,1:-1,:-2]*(counts_crop_binned[1:-1,1:-1,1:-1]**2))*averaged_errparams[0,1]
	# temp = 2*temp1-4*temp2-4*temp3
	# temp1 = averaged_errparams[2,2] + counts_crop_binned[1:-1,2:,1:-1]*counts_crop_binned[1:-1,:-2,1:-1]*averaged_errparams[1,1] + (counts_crop_binned[1:-1,2:,1:-1]**2)*(counts_crop_binned[1:-1,:-2,1:-1]**2)*averaged_errparams[0,0] + (estimate_counts_std(counts_crop_binned[1:-1,2:,1:-1],int_time=laser_int_time/1000)**2)*(estimate_counts_std(counts_crop_binned[1:-1,:-2,1:-1],int_time=laser_int_time/1000)**2)*(averaged_params[0]**2)/((shrink_factor_t*shrink_factor_x**2)**2) + (counts_crop_binned[1:-1,2:,1:-1]+counts_crop_binned[1:-1,:-2,1:-1])*averaged_errparams[1,2] + (counts_crop_binned[1:-1,2:,1:-1]**2+counts_crop_binned[1:-1,:-2,1:-1]**2)*averaged_errparams[0,2] + (counts_crop_binned[1:-1,2:,1:-1]*(counts_crop_binned[1:-1,:-2,1:-1]**2)+counts_crop_binned[1:-1,:-2,1:-1]*(counts_crop_binned[1:-1,2:,1:-1]**2))*averaged_errparams[0,1]
	# temp2 = averaged_errparams[2,2] + counts_crop_binned[1:-1,2:,1:-1]*counts_crop_binned[1:-1,1:-1,1:-1]*averaged_errparams[1,1] + (counts_crop_binned[1:-1,2:,1:-1]**2)*(counts_crop_binned[1:-1,1:-1,1:-1]**2)*averaged_errparams[0,0] + (estimate_counts_std(counts_crop_binned[1:-1,2:,1:-1],int_time=laser_int_time/1000)**2)*(estimate_counts_std(counts_crop_binned[1:-1,1:-1,1:-1],int_time=laser_int_time/1000)**2)*(averaged_params[0]**2)/((shrink_factor_t*shrink_factor_x**2)**2) + (counts_crop_binned[1:-1,2:,1:-1]+counts_crop_binned[1:-1,1:-1,1:-1])*averaged_errparams[1,2] + (counts_crop_binned[1:-1,2:,1:-1]**2+counts_crop_binned[1:-1,1:-1,1:-1]**2)*averaged_errparams[0,2] + (counts_crop_binned[1:-1,2:,1:-1]*(counts_crop_binned[1:-1,1:-1,1:-1]**2)+counts_crop_binned[1:-1,1:-1,1:-1]*(counts_crop_binned[1:-1,2:,1:-1]**2))*averaged_errparams[0,1]
	# temp3 = averaged_errparams[2,2] + counts_crop_binned[1:-1,1:-1,1:-1]*counts_crop_binned[1:-1,:-2,1:-1]*averaged_errparams[1,1] + (counts_crop_binned[1:-1,1:-1,1:-1]**2)*(counts_crop_binned[1:-1,:-2,1:-1]**2)*averaged_errparams[0,0] + (estimate_counts_std(counts_crop_binned[1:-1,1:-1,1:-1],int_time=laser_int_time/1000)**2)*(estimate_counts_std(counts_crop_binned[1:-1,:-2,1:-1],int_time=laser_int_time/1000)**2)*(averaged_params[0]**2)/((shrink_factor_t*shrink_factor_x**2)**2) + (counts_crop_binned[1:-1,1:-1,1:-1]+counts_crop_binned[1:-1,:-2,1:-1])*averaged_errparams[1,2] + (counts_crop_binned[1:-1,1:-1,1:-1]**2+counts_crop_binned[1:-1,:-2,1:-1]**2)*averaged_errparams[0,2] + (counts_crop_binned[1:-1,1:-1,1:-1]*(counts_crop_binned[1:-1,:-2,1:-1]**2)+counts_crop_binned[1:-1,:-2,1:-1]*(counts_crop_binned[1:-1,1:-1,1:-1]**2))*averaged_errparams[0,1]
	# temp += 2*temp1-4*temp2-4*temp3
	# d2Tdxy_std[:,nan_ROI_mask[1:-1,1:-1]]=np.add(temp[:,nan_ROI_mask[1:-1,1:-1]]/(dx**4),np.add(d2Tdx2_std[:,nan_ROI_mask[1:-1,1:-1]]**2,d2Tdy2_std[:,nan_ROI_mask[1:-1,1:-1]]**2))**0.5
	# del d2Tdx2_std,d2Tdy2_std
	# negd2Tdxy=np.multiply(-1,d2Tdxy)
	# negd2Tdxy_std=d2Tdxy_std
	# T4=(temperature_minus_background_crop_binned[1:-1,1:-1,1:-1]+np.nanmean(temperature_ref_crop_binned)+zeroC)**4
	# T04=(np.nanmean(temperature_ref_crop_binned)+zeroC)**4 *np.ones_like(temperature_minus_background_crop_binned[1:-1,1:-1,1:-1])
	# T4_T04 = np.ones_like(dTdt).astype(np.float32)*np.nan
	# T4_T04[:,nan_ROI_mask[1:-1,1:-1]] = (T4[:,nan_ROI_mask[1:-1,1:-1]]-T04[:,nan_ROI_mask[1:-1,1:-1]]).astype(np.float32)
	# T4_std=T4**(3/4) *4 *temperature_std_crop_binned[1:-1,1:-1,1:-1]	# the error resulting from doing the average on the whole ROI is completely negligible
	# T04_std=0
	# T4_T04_std = np.ones_like(dTdt).astype(np.float32)*np.nan
	# T4_T04_std[:,nan_ROI_mask[1:-1,1:-1]] = ((T4_std[:,nan_ROI_mask[1:-1,1:-1]]**2+T04_std**2)**0.5).astype(np.float32)

	BBrad,diffusion,timevariation,powernoback,BBrad_std,diffusion_std,timevariation_std,powernoback_std = calc_temp_to_power_BB_2(dTdt,dTdt_std,d2Tdxy,d2Tdxy_std,negd2Tdxy,negd2Tdxy_std,T4_T04,T4_T04_std,nan_ROI_mask,foilemissivityscaled,foilthicknessscaled,reciprdiffusivityscaled,Ptthermalconductivity)
	# BBrad = np.ones_like(dTdt).astype(np.float32)*np.nan
	# BBrad[:,nan_ROI_mask[1:-1,1:-1]] = (2*sigmaSB*T4_T04[:,nan_ROI_mask[1:-1,1:-1]] * foilemissivityscaled[nan_ROI_mask[1:-1,1:-1]]).astype(np.float32)
	# diffusion = np.ones_like(dTdt).astype(np.float32)*np.nan
	# diffusion[:,nan_ROI_mask[1:-1,1:-1]] = (Ptthermalconductivity*negd2Tdxy[:,nan_ROI_mask[1:-1,1:-1]]*foilthicknessscaled[nan_ROI_mask[1:-1,1:-1]]).astype(np.float32)
	# timevariation = np.ones_like(dTdt).astype(np.float32)*np.nan
	# timevariation[:,nan_ROI_mask[1:-1,1:-1]] = (Ptthermalconductivity*dTdt[:,nan_ROI_mask[1:-1,1:-1]]*foilthicknessscaled[nan_ROI_mask[1:-1,1:-1]]*reciprdiffusivityscaled[nan_ROI_mask[1:-1,1:-1]]).astype(np.float32)
	# powernoback = (diffusion + timevariation + BBrad).astype(np.float32)
	# BBrad_std = np.ones_like(dTdt).astype(np.float32)*np.nan
	# BBrad_std[:,nan_ROI_mask[1:-1,1:-1]] = (2*sigmaSB*T4_T04_std[:,nan_ROI_mask[1:-1,1:-1]]*foilemissivityscaled[nan_ROI_mask[1:-1,1:-1]]).astype(np.float32)
	# diffusion_std = np.ones_like(dTdt).astype(np.float32)*np.nan
	# diffusion_std[:,nan_ROI_mask[1:-1,1:-1]] = (Ptthermalconductivity*negd2Tdxy_std[:,nan_ROI_mask[1:-1,1:-1]]*foilthicknessscaled[nan_ROI_mask[1:-1,1:-1]]).astype(np.float32)
	# timevariation_std = np.ones_like(dTdt).astype(np.float32)*np.nan
	# timevariation_std[:,nan_ROI_mask[1:-1,1:-1]] = (Ptthermalconductivity*dTdt_std[:,nan_ROI_mask[1:-1,1:-1]]*foilthicknessscaled[nan_ROI_mask[1:-1,1:-1]]*reciprdiffusivityscaled[nan_ROI_mask[1:-1,1:-1]]).astype(np.float32)
	# powernoback_std = np.ones_like(powernoback)*np.nan
	# powernoback_std[:,nan_ROI_mask[1:-1,1:-1]] = ((diffusion_std[:,nan_ROI_mask[1:-1,1:-1]]**2 + timevariation_std[:,nan_ROI_mask[1:-1,1:-1]]**2 + BBrad_std[:,nan_ROI_mask[1:-1,1:-1]]**2)**0.5).astype(np.float32)

	horizontal_coord = np.arange(np.shape(powernoback[0])[1])
	vertical_coord = np.arange(np.shape(powernoback[0])[0])
	horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
	horizontal_coord = (horizontal_coord+1+0.5)*dx	# +1 because in the process of calculating the power I eliminate the first and last pixel in spatial coordinates, +0.5 do be the centre of the pixel
	vertical_coord = (vertical_coord+1+0.5)*dx
	horizontal_coord -= foil_position_dict['foilhorizw']*0.5+0.0198
	vertical_coord -= foil_position_dict['foilvertw']*0.5-0.0198
	distance_from_vertical = (horizontal_coord**2+vertical_coord**2)**0.5
	pinhole_to_foil_vertical = 0.008 + 0.003 + 0.002 + 0.045	# pinhole holder, washer, foil holder, standoff
	pinhole_to_pixel_distance = (pinhole_to_foil_vertical**2 + distance_from_vertical**2)**0.5

	etendue = np.ones_like(powernoback[0]) * (np.pi*(0.002**2)) / (pinhole_to_pixel_distance**2)	# I should include also the area of the pixel, but that is already in the w/m2 power
	etendue *= (pinhole_to_foil_vertical/pinhole_to_pixel_distance)**2	 # cos(a)*cos(b). for pixels not directly under the pinhole both pinhole and pixel are tilted respect to the vertical, with same angle.
	brightness = 4*np.pi*powernoback/etendue

	temp = brightness[:,:,:int(np.shape(brightness)[2]*0.75)]
	temp = np.sort(temp[np.max(temp,axis=(1,2)).argmax()].flatten())
	ani,efit_reconstruction = movie_from_data(np.array([np.flip(np.transpose(brightness,(0,2,1)),axis=2)]), 1/np.median(np.diff(time_binned)),timesteps=time_binned[1:-1],integration=laser_int_time/1000,time_offset=time_binned[0],extvmin=0,xlabel='horizontal coord [pixels]', ylabel='vertical coord [pixels]',barlabel='brightness [W/m2]', prelude='shot ' + laser_to_analyse[-9:-4]+'\n'+binning_type+'\n',overlay_structure=True,include_EFIT=True,pulse_ID=laser_to_analyse[-9:-4],overlay_x_point=True,overlay_mag_axis=True,overlay_strike_points=True,overlay_separatrix=True,EFIT_output_requested=True)
	ani.save('/home/ffederic/work/irvb/MAST-U/FAST_results/'+os.path.split(laser_to_analyse[:-4])[1]+ '_FAST_brightness.mp4', fps=5*(1/np.mean(np.diff(time_binned)))/383, writer='ffmpeg',codec='mpeg4')
	plt.close('all')
	powernoback_output = cp.deepcopy(powernoback)

	print('completed FAST power calculation ' + laser_to_analyse)

	inverted_dict = dict([])
	from scipy.ndimage import geometric_transform
	import time as tm
	import pickle
	from shapely.geometry import Point
	from shapely.geometry.polygon import Polygon
	shot_number = int(laser_to_analyse[-9:-4])
	for grid_resolution in [4, 2]:
	# for grid_resolution in [4]:
		inverted_dict[str(grid_resolution)] = dict([])
		# grid_resolution = 8  # in cm
		foil_resolution = '187'

		foil_res = '_foil_pixel_h_' + str(foil_resolution)

		grid_type = 'core_res_' + str(grid_resolution) + 'cm'
		path_sensitivity = '/home/ffederic/work/analysis_scripts/sensitivity_matrix_' + grid_type[5:] + foil_res + '_power'
		try:
			sensitivities = np.array((scipy.sparse.load_npz(path_sensitivity + '/sensitivity.npz')).todense())
		except:
			sensitivities = np.load(path_sensitivity + '/sensitivity.npy')

		filenames = all_file_names(path_sensitivity, '.csv')[0]
		with open(os.path.join(path_sensitivity, filenames)) as csv_file:
			csv_reader = csv.reader(csv_file, delimiter=',')
			for row in csv_reader:
				if row[0] == 'foil vertical pixels ':
					pixel_v = int(row[1])
				if row[0] == 'foil horizontal pixels ':
					pixel_h = int(row[1])
				if row[0] == 'pipeline type ':
					pipeline = row[1]
				if row[0] == 'type of volume grid ':
					grid_type = row[1]
			# print(row)

		directory = '/home/ffederic/work/cherab/cherab_mastu/cherab/mastu/bolometry/grid_construction'
		grid_file = os.path.join(directory,'{}_rectilinear_grid.pickle'.format(grid_type))
		with open(grid_file, 'rb') as f:
			grid_data_all = pickle.load(f)
		grid_laplacian = grid_data_all['laplacian']
		grid_mask = grid_data_all['mask']
		grid_data = grid_data_all['voxels']
		grid_index_2D_to_1D_map = grid_data_all['index_2D_to_1D_map']
		grid_index_1D_to_2D_map = grid_data_all['index_1D_to_2D_map']

		sensitivities_reshaped = sensitivities.reshape((pixel_v,pixel_h,len(grid_laplacian)))
		sensitivities_reshaped = np.transpose(sensitivities_reshaped , (1,0,2))

		if grid_resolution==8:
			# temp=1e-3
			temp=1e-7
		elif grid_resolution==2:
			temp=1e-4
		elif grid_resolution==4:
			temp=0
		sensitivities_reshaped_masked,grid_laplacian_masked,grid_data_masked,grid_Z_derivate_masked,grid_R_derivate_masked = reduce_voxels(sensitivities_reshaped,grid_laplacian,grid_data,std_treshold = temp,chop_top_corner = False,chop_corner_close_to_baffle = False, core_radious_treshold = 1.9,extra_chop_top_corner=False)

		# this step is to adapt the matrix to the size of the foil I measure, that can be slightly different
		binning_type = 'bin' + str(shrink_factor_t) + 'x' + str(shrink_factor_x) + 'x' + str(shrink_factor_x)
		shape = list(FAST_counts_minus_background_crop.shape[1:])
		if shape!=list(sensitivities_reshaped_masked.shape[:-1]):
			shape.extend([len(grid_laplacian_masked)])
			def mapping(output_coords):
				return(output_coords[0]/shape[0]*pixel_h,output_coords[1]/shape[1]*pixel_v,output_coords[2])
			sensitivities_reshaped_masked2 = geometric_transform(sensitivities_reshaped_masked,mapping,output_shape=shape)
		else:
			sensitivities_reshaped_masked2 = cp.deepcopy(sensitivities_reshaped_masked)

		sensitivities_binned = proper_homo_binning_1D_1D_1D(sensitivities_reshaped_masked2,shrink_factor_x,shrink_factor_x,1,type='np.nanmean')
		sensitivities_binned = sensitivities_binned[1:-1,1:-1]	# i need to remove 2 pixels per coordinate because this is done to calculate the lalacian
		sensitivities_binned = np.flip(sensitivities_binned,axis=1)	# it turns ou that I need to flip it

		# additional cropping of the foil to exlude regions without plasma LOS, the frame of the foil and gas puff
		# ROI = np.array([[0.2,0.85],[0.1,0.9]])
		# ROI = np.array([[0.05,0.95],[0.05,0.95]])
		# ROI = np.array([[0.2,0.95],[0.1,1]])
		ROI1 = np.array([[0.03,0.80],[0.03,0.85]])
		ROI2 = np.array([[0.03,0.7],[0.03,0.91]])
		ROI_beams = np.array([[0.,0.3],[0.5,1]])
		sensitivities_binned_crop,selected_ROI = cut_sensitivity_matrix_based_on_foil_anysotropy(sensitivities_binned,ROI1,ROI2,ROI_beams,laser_to_analyse)

		select_foil_region_with_plasma = (np.sum(sensitivities_binned_crop,axis=-1)>1e-3)
		selected_ROI_no_plasma = np.logical_and(selected_ROI,np.logical_not(select_foil_region_with_plasma))
		select_foil_region_with_plasma = select_foil_region_with_plasma.flatten()

		if grid_resolution==8:
			# temp=1e-3
			temp=1e-7
		elif grid_resolution==2:
			temp=1e-4
		elif grid_resolution==4:
			temp=0
		sensitivities_binned_crop,grid_laplacian_masked_crop,grid_data_masked_crop,grid_Z_derivate_masked_crop,grid_R_derivate_masked_crop = reduce_voxels(sensitivities_binned_crop,grid_laplacian_masked,grid_data_masked,std_treshold = temp)

		selected_super_x_cells = np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,0]>0.85,np.mean(grid_data_masked_crop,axis=1)[:,1]<-1.65)

		x1 = [1.55,0.25]	# r,z
		x2 = [1.1,-0.15]
		interp = interp1d([x1[0],x2[0]],[x1[1],x2[1]],fill_value="extrapolate",bounds_error=False)
		select = np.mean(grid_data_masked_crop,axis=1)[:,1]>interp(np.mean(grid_data_masked_crop,axis=1)[:,0])
		selected_central_border_cells = np.logical_and(select,np.logical_and(np.max(grid_Z_derivate_masked_crop,axis=(1))==1,np.mean(grid_data_masked_crop,axis=1)[:,1]>-0.5))
		selected_central_border_cells = np.dot(grid_laplacian_masked_crop,selected_central_border_cells*np.random.random(selected_central_border_cells.shape))!=0

		selected_central_column_border_cells = np.logical_and(np.logical_and(np.max(grid_R_derivate_masked_crop,axis=(1))==1,np.mean(grid_data_masked_crop,axis=1)[:,0]<0.7),np.mean(grid_data_masked_crop,axis=1)[:,1]<-0.7)
		selected_central_column_border_cells = np.logical_and(np.logical_and(np.dot(grid_laplacian_masked_crop,selected_central_column_border_cells*np.random.random(selected_central_column_border_cells.shape))!=0,np.mean(grid_data_masked_crop,axis=1)[:,0]<0.7),np.mean(grid_data_masked_crop,axis=1)[:,1]<-0.7)

		selected_edge_cells = np.logical_and(np.logical_and(np.max(grid_laplacian_masked_crop,axis=(0))<=6,np.mean(grid_data_masked_crop,axis=1)[:,0]>1.35),np.mean(grid_data_masked_crop,axis=1)[:,1]>-1.1)
		selected_edge_cells = np.logical_or(selected_edge_cells,np.logical_and(np.logical_and(np.logical_and(np.max(grid_laplacian_masked_crop,axis=(0))<=6,np.mean(grid_data_masked_crop,axis=1)[:,0]>1.05),np.mean(grid_data_masked_crop,axis=1)[:,1]>-1.5),np.mean(grid_data_masked_crop,axis=1)[:,1]<-0.5))

		selected_edge_cells_for_laplacian = np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,0]>1.05,np.dot(grid_laplacian_masked_crop,selected_edge_cells*np.random.random(selected_edge_cells.shape))!=0)
		if grid_resolution<8:
			selected_edge_cells_for_laplacian = np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,0]>1.05,np.dot(grid_laplacian_masked_crop,selected_edge_cells_for_laplacian*np.random.random(selected_edge_cells_for_laplacian.shape))!=0)
		if grid_resolution<4:
			selected_edge_cells_for_laplacian = np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,0]>1.05,np.dot(grid_laplacian_masked_crop,selected_edge_cells_for_laplacian*np.random.random(selected_edge_cells_for_laplacian.shape))!=0)

		sensitivities_binned_crop_shape = sensitivities_binned_crop.shape
		sensitivities_binned_crop = sensitivities_binned_crop.reshape((sensitivities_binned_crop.shape[0]*sensitivities_binned_crop.shape[1],sensitivities_binned_crop.shape[2]))

		if shrink_factor_x > 1:
			foil_resolution = str(shrink_factor_x) + 'x' + str(shrink_factor_x)
		else:
			foil_resolution = str(shape[0])

		foil_res = '_foil_pixel_h_' + str(foil_resolution)
		path_sensitivity = '/home/ffederic/work/analysis_scripts/sensitivity_matrix_'+grid_type[5:]+foil_res+'_power'
		path_sensitivity_original = cp.deepcopy(path_sensitivity)

		binning_type = 'bin' + str(shrink_factor_t) + 'x' + str(shrink_factor_x) + 'x' + str(shrink_factor_x)
		print('starting '+binning_type)
		# powernoback_full = saved_file_dict_short[binning_type].all()['powernoback_full']
		# powernoback_std_full = saved_file_dict_short[binning_type].all()['powernoback_std_full']

		# from here I make the new method.
		# I consider the nominal properties as central value, with:
		# emissivity -10% (from Japanese properties i have std of ~5%, but my nominal value is ~1 and emissivity cannot be >1 so I double the interval down)
		# thickness +/-15% (from Japanese properties i have std of ~15%)
		# diffusivity -10% (this is missing from the Japanese data, so I guess std ~10%)

		emissivity_steps = 5
		thickness_steps = 9
		rec_diffusivity_steps = 9
		sigma_emissivity = 0.1
		sigma_thickness = 0.15
		sigma_rec_diffusivity = 0.1
		emissivity_array = np.linspace(1-sigma_emissivity*3,1,num=emissivity_steps)
		emissivity_log_prob =  -(0.5*(((1-emissivity_array)/sigma_emissivity)**2))**1	# super gaussian order 1, probability assigned linearly
		emissivity_log_prob = emissivity_log_prob -np.log(np.trapz(np.exp(emissivity_log_prob),x=emissivity_array))	# normalisation for logarithmic probabilities
		thickness_array = np.linspace(1-sigma_thickness*3,1+sigma_thickness*3,num=thickness_steps)
		thickness_log_prob =  -(0.5*(((1-thickness_array)/sigma_thickness)**2))**1	# super gaussian order 1, probability assigned linearly
		thickness_log_prob = thickness_log_prob -np.log(np.trapz(np.exp(thickness_log_prob),x=thickness_array))	# normalisation for logarithmic probabilities
		rec_diffusivity = 1/diffusivity
		rec_diffusivity_array = np.linspace(1-sigma_rec_diffusivity*3,1+sigma_rec_diffusivity*3,num=rec_diffusivity_steps)
		rec_diffusivity_log_prob =  -(0.5*(((1-rec_diffusivity_array)/sigma_rec_diffusivity)**2))**1	# super gaussian order 1, probability assigned linearly
		rec_diffusivity_log_prob = rec_diffusivity_log_prob -np.log(np.trapz(np.exp(rec_diffusivity_log_prob),x=rec_diffusivity_array))	# normalisation for logarithmic probabilities

		tend = get_tend(laser_to_analyse[-9:-4])+0.01	 # I add 10ms just for safety and to catch disruptions

		time_full_binned = time_binned[1:-1]
		BBrad_full_crop = BBrad[time_full_binned<tend]
		BBrad_full_crop[:,np.logical_not(selected_ROI)] = 0
		BBrad_std_full_crop = BBrad_std[time_full_binned<tend]
		BBrad_std_full_crop[:,np.logical_not(selected_ROI)] = 0
		diffusion_full_crop = diffusion[time_full_binned<tend]
		diffusion_full_crop[:,np.logical_not(selected_ROI)] = 0
		diffusion_std_full_crop = diffusion_std[time_full_binned<tend]
		diffusion_std_full_crop[:,np.logical_not(selected_ROI)] = 0
		timevariation_full_crop = timevariation[time_full_binned<tend]
		timevariation_full_crop[:,np.logical_not(selected_ROI)] = 0
		timevariation_std_full_crop = timevariation_std[time_full_binned<tend]
		timevariation_std_full_crop[:,np.logical_not(selected_ROI)] = 0
		time_full_binned_crop = time_full_binned[time_full_binned<tend]

		powernoback_full = (np.array([[[BBrad_full_crop[0].tolist()]*rec_diffusivity_steps]*thickness_steps]*emissivity_steps).T*emissivity_array).T	# emissivity, thickness, rec_diffusivity
		powernoback_full += (np.array([(np.array([[diffusion_full_crop[0].tolist()]*rec_diffusivity_steps]*thickness_steps).T*thickness_array).T.tolist()]*emissivity_steps))
		powernoback_full += (np.array([(np.array([(np.array([timevariation_full_crop[0].tolist()]*rec_diffusivity_steps).T*rec_diffusivity_array).T.tolist()]*thickness_steps).T*thickness_array).T.tolist()]*emissivity_steps))

		alpha = 1e-4
		A_ = np.dot(sensitivities_binned_crop.T, sensitivities_binned_crop) + (alpha**2) * np.dot(grid_laplacian_masked_crop.T, grid_laplacian_masked_crop)
		d=powernoback_full.reshape(emissivity_steps*thickness_steps*rec_diffusivity_steps,powernoback_full.shape[-2]*powernoback_full.shape[-1])
		b_ = np.dot(sensitivities_binned_crop.T,d.T)

		U, s, Vh = np.linalg.svd(A_)
		sigma = np.diag(s)
		inv_sigma = np.diag(1 / s)
		a1 = np.dot(U, np.dot(sigma, Vh))
		a1_inv = np.dot(Vh.T, np.dot(inv_sigma, U.T))
		m = np.dot(a1_inv, b_).T
		neg_m_penalty = np.zeros_like(m)
		neg_m_penalty[m<0] = m[m<0]
		# neg_d_penalty = np.dot(sensitivities_binned_crop,neg_m_penalty.T).T
		neg_m_penalty = -20*neg_m_penalty/np.median(np.flip(np.sort(m[m<0]),axis=0)[-np.sum(m<0)//10:])
		neg_m_penalty = neg_m_penalty.reshape((*powernoback_full.shape[:-2],neg_m_penalty.shape[-1]))

		edge_penalty = np.zeros_like(m)
		edge_penalty[:,selected_edge_cells] = np.max(m[:,selected_edge_cells],0)
		edge_penalty = -50*edge_penalty/np.median(np.sort(edge_penalty[edge_penalty>0])[-np.sum(edge_penalty>0)//10:])
		edge_penalty = edge_penalty.reshape((*powernoback_full.shape[:-2],edge_penalty.shape[-1]))

		if True:	# if I want to bypass this penalty
			neg_powernoback_full_penalty = np.zeros_like(powernoback_full)	# emissivity, thickness, rec_diffusivity
			neg_powernoback_full_penalty[powernoback_full<0] = powernoback_full[powernoback_full<0]
			neg_powernoback_full_penalty = neg_powernoback_full_penalty.reshape((emissivity_steps*thickness_steps*rec_diffusivity_steps,neg_powernoback_full_penalty.shape[-2]*neg_powernoback_full_penalty.shape[-1]))
			neg_powernoback_full_penalty = np.dot(a1_inv, np.dot(sensitivities_binned_crop.T,neg_powernoback_full_penalty.T)).T
			neg_powernoback_full_penalty -= np.max(neg_powernoback_full_penalty)
			# neg_powernoback_full_penalty[neg_powernoback_full_penalty<0] = -10*neg_powernoback_full_penalty[neg_powernoback_full_penalty<0]/np.min(neg_powernoback_full_penalty[neg_powernoback_full_penalty<0])
			if neg_powernoback_full_penalty.min()<0:
				neg_powernoback_full_penalty = -20*neg_powernoback_full_penalty/np.median(np.flip(np.sort(neg_powernoback_full_penalty[neg_powernoback_full_penalty<0]),axis=0)[-np.sum(neg_powernoback_full_penalty<0)//10:])
			neg_powernoback_full_penalty = neg_powernoback_full_penalty.reshape(neg_m_penalty.shape)
		else:
			neg_powernoback_full_penalty = np.zeros_like(neg_m_penalty)
		# neg_powernoback_full_penalty = np.zeros_like(powernoback_full[:,:,:,0])	# emissivity, thickness, rec_diffusivity,coord_1,coord_2
		likelihood = np.transpose(np.transpose(neg_powernoback_full_penalty, (1,2,3,0)) + emissivity_log_prob, (3,0,1,2))
		likelihood = np.transpose(np.transpose(likelihood, (0,2,3,1)) + thickness_log_prob, (0,3,1,2))
		likelihood = np.transpose(np.transpose(likelihood, (0,1,3,2)) + rec_diffusivity_log_prob, (0,1,3,2))
		likelihood += neg_m_penalty
		likelihood += edge_penalty
		# likelihood = np.sum(likelihood, axis=-3)
		likelihood = likelihood -np.log(np.trapz(np.trapz(np.trapz(np.exp(likelihood),x=emissivity_array,axis=0),x=thickness_array,axis=0),x=rec_diffusivity_array,axis=0))	# normalisation for logarithmic probabilities
		total_volume = np.trapz(np.trapz(np.trapz(np.ones((emissivity_steps,thickness_steps,rec_diffusivity_steps)),x=emissivity_array,axis=0),x=thickness_array,axis=0),x=rec_diffusivity_array,axis=0)
		final_emissivity = np.trapz(np.trapz(np.trapz(np.exp(likelihood)*(m.reshape(neg_m_penalty.shape)),x=emissivity_array,axis=0),x=thickness_array,axis=0),x=rec_diffusivity_array,axis=0)

		powernoback_full_orig = diffusion_full_crop + timevariation_full_crop + BBrad_full_crop
		sigma_powernoback_full = ( (diffusion_full_crop**2)*((diffusion_std_full_crop/diffusion_full_crop)**2+sigma_thickness**2) + (timevariation_full_crop**2)*((timevariation_std_full_crop/timevariation_full_crop)**2+sigma_thickness**2+sigma_rec_diffusivity**2) + (BBrad_full_crop**2)*((BBrad_std_full_crop/BBrad_full_crop)**2+sigma_emissivity**2) )**0.5

		grid_laplacian_masked_crop_scaled = grid_laplacian_masked_crop/((1e-2*grid_resolution)**2)
		grid_Z_derivate_masked_crop_scaled = grid_Z_derivate_masked_crop/((1e-2*grid_resolution)**1)
		grid_R_derivate_masked_crop_scaled = grid_R_derivate_masked_crop/((1e-2*grid_resolution)**1)
		reference_sigma_powernoback = np.nanmedian(sigma_powernoback_full)
		regolarisation_coeff = 1e-3	# ok for np.median(sigma_powernoback_full)=78.18681
		if grid_resolution==4:
			regolarisation_coeff = 3e-4
		elif grid_resolution==2:
			regolarisation_coeff = 1e-4
		# regolarisation_coeff = 1e-5 / ((reference_sigma_powernoback/78.18681)**0.5)
		sigma_emissivity = 2e3	# this is completely arbitrary
		# sigma_emissivity = 1e4 * ((np.median(sigma_powernoback_full)/78)**0.5)	# I think it must go hand in hand with the uncertanty in the pixels
		regolarisation_coeff_edge = 1e1	# I raiset it artificially from 1e-3 to engage regolarisation_coeff_central_border_Z_derivate and regolarisation_coeff_central_column_border_R_derivate
		regolarisation_coeff_central_border_Z_derivate = 1e-20
		regolarisation_coeff_central_column_border_R_derivate = 1e-20
		regolarisation_coeff_divertor = regolarisation_coeff/1.5
		sigma_emissivity_2 = sigma_emissivity**2

		sigma_powernoback_full[np.isnan(sigma_powernoback_full)] = 1
		selected_ROI_internal = selected_ROI.flatten()
		inverted_data = []
		inverted_data_likelihood = []
		inverted_data_info = []
		inverted_data_plasma_region_offset = []
		inverted_data_homogeneous_offset = []
		fitted_foil_power = []
		foil_power = []
		foil_power_residuals = []
		fit_error = []
		chi_square_all = []
		regolarisation_coeff_all = []
		time_per_iteration = []
		for i_t in range(len(time_full_binned_crop)):
			time_start = tm.time()

			print('starting t=%.4gms' %(time_full_binned_crop[i_t]*1e3))
			# plt.figure()
			# plt.imshow(powernoback_full_orig[i_t])
			# plt.colorbar()
			# plt.pause(0.01)
			#
			# plt.figure()
			# plt.imshow(sigma_powernoback_full[i_t])
			# plt.colorbar()
			# plt.pause(0.01)

			powernoback = powernoback_full_orig[i_t].flatten()
			sigma_powernoback = sigma_powernoback_full[i_t].flatten()
			# sigma_powernoback = np.ones_like(powernoback)*10
			sigma_powernoback_2 = sigma_powernoback**2
			homogeneous_scaling=1e-4

			if time_full_binned_crop[i_t]<0.2:
				if False:	# I can start from a random sample no problem
					guess = final_emissivity+(np.random.random(final_emissivity.shape)-0.5)*1e3	# the 1e3 is to make things harder for the solver, but also to go away a bit from the best fit of other functions
					guess[selected_edge_cells] = guess[selected_edge_cells]*1e-4
					guess = np.concatenate((guess,[0.1/homogeneous_scaling,0.1/homogeneous_scaling]))
				else:
					guess = np.random.random(sensitivities_binned_crop.shape[1]+2)*1e2
			else:
				guess = cp.deepcopy(x_optimal)
				guess[:-2] += (np.random.random(x_optimal.shape[0]-2)-0.5)*1e3	# I still add a bit of scramble to give it the freedom to find the best configuration
			# target_chi_square = 1020	# obtained doing a scan of the regularisation coefficient. this was the result for regolarisation_coeff~1e-3
			target_chi_square = sensitivities_binned_crop.shape[1]	# obtained doing a scan of the regularisation coefficient. this was the result for regolarisation_coeff~1e-3
			target_chi_square_sigma = 200	# this should be tight, because for such a high number of degrees of freedom things should average very well

			def prob_and_gradient(emissivity_plus,*powernoback):
				homogeneous_offset = emissivity_plus[-1]*homogeneous_scaling	# scaling added such that all variables have the same order of magnitude
				homogeneous_offset_plasma = emissivity_plus[-2]*homogeneous_scaling	# scaling added such that all variables have the same order of magnitude
				# print(homogeneous_offset,homogeneous_offset_plasma)
				emissivity = emissivity_plus[:-2]
				emissivity[emissivity==0] = 1e-10
				foil_power_guess = np.dot(sensitivities_binned_crop,emissivity) + selected_ROI_internal*homogeneous_offset + homogeneous_offset_plasma*select_foil_region_with_plasma
				foil_power_error = powernoback - foil_power_guess
				emissivity_laplacian = np.dot(grid_laplacian_masked_crop_scaled,emissivity)
				Z_derivate = np.dot(grid_Z_derivate_masked_crop_scaled,emissivity)
				R_derivate = np.dot(grid_R_derivate_masked_crop_scaled,emissivity)

				likelihood_power_fit = np.sum((foil_power_error/sigma_powernoback)**2)
				likelihood_chi_square =0# ((likelihood_power_fit-target_chi_square)/target_chi_square_sigma)**2
				likelihood_emissivity_pos = np.sum((np.minimum(0.,emissivity)/sigma_emissivity)**2)
				likelihood_emissivity_laplacian = (regolarisation_coeff**2)* np.sum(((emissivity_laplacian*np.logical_not(selected_super_x_cells) /sigma_emissivity)**2))
				likelihood_emissivity_laplacian_superx = (regolarisation_coeff_divertor**2)* np.sum(((emissivity_laplacian*selected_super_x_cells /sigma_emissivity)**2))
				likelihood_emissivity_edge_laplacian = 0#(regolarisation_coeff_edge**2)* np.sum(((emissivity_laplacian*selected_edge_cells_for_laplacian /sigma_emissivity)**2))
				likelihood_emissivity_edge = (regolarisation_coeff_edge**2)*np.sum((emissivity*selected_edge_cells/sigma_emissivity)**2)
				likelihood_emissivity_central_border_Z_derivate = (regolarisation_coeff_central_border_Z_derivate**2)* np.sum((Z_derivate*selected_central_border_cells/sigma_emissivity)**2)
				likelihood_emissivity_central_column_border_R_derivate = (regolarisation_coeff_central_column_border_R_derivate**2)* np.sum((R_derivate*selected_central_column_border_cells/sigma_emissivity)**2)
				likelihood = likelihood_power_fit + likelihood_emissivity_pos + likelihood_emissivity_laplacian + likelihood_emissivity_edge_laplacian + likelihood_emissivity_edge + likelihood_emissivity_central_border_Z_derivate + likelihood_emissivity_central_column_border_R_derivate + likelihood_emissivity_laplacian_superx
				likelihood_homogeneous_offset = 0#(homogeneous_offset/reference_sigma_powernoback)**2
				likelihood_homogeneous_offset_plasma = (homogeneous_offset_plasma/reference_sigma_powernoback)**2
				likelihood = likelihood + likelihood_homogeneous_offset + likelihood_homogeneous_offset_plasma + likelihood_chi_square

				likelihood_power_fit_derivate = np.concatenate((-2*np.dot((foil_power_error/sigma_powernoback_2),sensitivities_binned_crop),[-2*np.sum(foil_power_error*select_foil_region_with_plasma/sigma_powernoback_2)*homogeneous_scaling,-2*np.sum(foil_power_error*selected_ROI_internal/sigma_powernoback_2)*homogeneous_scaling]))
				likelihood_chi_square_derivate =0# likelihood_power_fit_derivate * 2 *(likelihood_power_fit-target_chi_square)/(target_chi_square_sigma**2)
				likelihood_emissivity_pos_derivate = 2*(np.minimum(0.,emissivity)**2)/emissivity/sigma_emissivity_2
				likelihood_emissivity_laplacian_derivate = 2*(regolarisation_coeff**2) * np.dot(emissivity_laplacian*np.logical_not(selected_super_x_cells) , grid_laplacian_masked_crop_scaled) / (sigma_emissivity**2)
				likelihood_emissivity_laplacian_derivate_superx = 2*(regolarisation_coeff_divertor**2) * np.dot(emissivity_laplacian*selected_super_x_cells , grid_laplacian_masked_crop_scaled) / (sigma_emissivity**2)
				likelihood_emissivity_edge_laplacian_derivate = 0#2*(regolarisation_coeff_edge**2) * np.dot(emissivity_laplacian*selected_edge_cells_for_laplacian , grid_laplacian_masked_crop_scaled) / (sigma_emissivity**2)
				likelihood_emissivity_edge_derivate = 2*(regolarisation_coeff_edge**2)*emissivity*selected_edge_cells/sigma_emissivity_2
				likelihood_emissivity_central_border_Z_derivate_derivate = 2*(regolarisation_coeff_central_border_Z_derivate**2)*np.dot(Z_derivate*selected_central_border_cells,grid_Z_derivate_masked_crop_scaled)/sigma_emissivity_2
				likelihood_emissivity_central_column_border_R_derivate_derivate = 2*(regolarisation_coeff_central_column_border_R_derivate**2)*np.dot(R_derivate*selected_central_column_border_cells,grid_R_derivate_masked_crop_scaled)/sigma_emissivity_2
				likelihood_derivate = likelihood_emissivity_pos_derivate + likelihood_emissivity_laplacian_derivate + likelihood_emissivity_edge_laplacian_derivate + likelihood_emissivity_edge_derivate + likelihood_emissivity_central_border_Z_derivate_derivate + likelihood_emissivity_central_column_border_R_derivate_derivate + likelihood_emissivity_laplacian_derivate_superx
				likelihood_homogeneous_offset_derivate = 0#2*homogeneous_offset*homogeneous_scaling/(reference_sigma_powernoback**2)
				likelihood_homogeneous_offset_plasma_derivate = 2*homogeneous_offset_plasma*homogeneous_scaling/(reference_sigma_powernoback**2)
				likelihood_derivate = np.concatenate((likelihood_derivate,[likelihood_homogeneous_offset_plasma_derivate,likelihood_homogeneous_offset_derivate])) + likelihood_power_fit_derivate + likelihood_chi_square_derivate
				# likelihood_derivate = likelihood_emissivity_central_border_derivate
				# print([likelihood,likelihood_derivate.max(),likelihood_derivate.min()])
				return likelihood,likelihood_derivate

			if time_full_binned_crop[i_t]<0.1:
				x_optimal, y_opt, opt_info = scipy.optimize.fmin_l_bfgs_b(prob_and_gradient, x0=guess, args = (powernoback), iprint=0, factr=1e0, pgtol=1e-6, maxiter=5000)#,m=1000, maxls=1000, pgtol=1e-10, factr=1e0)#,approx_grad = True)
			else:
				x_optimal, y_opt, opt_info = scipy.optimize.fmin_l_bfgs_b(prob_and_gradient, x0=guess, args = (powernoback), iprint=0, factr=1e0, pgtol=1e-7, maxiter=5000)#,m=1000, maxls=1000, pgtol=1e-10, factr=1e0)#,approx_grad = True)
			# if opt_info['warnflag']>0:
			# 	print('incomplete fit so restarted')
			# 	x_optimal, y_opt, opt_info = scipy.optimize.fmin_l_bfgs_b(prob_and_gradient, x0=x_optimal, args = (powernoback), iprint=0, factr=1e0, pgtol=1e-7)#,m=1000, maxls=1000, pgtol=1e-10, factr=1e0)#,approx_grad = True)
			# x_optimal[-2:] *= homogeneous_scaling
			x_optimal[-2:] *= np.array([homogeneous_scaling,homogeneous_scaling])

			foil_power_guess = np.dot(sensitivities_binned_crop,x_optimal[:-2])+x_optimal[-2]*select_foil_region_with_plasma+x_optimal[-1]*selected_ROI_internal
			foil_power_error = powernoback - foil_power_guess
			chi_square = np.sum((foil_power_error/sigma_powernoback)**2)
			print('chi_square '+str(chi_square))

			voxels_centre = np.mean(grid_data_masked_crop,axis=1)
			dr = np.median(np.diff(np.unique(voxels_centre[:,0])))
			dz = np.median(np.diff(np.unique(voxels_centre[:,1])))
			dist_mean = (dz**2 + dr**2)/2
			recompose_voxel_emissivity = np.zeros((len(np.unique(voxels_centre[:,0])),len(np.unique(voxels_centre[:,1]))))*np.nan
			for i_r,r in enumerate(np.unique(voxels_centre[:,0])):
				for i_z,z in enumerate(np.unique(voxels_centre[:,1])):
					dist = (voxels_centre[:,0]-r)**2 + (voxels_centre[:,1]-z)**2
					if dist.min()<dist_mean/2:
						index = np.abs(dist).argmin()
						# recompose_voxel_emissivity[i_r,i_z] = guess[index]
						# recompose_voxel_emissivity[i_r,i_z] = (x_optimal-guess)[index]
						# recompose_voxel_emissivity[i_r,i_z] = (x_optimal2-x_optimal3)[index]
						recompose_voxel_emissivity[i_r,i_z] = x_optimal[index]
						# recompose_voxel_emissivity[i_r,i_z] = likelihood_emissivity_laplacian[index]

			recompose_voxel_emissivity *= 4*np.pi	# this exist because the sensitivity matrix is built with 1W/str/m^3/ x nm emitters while I use 1W as reference, so I need to multiply the results by 4pi

			if False:	# only for visualisation
				plt.figure(figsize=(12,13))
				# plt.scatter(np.mean(grid_data_masked_crop,axis=1)[:,0],np.mean(grid_data_masked_crop,axis=1)[:,1],c=x_optimal,s=100,marker='s',cmap='rainbow')
				plt.imshow(np.flip(np.flip(np.flip(np.transpose(recompose_voxel_emissivity,(1,0)),axis=1),axis=1),axis=0),extent=[grid_data_masked_crop[:,:,0].min(),grid_data_masked_crop[:,:,0].max(),grid_data_masked_crop[:,:,1].min(),grid_data_masked_crop[:,:,1].max()])
				plt.plot(_MASTU_CORE_GRID_POLYGON[:, 0], _MASTU_CORE_GRID_POLYGON[:, 1], 'k')
				temp = np.abs(efit_reconstruction.time-time_full_binned_crop[i_t]).argmin()
				for i in range(len(all_time_sep_r[temp])):
					plt.plot(r_fine[all_time_sep_r[temp][i]],z_fine[all_time_sep_z[temp][i]],'--b')
				plt.plot(efit_reconstruction.lower_xpoint_r[temp],efit_reconstruction.lower_xpoint_z[temp],'xr')
				plt.plot(efit_reconstruction.strikepointR[temp],efit_reconstruction.strikepointZ[temp],'xr')
				plt.title('sigma_emissivity %.3g\nregolarisation_coeff %.3g\nregolarisation_coeff_edge %.3g\nregolarisation_coeff_central_border_Z_derivate %.3g\nregolarisation_coeff_central_column_border_R_derivate %.3g' %(sigma_emissivity,regolarisation_coeff,regolarisation_coeff_edge,regolarisation_coeff_central_border_Z_derivate,regolarisation_coeff_central_column_border_R_derivate))
				plt.colorbar().set_label('emissivity [W/m3]')
				plt.ylim(top=0.5)
				# plt.savefig(path_power_output + '/'+ str(shot_number)+'_'+binning_type+'_gridres'+str(grid_resolution)+'cm_example19.eps')
				plt.pause(0.01)

			inverted_data.append(recompose_voxel_emissivity)
			inverted_data_likelihood.append(y_opt)
			inverted_data_plasma_region_offset.append(x_optimal[-2])
			inverted_data_homogeneous_offset.append(x_optimal[-1])
			inverted_data_info.append(opt_info)
			fitted_foil_power.append((np.dot(sensitivities_binned_crop,x_optimal[:-2])+x_optimal[-2]*select_foil_region_with_plasma+x_optimal[-1]*selected_ROI_internal).reshape(powernoback_full_orig[i_t].shape))
			foil_power.append(powernoback_full_orig[i_t])
			foil_power_residuals.append(powernoback_full_orig[i_t]-(np.dot(sensitivities_binned_crop,x_optimal[:-2])+x_optimal[-2]*select_foil_region_with_plasma+x_optimal[-1]*selected_ROI_internal).reshape(powernoback_full_orig[i_t].shape))
			fit_error.append(np.sum(((powernoback_full_orig[i_t][selected_ROI]-(np.dot(sensitivities_binned_crop,x_optimal[:-2])+x_optimal[-2]*select_foil_region_with_plasma+x_optimal[-1]*selected_ROI_internal).reshape(powernoback_full_orig[i_t].shape)[selected_ROI]))**2)**0.5/np.sum(selected_ROI))
			chi_square_all.append(chi_square)
			regolarisation_coeff_all.append(regolarisation_coeff)
			time_per_iteration.append(tm.time()-time_start)

		inverted_data = np.array(inverted_data)
		inverted_data_likelihood = -np.array(inverted_data_likelihood)
		inverted_data_plasma_region_offset = np.array(inverted_data_plasma_region_offset)
		inverted_data_homogeneous_offset = np.array(inverted_data_homogeneous_offset)
		fit_error = np.array(fit_error)
		chi_square_all = np.array(chi_square_all)
		regolarisation_coeff_all = np.array(regolarisation_coeff_all)
		time_per_iteration = np.array(time_per_iteration)
		fitted_foil_power = np.array(fitted_foil_power)
		foil_power = np.array(foil_power)
		foil_power_residuals = np.array(foil_power_residuals)

		timeout = 20*60	# 20 minutes
		while efit_reconstruction==None and timeout>0:
			try:
				EFIT_path_default = '/common/uda-scratch/lkogan/efitpp_eshed'
				efit_reconstruction = mclass(EFIT_path_default+'/epm0'+laser_to_analyse[-9:-4]+'.nc',pulse_ID=laser_to_analyse[-9:-4])

			except:
				print('EFIT missing, waiting 20 seconds')
			tm.sleep(20)
			timeout -= 20

		if efit_reconstruction!=None:

			temp = brightness[:,:,:int(np.shape(brightness)[2]*0.75)]
			temp = np.sort(temp[np.max(temp,axis=(1,2)).argmax()].flatten())
			ani,efit_reconstruction = movie_from_data(np.array([np.flip(np.transpose(brightness,(0,2,1)),axis=2)]), 1/np.median(np.diff(time_binned)),timesteps=time_binned[1:-1],integration=laser_int_time/1000,time_offset=time_binned[0],extvmin=0,xlabel='horizontal coord [pixels]', ylabel='vertical coord [pixels]',barlabel='brightness [W/m2]', prelude='shot ' + laser_to_analyse[-9:-4]+'\n'+binning_type+'\n',overlay_structure=True,include_EFIT=True,pulse_ID=laser_to_analyse[-9:-4],overlay_x_point=True,overlay_mag_axis=True,overlay_strike_points=True,overlay_separatrix=True,EFIT_output_requested=True,efit_reconstruction=efit_reconstruction)
			ani.save('/home/ffederic/work/irvb/MAST-U/FAST_results/'+os.path.split(laser_to_analyse[:-4])[1]+ '_FAST_brightness.mp4', fps=5*(1/np.mean(np.diff(time_binned)))/383, writer='ffmpeg',codec='mpeg4')
			plt.close('all')

			all_time_sep_r,all_time_sep_z,r_fine,z_fine = efit_reconstruction_to_separatrix_on_foil(efit_reconstruction)
			all_time_strike_points_location = return_all_time_strike_points_location_radial(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine)
			outer_leg_tot_rad_power_all = []
			sxd_tot_rad_power_all = []
			inner_leg_tot_rad_power_all = []
			core_tot_rad_power_all = []
			x_point_tot_rad_power_all = []
			for i_t in range(len(time_full_binned_crop)):
				temp = np.abs(efit_reconstruction.time-time_full_binned_crop[i_t]).argmin()
				xpoint_r = efit_reconstruction.lower_xpoint_r[temp]
				xpoint_z = efit_reconstruction.lower_xpoint_z[temp]
				z_,r_ = np.meshgrid(np.unique(voxels_centre[:,1]),np.unique(voxels_centre[:,0]))
				temp = cp.deepcopy(inverted_data[i_t])
				temp[z_>xpoint_z] = 0
				temp[r_<xpoint_r] = 0
				outer_leg_tot_rad_power = np.nansum(temp*2*np.pi*r_*((grid_resolution*0.01)**2))
				temp = cp.deepcopy(inverted_data[i_t])
				temp[z_>xpoint_z] = 0
				temp[r_>xpoint_r] = 0
				inner_leg_tot_rad_power = np.nansum(temp*2*np.pi*r_*((grid_resolution*0.01)**2))
				temp = cp.deepcopy(inverted_data[i_t])
				temp[z_<xpoint_z] = 0
				temp[z_>0] = 0
				core_tot_rad_power = np.nansum(temp*2*np.pi*r_*((grid_resolution*0.01)**2))
				temp = cp.deepcopy(inverted_data[i_t])
				temp[z_<-1.5] = 0
				temp[r_>0.8] = 0
				sxd_tot_rad_power = np.nansum(temp*2*np.pi*r_*((grid_resolution*0.01)**2))
				temp = cp.deepcopy(inverted_data[i_t])
				temp[((z_-xpoint_z)**2+(r_-xpoint_r)**2)**0.5>0.10] = 0
				x_point_tot_rad_power = np.nansum(temp*2*np.pi*r_*((grid_resolution*0.01)**2))
				outer_leg_tot_rad_power_all.append(outer_leg_tot_rad_power)
				inner_leg_tot_rad_power_all.append(inner_leg_tot_rad_power)
				core_tot_rad_power_all.append(core_tot_rad_power)
				sxd_tot_rad_power_all.append(sxd_tot_rad_power)
				x_point_tot_rad_power_all.append(x_point_tot_rad_power)
			outer_leg_tot_rad_power_all = np.array(outer_leg_tot_rad_power_all)
			inner_leg_tot_rad_power_all = np.array(inner_leg_tot_rad_power_all)
			core_tot_rad_power_all = np.array(core_tot_rad_power_all)
			sxd_tot_rad_power_all = np.array(sxd_tot_rad_power_all)
			x_point_tot_rad_power_all = np.array(x_point_tot_rad_power_all)

			plt.figure(figsize=(20, 10))
			plt.plot(time_full_binned_crop,outer_leg_tot_rad_power_all/1e3,label='outer_leg')
			plt.plot(time_full_binned_crop,sxd_tot_rad_power_all/1e3,label='sxd')
			plt.plot(time_full_binned_crop,inner_leg_tot_rad_power_all/1e3,label='inner_leg')
			plt.plot(time_full_binned_crop,core_tot_rad_power_all/1e3,label='core')
			plt.plot(time_full_binned_crop,x_point_tot_rad_power_all/1e3,label='x_point')
			plt.plot(time_full_binned_crop,outer_leg_tot_rad_power_all/1e3+inner_leg_tot_rad_power_all/1e3+core_tot_rad_power_all/1e3,label='tot')
			plt.title('radiated power in the lower half of the machine')
			plt.legend(loc='best', fontsize='x-small')
			plt.xlabel('time [s]')
			plt.ylabel('power [kW]')
			plt.grid()
			plt.savefig('/home/ffederic/work/irvb/MAST-U/FAST_results/'+os.path.split(laser_to_analyse[:-4])[1]+'_'+binning_type+'_gridres'+str(grid_resolution)+'cm_FAST_tot_rad_power.eps')
			plt.close()

			inverted_dict[str(grid_resolution)]['outer_leg_tot_rad_power_all'] = outer_leg_tot_rad_power_all
			inverted_dict[str(grid_resolution)]['inner_leg_tot_rad_power_all'] = inner_leg_tot_rad_power_all
			inverted_dict[str(grid_resolution)]['core_tot_rad_power_all'] = core_tot_rad_power_all
			inverted_dict[str(grid_resolution)]['sxd_tot_rad_power_all'] = sxd_tot_rad_power_all
			inverted_dict[str(grid_resolution)]['x_point_tot_rad_power_all'] = x_point_tot_rad_power_all


		path_power_output = os.path.split(laser_to_analyse)[0] + '/' + str(shot_number)
		if not os.path.exists(path_power_output):
			os.makedirs(path_power_output)
		plt.figure(figsize=(20, 10))
		plt.plot(time_full_binned_crop,time_per_iteration)
		# plt.semilogy()
		plt.title('time spent per iteration')
		plt.xlabel('time [s]')
		plt.ylabel('time [s]')
		plt.grid()
		plt.savefig(path_power_output + '/'+ str(shot_number)+'_'+binning_type+'_gridres'+str(grid_resolution)+'cm_FAST_time_trace.eps')
		plt.close()

		plt.figure(figsize=(20, 10))
		plt.plot(time_full_binned_crop,inverted_data_likelihood)
		# plt.semilogy()
		plt.title('Fit log likelihood')
		plt.xlabel('time [s]')
		plt.ylabel('log likelihoog [au]')
		plt.grid()
		plt.savefig(path_power_output + '/'+ str(shot_number)+'_'+binning_type+'_gridres'+str(grid_resolution)+'cm_FAST_likelihood.eps')
		plt.close()

		plt.figure(figsize=(20, 10))
		plt.plot(time_full_binned_crop,chi_square_all)
		plt.plot(time_full_binned_crop,np.ones_like(time_full_binned_crop)*target_chi_square,'--k')
		# plt.semilogy()
		if False:
			plt.title('chi square obtained vs requested\nfixed regularisation of '+str(regolarisation_coeff))
		else:
			plt.title('chi square obtained vs requested\nflexible regolarisation coefficient')
		plt.xlabel('time [s]')
		plt.ylabel('chi square [au]')
		plt.grid()
		plt.savefig(path_power_output + '/'+ str(shot_number)+'_'+binning_type+'_gridres'+str(grid_resolution)+'cm_FAST_chi_square.eps')
		plt.close()

		plt.figure(figsize=(20, 10))
		plt.plot(time_full_binned_crop,regolarisation_coeff_all)
		# plt.semilogy()
		plt.title('regolarisation coefficient obtained')
		plt.semilogy()
		plt.xlabel('time [s]')
		plt.ylabel('regolarisation coefficient [au]')
		plt.grid()
		plt.savefig(path_power_output + '/'+ str(shot_number)+'_'+binning_type+'_gridres'+str(grid_resolution)+'cm_FAST_regolarisation_coeff.eps')
		plt.close()

		plt.figure(figsize=(20, 10))
		plt.plot(time_full_binned_crop,fit_error)
		# plt.semilogy()
		plt.title('Fit error ( sum((image-fit)^2)^0.5/num pixels )')
		plt.xlabel('time [s]')
		plt.ylabel('average fit error [W/m2]')
		plt.grid()
		plt.savefig(path_power_output + '/'+ str(shot_number)+'_'+binning_type+'_gridres'+str(grid_resolution)+'cm_FAST_fit_error.eps')
		plt.close()

		plt.figure(figsize=(20, 10))
		plt.plot(time_full_binned_crop,inverted_data_plasma_region_offset,label='plasma region')
		plt.plot(time_full_binned_crop,inverted_data_homogeneous_offset,label='whole foil')
		plt.title('Offsets to match foil power')
		plt.legend(loc='best', fontsize='x-small')
		plt.xlabel('time [s]')
		plt.ylabel('power density [W/m2]')
		plt.grid()
		plt.savefig(path_power_output + '/'+ str(shot_number)+'_'+binning_type+'_gridres'+str(grid_resolution)+'cm_FAST_offsets.eps')
		plt.close()

		if efit_reconstruction!=None:

			additional_points_dict,radiator_xpoint_distance_all,radiator_above_xpoint_all,radiator_magnetic_radious_all,radiator_baricentre_magnetic_radious_all,radiator_baricentre_above_xpoint_all = find_radiator_location(inverted_data,np.unique(voxels_centre[:,0]),np.unique(voxels_centre[:,1]),time_full_binned_crop,efit_reconstruction)

			inverted_dict[str(grid_resolution)]['radiator_location_all'] = additional_points_dict['0']
			inverted_dict[str(grid_resolution)]['radiator_xpoint_distance_all'] = radiator_xpoint_distance_all
			inverted_dict[str(grid_resolution)]['radiator_above_xpoint_all'] = radiator_above_xpoint_all
			inverted_dict[str(grid_resolution)]['radiator_magnetic_radious_all'] = radiator_magnetic_radious_all

			fig, ax = plt.subplots( 2,1,figsize=(8, 12), squeeze=False,sharex=True)
			ax[0,0].plot(time_full_binned_crop,radiator_magnetic_radious_all)
			ax[0,0].plot(time_full_binned_crop,radiator_baricentre_magnetic_radious_all,'--')
			ax[0,0].set_ylim(top=min(np.nanmax(radiator_magnetic_radious_all),1.1),bottom=max(np.nanmin(radiator_magnetic_radious_all),0.9))
			ax[1,0].plot(time_full_binned_crop,radiator_above_xpoint_all)
			ax[1,0].plot(time_full_binned_crop,radiator_baricentre_above_xpoint_all,'--')
			fig.suptitle('Location of the x-point radiator\n"--"=baricentre r=20cm around x-point')
			ax[0,0].set_ylabel('normalised psi [au]')
			ax[0,0].grid()
			ax[1,0].set_xlabel('time [s]')
			ax[1,0].set_ylabel('position above x-point [m]')
			ax[1,0].grid()
			plt.savefig('/home/ffederic/work/irvb/MAST-U/FAST_results/'+os.path.split(laser_to_analyse[:-4])[1]+'_'+binning_type+'_gridres'+str(grid_resolution)+'cm_FAST_x_point_location.eps')
			plt.close()

			extent = [grid_data_masked_crop[:,:,0].min(), grid_data_masked_crop[:,:,0].max(), grid_data_masked_crop[:,:,1].min(), grid_data_masked_crop[:,:,1].max()]
			image_extent = [grid_data_masked_crop[:,:,0].min(), grid_data_masked_crop[:,:,0].max(), grid_data_masked_crop[:,:,1].min(), grid_data_masked_crop[:,:,1].max()]
			ani,trash = movie_from_data_radial_profile(np.array([np.flip(np.transpose(inverted_data,(0,2,1)),axis=2)]), 1/(np.mean(np.diff(time_full_binned_crop))), extent = extent, image_extent=image_extent,timesteps=time_full_binned_crop,integration=laser_int_time/1000,barlabel='Emissivity [W/m3]',xlabel='R [m]', ylabel='Z [m]', prelude='shot '  + laser_to_analyse[-9:-4] + '\n'+binning_type+'\n'+'sigma_emissivity %.3g\nregolarisation_coeff %.3g\nregolarisation_coeff_edge %.3g\nregolarisation_coeff_central_border_Z_derivate %.3g\nregolarisation_coeff_central_column_border_R_derivate %.3g\nregolarisation_coeff_divertor %.3g\ngrid resolution %.3g\n' %(sigma_emissivity,regolarisation_coeff,regolarisation_coeff_edge,regolarisation_coeff_central_border_Z_derivate,regolarisation_coeff_central_column_border_R_derivate,regolarisation_coeff_divertor,grid_resolution) ,overlay_structure=True,include_EFIT=True,EFIT_output_requested=True,efit_reconstruction=efit_reconstruction,pulse_ID=laser_to_analyse[-9:-4],overlay_x_point=True,overlay_mag_axis=True,overlay_strike_points=True,overlay_separatrix=True,additional_points_dict=additional_points_dict)#,extvmin=0,extvmax=4e4)

		else:

			# ani = movie_from_data(np.array([np.flip(np.transpose(recompose_voxel_emissivity,(0,2,1)),axis=2)]), 1/(np.mean(np.diff(time_full_binned_crop))),integration=laser_int_time/1000,barlabel='Emissivity [W/m3]')#,extvmin=0,extvmax=4e4)
			extent = [grid_data_masked_crop[:,:,0].min(), grid_data_masked_crop[:,:,0].max(), grid_data_masked_crop[:,:,1].min(), grid_data_masked_crop[:,:,1].max()]
			image_extent = [grid_data_masked_crop[:,:,0].min(), grid_data_masked_crop[:,:,0].max(), grid_data_masked_crop[:,:,1].min(), grid_data_masked_crop[:,:,1].max()]
			ani,trash = movie_from_data_radial_profile(np.array([np.flip(np.transpose(inverted_data,(0,2,1)),axis=2)]), 1/(np.mean(np.diff(time_full_binned_crop))), extent = extent, image_extent=image_extent,timesteps=time_full_binned_crop,integration=laser_int_time/1000,barlabel='Emissivity [W/m3]',xlabel='R [m]', ylabel='Z [m]', prelude='shot '  + laser_to_analyse[-9:-4] + '\n'+binning_type+'\n'+'sigma_emissivity %.3g\nregolarisation_coeff %.3g\nregolarisation_coeff_edge %.3g\nregolarisation_coeff_central_border_Z_derivate %.3g\nregolarisation_coeff_central_column_border_R_derivate %.3g\nregolarisation_coeff_divertor %.3g\ngrid resolution %.3g\n' %(sigma_emissivity,regolarisation_coeff,regolarisation_coeff_edge,regolarisation_coeff_central_border_Z_derivate,regolarisation_coeff_central_column_border_R_derivate,regolarisation_coeff_divertor,grid_resolution) ,overlay_structure=True,include_EFIT=True,EFIT_output_requested=True,efit_reconstruction=efit_reconstruction,pulse_ID=laser_to_analyse[-9:-4],overlay_x_point=True,overlay_mag_axis=True,overlay_strike_points=True,overlay_separatrix=True)#,extvmin=0,extvmax=4e4)
		ani.save('/home/ffederic/work/irvb/MAST-U/FAST_results/'+os.path.split(laser_to_analyse[:-4])[1]+'_'+binning_type+'_gridres'+str(grid_resolution)+'cm_FAST_reconstruct_emissivity_bayesian.mp4', fps=5*(1/(np.mean(np.diff(time_full_binned_crop))))/383, writer='ffmpeg',codec='mpeg4')
		plt.close()


		inverted_dict[str(grid_resolution)]['binning_type'] = binning_type
		inverted_dict[str(grid_resolution)]['inverted_data'] = inverted_data
		inverted_dict[str(grid_resolution)]['inverted_data_likelihood'] = inverted_data_likelihood
		inverted_dict[str(grid_resolution)]['inverted_data_info'] = inverted_data_info
		inverted_dict[str(grid_resolution)]['select_foil_region_with_plasma'] = select_foil_region_with_plasma
		inverted_dict[str(grid_resolution)]['inverted_data_plasma_region_offset'] = inverted_data_plasma_region_offset
		inverted_dict[str(grid_resolution)]['inverted_data_homogeneous_offset'] = inverted_data_homogeneous_offset
		inverted_dict[str(grid_resolution)]['time_full_binned_crop'] = time_full_binned_crop
		inverted_dict[str(grid_resolution)]['fitted_foil_power'] = fitted_foil_power
		inverted_dict[str(grid_resolution)]['foil_power'] = foil_power
		inverted_dict[str(grid_resolution)]['foil_power_residuals'] = foil_power_residuals
		inverted_dict[str(grid_resolution)]['fit_error'] = fit_error
		inverted_dict[str(grid_resolution)]['chi_square_all'] = chi_square_all
		inverted_dict[str(grid_resolution)]['geometry'] = dict([])
		inverted_dict[str(grid_resolution)]['geometry']['R'] = np.unique(voxels_centre[:,0])
		inverted_dict[str(grid_resolution)]['geometry']['Z'] = np.unique(voxels_centre[:,1])

		if efit_reconstruction!=None:

			inversion_R = np.unique(voxels_centre[:,0])
			inversion_Z = np.unique(voxels_centre[:,1])
			local_mean_emis_all,local_power_all,leg_length_interval_all,leg_length_all,data_length,leg_resolution = track_outer_leg_radiation(inverted_data,inversion_R,inversion_Z,time_full_binned_crop,efit_reconstruction)

			try:
				fig, ax = plt.subplots( 1,2,figsize=(10, 20), squeeze=False,sharey=True)
				temp = np.array(local_power_all)
				temp[np.isnan(temp)] = 0
				im1 = ax[0,0].imshow(temp,'rainbow',origin='lower',extent=[(0-0.5)*leg_resolution,(data_length+0.5)*leg_resolution,time_full_binned_crop[0]-np.diff(time_full_binned_crop)[0]/2,time_full_binned_crop[-1]+np.diff(time_full_binned_crop)[-1]/2],aspect=10,vmin=np.min(temp[:-4]),vmax=np.max(temp[:-4]))
				ax[0,0].plot(leg_length_all,time_full_binned_crop,'--k')
				temp = np.array(local_mean_emis_all)
				temp[np.isnan(temp)] = 0
				im2 = ax[0,1].imshow(temp,'rainbow',origin='lower',extent=[(0-0.5)*leg_resolution,(data_length+0.5)*leg_resolution,time_full_binned_crop[0]-np.diff(time_full_binned_crop)[0]/2,time_full_binned_crop[-1]+np.diff(time_full_binned_crop)[-1]/2],aspect=10,vmin=np.min(temp[:-4]),vmax=np.max(temp[:-4]))
				ax[0,1].plot(leg_length_all,time_full_binned_crop,'--k')
				fig.suptitle('tracking radiation on the outer leg')
				ax[0,0].set_xlabel('distance from the strike point [m]')
				ax[0,0].grid()
				ax[0,0].set_ylabel('time [s]')
				plt.colorbar(im1,ax=ax[0,0]).set_label('Integrated power [W]')
				# ax[0,0].colorbar().set_label('Integrated power [W]')
				ax[0,1].set_xlabel('distance from the strike point [m]')
				ax[0,1].grid()
				# ax[0,1].colorbar().set_label('Emissivity [W/m3]')
				plt.colorbar(im2,ax=ax[0,1]).set_label('Emissivity [W/m3]')
				plt.savefig('/home/ffederic/work/irvb/MAST-U/FAST_results/'+os.path.split(laser_to_analyse[:-4])[1]+'_'+binning_type+'_gridres'+str(grid_resolution)+'cm_outer_leg_radiation_tracking.eps')
				plt.close()


				number_of_curves_to_plot = 12
				alpha = 0.9
				# colors_smooth = np.array([np.linspace(0,1,num=number_of_curves_to_plot),np.linspace(1,0,num=number_of_curves_to_plot),[0]*number_of_curves_to_plot]).T
				colors_smooth = np.linspace(0,0.9,num=number_of_curves_to_plot).astype(str)
				select = np.unique(np.round(np.linspace(0,len(local_mean_emis_all)-1,num=number_of_curves_to_plot))).astype(int)
				fig, ax = plt.subplots( 2,1,figsize=(10, 20), squeeze=False,sharex=True)
				fig.suptitle('tracking radiation on the outer leg')
				# plt.figure()
				for i_i_t,i_t in enumerate(select):
					# ax[0,0].plot(np.cumsum(leg_length_interval_all[i_t]),local_mean_emis_all[i_t],color=colors_smooth[i_i_t],label = '%.3gs' %(time_full_binned_crop[i_t]),linewidth=3)
					# ax[1,0].plot(np.cumsum(leg_length_interval_all[i_t]),local_power_all[i_t],color=colors_smooth[i_i_t],linewidth=3)
					to_plot_x = np.array(leg_length_interval_all[i_t])
					to_plot_y1 = np.array(local_mean_emis_all[i_t])
					to_plot_y1 = to_plot_y1[to_plot_x>0]
					to_plot_y2 = np.array(local_power_all[i_t])
					to_plot_y2 = to_plot_y2[to_plot_x>0]
					to_plot_x = to_plot_x[to_plot_x>0]
					to_plot_x = np.flip(np.sum(to_plot_x)-(np.cumsum(to_plot_x)-np.array(to_plot_x)/2),axis=0)
					to_plot_y1 = np.flip(to_plot_y1,axis=0)
					to_plot_y2 = np.flip(to_plot_y2,axis=0)
					if i_i_t%3==0:
						ax[0,0].plot(to_plot_x,to_plot_y1,'-',color=colors_smooth[i_i_t],label = '%.3gs' %(time_full_binned_crop[i_t]),linewidth=3,alpha=alpha)
						ax[1,0].plot(to_plot_x,to_plot_y2,'-',color=colors_smooth[i_i_t],linewidth=3,alpha=alpha)
					elif i_i_t%3==1:
						ax[0,0].plot(to_plot_x,to_plot_y1,'-.',color=colors_smooth[i_i_t],label = '%.3gs' %(time_full_binned_crop[i_t]),linewidth=3,alpha=alpha)
						ax[1,0].plot(to_plot_x,to_plot_y2,'-.',color=colors_smooth[i_i_t],linewidth=3,alpha=alpha)
					elif i_i_t%3==2:
						ax[0,0].plot(to_plot_x,to_plot_y1,'--',color=colors_smooth[i_i_t],label = '%.3gs' %(time_full_binned_crop[i_t]),linewidth=3,alpha=alpha)
						ax[1,0].plot(to_plot_x,to_plot_y2,'--',color=colors_smooth[i_i_t],linewidth=3,alpha=alpha)
				ax[0,0].legend(loc='best', fontsize='x-small')
				ax[0,0].set_ylabel('average emissivity [W/m3]')
				ax[1,0].set_ylabel('local radiated power [W]')
				# ax[1,0].set_xlabel('distance from target [m]')
				ax[1,0].set_xlabel('distance from x-point [m]')
				ax[0,0].grid()
				ax[1,0].grid()
				plt.savefig('/home/ffederic/work/irvb/MAST-U/FAST_results/'+os.path.split(laser_to_analyse[:-4])[1]+'_'+binning_type+'_gridres'+str(grid_resolution)+'cm_outer_leg_radiation_tracking_2.png')
				plt.close()

			except Exception as e:
				logging.exception('with error: ' + str(e))
				print('failed to print\n'+'/home/ffederic/work/irvb/MAST-U/FAST_results/'+os.path.split(laser_to_analyse[:-4])[1]+'_'+binning_type+'_gridres'+str(grid_resolution)+'cm_outer_leg_radiation_tracking.eps')

			inverted_dict[str(grid_resolution)]['local_outer_leg_power'] = local_power_all
			inverted_dict[str(grid_resolution)]['local_outer_leg_mean_emissivity'] = local_mean_emis_all
			inverted_dict[str(grid_resolution)]['leg_length_all'] = leg_length_all
			inverted_dict[str(grid_resolution)]['leg_length_interval_all'] = leg_length_interval_all

	return foilrotdeg,out_of_ROI_mask,foildw,foilup,foillx,foilrx,FAST_counts_minus_background_crop_binned,time_binned,powernoback_output,brightness,binning_type,inverted_dict


def MASTU_pulse_process_FAST3_BB(laser_counts_corrected,time_of_experiment_digitizer_ID,time_of_experiment,external_clock_marker,aggregated_correction_coefficients,laser_framerate,laser_digitizer_ID,laser_int_time,seconds_for_reference_frame,start_time_of_pulse,laser_to_analyse,height,width,flag_use_of_first_frames_as_reference,params,errparams,params_BB,errparams_BB,photon_flux_over_temperature_interpolator,BB_proportional,BB_proportional_std,foil_position_dict,pass_number = 0,disruption_check=False,x_point_region_radious=0.1,wavewlength_top=5.1,wavelength_bottom=1.5,override_second_pass=True,override_third_pass=True,time_shot_min=-0.1,time_shot_max=1.5,only_plot_brightness=False,use_pinhole_plate_over_temperature = True,fix_pinhole_emis_zero=True,std_from_pure_noise=False,decrease_smoothing_inner_leg_x_point=True):
	# created 2022/01/11
	# created modifying MASTU_pulse_process_FAST3 to go from polynomial temperature calibration fo black body
	import time as tm
	start_processing_fast = tm.time()
	from scipy.ndimage import median_filter

	# if False:	# lines just for debugging
	# 	retrive_shot_scenario = coleval.retrive_shot_scenario
	# 	real_mean_filter_agent = coleval.real_mean_filter_agent
	# 	estimate_counts_std = coleval.estimate_counts_std
	# 	get_rotation_crop_parameters = coleval.get_rotation_crop_parameters
	# 	rotate_and_crop_3D = coleval.rotate_and_crop_3D
	# 	retrive_vessel_average_temp_archve = coleval.retrive_vessel_average_temp_archve
	# 	count_to_temp_BB_multi_digitizer = coleval.count_to_temp_BB_multi_digitizer
	# 	real_mean_filter_agent = coleval.real_mean_filter_agent
	# 	rotate_and_crop_3D = coleval.rotate_and_crop_3D
	# 	rotate_and_crop_2D = coleval.rotate_and_crop_2D
	# 	proper_homo_binning_t_2D = coleval.proper_homo_binning_t_2D
	# 	proper_homo_binning_2D = coleval.proper_homo_binning_2D
	# 	proper_homo_binning_t = coleval.proper_homo_binning_t
	# 	calc_temp_to_power_BB_1 = coleval.calc_temp_to_power_BB_1
	# 	calc_temp_to_power_BB_2 = coleval.calc_temp_to_power_BB_2
	# 	movie_from_data = coleval.movie_from_data
	# 	all_file_names = coleval.all_file_names
	# 	reduce_voxels = coleval.reduce_voxels
	# 	proper_homo_binning_1D_1D_1D = coleval.proper_homo_binning_1D_1D_1D
	# 	cut_sensitivity_matrix_based_on_foil_anysotropy = coleval.cut_sensitivity_matrix_based_on_foil_anysotropy
	# 	build_laplacian = coleval.build_laplacian
	# 	select_region_with_plasma = coleval.select_region_with_plasma
	# 	build_Z_derivate = coleval.build_Z_derivate
	# 	build_R_derivate = coleval.build_R_derivate
	#
	# 	calculate_tangency_angle_for_poloidal_section=coleval.calculate_tangency_angle_for_poloidal_section
	# 	client=pyuda.Client()
	# 	exec(open("/home/ffederic/work/analysis_scripts/scripts/python_library/collect_and_eval/collect_and_eval/MASTU_structure.py").read())
	# 	# reset_connection(client)
	# 	del client
	#
	# 	calculate_tile_geometry = coleval.calculate_tile_geometry
	# 	define_fitting_functions = coleval.define_fitting_functions
	# 	loop_fit_over_regularisation = coleval.loop_fit_over_regularisation
	# 	calc_IRVB_head_power_correction = coleval.calc_IRVB_head_power_correction
	# 	find_optimal_regularisation_minimal = coleval.find_optimal_regularisation_minimal
	# 	find_optimal_regularisation = coleval.find_optimal_regularisation
	# 	translate_emissivity_profile_with_homo_temp = coleval.translate_emissivity_profile_with_homo_temp
	# 	efit_reconstruction_to_separatrix_on_foil = coleval.efit_reconstruction_to_separatrix_on_foil
	# 	return_all_time_strike_points_location_radial = coleval.return_all_time_strike_points_location_radial
	# 	return_MU01_sxd_region_delimiter = coleval.return_MU01_sxd_region_delimiter
	# 	get_tend = coleval.get_tend
	# 	retrive_shot_foil_mask_type = coleval.retrive_shot_foil_mask_type
	# 	locate_pinhole = coleval.locate_pinhole

	# define filename
	filename_root = '/home/ffederic/work/irvb/MAST-U/FAST_results/'+laser_to_analyse[-9:-4]
	if not os.path.exists(filename_root):
		os.makedirs(filename_root)
	filename_root = filename_root+'/'+os.path.split(laser_to_analyse[:-4])[1]+'_pass'+str(pass_number)


	# sanity check to see that the data is good
	force_reference_after_shot = False
	if np.sum([np.diff(time_of_experiment_digitizer_ID[i]).max()>np.median(np.diff(time_of_experiment_digitizer_ID[i]))*1.8 for i in range(len(laser_digitizer_ID))])>0:
		time_of_break = ((time_of_experiment_digitizer_ID[0]-time_of_experiment[0])*1e-6-start_time_of_pulse)[(np.diff(time_of_experiment_digitizer_ID[0])>np.median(np.diff(time_of_experiment_digitizer_ID[0]))*1.8).argmax()]
		if time_of_break>time_shot_min and time_of_break<time_shot_max:
			print('there is something wrong, there are holes in the time axis right during the shot, process ended')
			sbla=asdaDASD	# I want an error to occour here
		elif time_of_break<time_shot_min:
			print('Still have to fix this case')
			sbla=asdaDASD	# I want an error to occour here
			laser_counts_corrected[0] = laser_counts_corrected[0][((time_of_experiment_digitizer_ID[0]-time_of_experiment[0])*1e-6-start_time_of_pulse)>time_of_break]
			time_of_experiment_digitizer_ID[0] = time_of_experiment_digitizer_ID[0][((time_of_experiment_digitizer_ID[0]-time_of_experiment[0])*1e-6-start_time_of_pulse)>time_of_break]
			laser_counts_corrected[1] = laser_counts_corrected[1][((time_of_experiment_digitizer_ID[1]-time_of_experiment[0])*1e-6-start_time_of_pulse)>time_of_break]
			time_of_experiment_digitizer_ID[1] = time_of_experiment_digitizer_ID[1][((time_of_experiment_digitizer_ID[1]-time_of_experiment[0])*1e-6-start_time_of_pulse)>time_of_break]
			if time_of_break>-1.5:
				force_reference_after_shot = True
		elif time_of_break>time_shot_max:
			laser_counts_corrected[0] = laser_counts_corrected[0][((time_of_experiment_digitizer_ID[0]-time_of_experiment[0])*1e-6-start_time_of_pulse)<time_of_break]
			time_of_experiment_digitizer_ID[0] = time_of_experiment_digitizer_ID[0][((time_of_experiment_digitizer_ID[0]-time_of_experiment[0])*1e-6-start_time_of_pulse)<time_of_break]
			laser_counts_corrected[1] = laser_counts_corrected[1][((time_of_experiment_digitizer_ID[1]-time_of_experiment[0])*1e-6-start_time_of_pulse)<time_of_break]
			time_of_experiment_digitizer_ID[1] = time_of_experiment_digitizer_ID[1][((time_of_experiment_digitizer_ID[1]-time_of_experiment[0])*1e-6-start_time_of_pulse)<time_of_break]
			print('data cut because of hole after the pulse')


	# first I want to make sure that the number of frames for the 2 digitisers are the same
	len_dig0 = len(time_of_experiment_digitizer_ID[0])
	len_dig1 = len(time_of_experiment_digitizer_ID[1])
	if len_dig0>len_dig1:
		while len_dig0>len_dig1:
			print('digitiser 0 shortened of 1 frame')
			time_of_experiment_digitizer_ID[0] = time_of_experiment_digitizer_ID[0][:-1]
			laser_counts_corrected[0] = laser_counts_corrected[0][:-1]
			len_dig0 = len(time_of_experiment_digitizer_ID[0])
	if len_dig0<len_dig1:
		while len_dig0<len_dig1:
			print('digitiser 1 shortened of 1 frame')
			time_of_experiment_digitizer_ID[1] = time_of_experiment_digitizer_ID[1][:-1]
			laser_counts_corrected[1] = laser_counts_corrected[1][:-1]
			len_dig1 = len(time_of_experiment_digitizer_ID[1])

	print(np.shape(laser_counts_corrected))
	from scipy.ndimage import generic_filter
	# shot_list = get_data('/home/ffederic/work/irvb/MAST-U/'+'shot_list2.ods')
	# temp1 = (np.array(shot_list['Sheet1'][0])=='shot number').argmax()
	# scenario = ''
	# try:
	# 	for i in range(1,len(shot_list['Sheet1'])):
	# 		if shot_list['Sheet1'][i][temp1] == int(laser_to_analyse[-9:-4]):
	# 			scenario = shot_list['Sheet1'][i][(np.array(shot_list['Sheet1'][0])=='Scenario').argmax()]
	# 			break
	# except:
	# 	pass
	scenario = retrive_shot_scenario(laser_to_analyse[-9:-4])

	# max_ROI = [[0,255],[0,319]]
	max_ROI = [[0,height-1],[0,width-1]]
	# foil_position_dict = dict([('angle',0.5),('foilcenter',[158,136]),('foilhorizw',0.09),('foilvertw',0.07),('foilhorizwpixel',241)])	# fixed orientation, for now, this is from 2021-06-04/44168
	temp_ref_counts = []
	temp_ref_counts_std = []
	temp_counts_minus_background = []
	counts_std = []
	time_partial = []
	timesteps = np.inf
	for i in range(len(laser_digitizer_ID)):
		time_of_experiment_digitizer_ID_seconds = (time_of_experiment_digitizer_ID[i]-time_of_experiment[0])*1e-6-start_time_of_pulse
		if external_clock_marker:
			time_of_experiment_digitizer_ID_seconds = time_of_experiment_digitizer_ID_seconds-np.mean(aggregated_correction_coefficients[:,4])	# I use the mean of the coefficients because I want to avoid small unpredictable differences between the digitisers

		if laser_framerate>60:	# this is to try to see something for the shots in which the framerate is 50Hz
			# basic smoothing
			spectra_orig=np.fft.fft(np.mean(laser_counts_corrected[i],axis=(-1,-2)))
			magnitude = 2 * np.abs(spectra_orig) / len(spectra_orig)
			freq = np.fft.fftfreq(len(magnitude), d=np.mean(np.diff(time_of_experiment_digitizer_ID_seconds)))
			magnitude = np.array([y for _, y in sorted(zip(freq, magnitude))])
			freq = np.sort(freq)
			magnitude_smooth = generic_filter(np.log(magnitude),np.median,size=[7])
			peak_oscillation = (magnitude-np.exp(magnitude_smooth))[np.logical_and(freq>20,freq<50)].argmax()
			peak_oscillation_freq = freq[np.logical_and(freq>20,freq<50)][peak_oscillation]
			if False:
				select = np.zeros_like(laser_counts[0][0]).astype(bool)
				select[:25]=True
				select[-25:]=True
				select[:,:25]=True
				select[:,-25:]=True
				for i in range(30):
					gna = laser_counts[0][300*i:434*2+300*(i)][:,select]
					spectra_orig=np.fft.fft(np.mean(gna-np.mean(gna,axis=0),axis=(-1)))
					magnitude = 2 * np.abs(spectra_orig) / len(spectra_orig)
					freq = np.fft.fftfreq(len(magnitude), d=1/(383/2))
					plt.plot(freq,magnitude,'--')

		else:
			# peak_oscillation_freq = freq[np.logical_and(freq>1,freq<2)][peak_oscillation]	# I should do this, but it removes too much signal
			peak_oscillation_freq=29	# Hz	laser_framerate/2	# dunno why it was laser_framerate/2 before, I cannot follow tthe logic now
		frames_to_average = max(1,1/peak_oscillation_freq/np.mean(np.diff(time_of_experiment_digitizer_ID_seconds)))
		if False:	# avoided here, as it should be done with the temp and not the counts
			laser_counts_corrected_filtered = real_mean_filter_agent(laser_counts_corrected[i],frames_to_average)
		else:
			laser_counts_corrected_filtered = cp.deepcopy(laser_counts_corrected[i])

		if flag_use_of_first_frames_as_reference:
			# temp_ref_counts.append(np.mean(laser_counts_corrected_filtered[time_of_experiment_digitizer_ID_seconds<0],axis=0))
			temp_ref_counts.append(np.mean(laser_counts_corrected_filtered[np.logical_and(time_of_experiment_digitizer_ID_seconds<-0.5,time_of_experiment_digitizer_ID_seconds>time_of_experiment_digitizer_ID_seconds.min()+0.5)],axis=0))
			# temp_ref_counts_std.append(np.std(laser_counts_corrected_filtered[np.logical_and(time_of_experiment_digitizer_ID_seconds<-0.5,time_of_experiment_digitizer_ID_seconds>time_of_experiment_digitizer_ID_seconds.min()+0.5)],axis=0))
			# the previous calculation does not take into account the fact thta I calculate the average over a given duration!
			temp_ref_counts_std.append((1/np.sum(np.logical_and(time_of_experiment_digitizer_ID_seconds<-0.5,time_of_experiment_digitizer_ID_seconds>time_of_experiment_digitizer_ID_seconds.min()+0.5))**0.5)*np.std(laser_counts_corrected_filtered[np.logical_and(time_of_experiment_digitizer_ID_seconds<-0.5,time_of_experiment_digitizer_ID_seconds>time_of_experiment_digitizer_ID_seconds.min()+0.5)],axis=0))
		else:
			temp_ref_counts.append(np.mean(laser_counts_corrected_filtered[-int(seconds_for_reference_frame*laser_framerate/len(laser_digitizer_ID))+np.abs(time_of_experiment_digitizer_ID_seconds-40).argmin():+np.abs(time_of_experiment_digitizer_ID_seconds-40).argmin()],axis=0))	# for framerate=50 I avoid it considers too late times
			# temp_ref_counts_std.append(np.std(laser_counts_corrected_filtered[-int(seconds_for_reference_frame*laser_framerate/len(laser_digitizer_ID))+np.abs(time_of_experiment_digitizer_ID_seconds-40).argmin():+np.abs(time_of_experiment_digitizer_ID_seconds-40).argmin()],axis=0))
			# the previous calculation does not take into account the fact thta I calculate the average over a given duration!
			temp_ref_counts_std.append((1/int(seconds_for_reference_frame*laser_framerate/len(laser_digitizer_ID))**0.5)*np.std(laser_counts_corrected_filtered[-int(seconds_for_reference_frame*laser_framerate/len(laser_digitizer_ID))+np.abs(time_of_experiment_digitizer_ID_seconds-40).argmin():+np.abs(time_of_experiment_digitizer_ID_seconds-40).argmin()],axis=0))
		select_time = np.logical_and(time_of_experiment_digitizer_ID_seconds>=time_shot_min,time_of_experiment_digitizer_ID_seconds<=time_shot_max)
		temp_counts_minus_background.append(laser_counts_corrected_filtered[select_time]-temp_ref_counts[-1])
		counts_std.append(estimate_counts_std(laser_counts_corrected_filtered[select_time],int_time=laser_int_time/1000))	# no averaging done at the counts level, so original noise remains
		time_partial.append(time_of_experiment_digitizer_ID_seconds[select_time])
		timesteps = min(timesteps,len(temp_counts_minus_background[-1]))

	for i in range(len(laser_digitizer_ID)):	# this is also a way to have 2 digitizers with the same size. redundant but I leave it
		temp_counts_minus_background[i] = temp_counts_minus_background[i][:timesteps]
		counts_std[i] = counts_std[i][:timesteps]
		time_partial[i] = time_partial[i][:timesteps]
	if False:	# this is averaging among the 2 digitizers, and that is wrong!!
		temp_counts_minus_background = np.nanmean(temp_counts_minus_background,axis=0)
		counts_std = (1/len(laser_digitizer_ID))*np.nansum(np.array(counts_std)**2,axis=0)**0.5
		temp_ref_counts = np.nanmean(temp_ref_counts,axis=0)
		temp_ref_counts_std = (1/len(laser_digitizer_ID))*np.nansum(np.array(temp_ref_counts_std)**2,axis=0)**0.5
		FAST_counts_minus_background_crop_time = np.nanmean(time_partial,axis=0)
	else:
		FAST_temp_counts_minus_background = np.nanmean(temp_counts_minus_background,axis=0)
		FAST_temp_ref_counts = np.nanmean(temp_ref_counts,axis=0)
		FAST_temp_ref_counts_std = (1/len(laser_digitizer_ID))*np.nansum(np.array(temp_ref_counts_std)**2,axis=0)**0.5
		FAST_counts_minus_background_crop_time = np.nanmean(time_partial,axis=0)

	# I'm going to use the reference frames for foil position
	foilrotdeg,out_of_ROI_mask,foildw,foilup,foillx,foilrx = get_rotation_crop_parameters(FAST_temp_ref_counts,foil_position_dict,laser_to_analyse,FAST_temp_counts_minus_background,FAST_counts_minus_background_crop_time)

	# rotation and crop
	# temp_counts_minus_background_rot=rotate(temp_counts_minus_background,foilrotdeg,axes=(-1,-2))
	# if not (height==max_ROI[0][1]+1 and width==max_ROI[1][1]+1):
	# 	temp_counts_minus_background_rot*=out_of_ROI_mask
	# 	temp_counts_minus_background_rot[np.logical_and(temp_counts_minus_background_rot<np.nanmin(temp_counts_minus_background),temp_counts_minus_background_rot>np.nanmax(temp_counts_minus_background))]=0
	# FAST_counts_minus_background_crop = temp_counts_minus_background_rot[:,foildw:foilup,foillx:foilrx]
	FAST_counts_minus_background_crop = rotate_and_crop_3D(FAST_temp_counts_minus_background,foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx)
	original_counts_shape = FAST_counts_minus_background_crop.shape

	# I drop this to save memory
	# temp = FAST_counts_minus_background_crop[:,:,:int(np.shape(FAST_counts_minus_background_crop)[2]*0.75)]
	# temp = np.sort(temp[np.max(temp,axis=(1,2)).argmax()].flatten())
	# ani,efit_reconstruction = movie_from_data(np.array([np.flip(np.transpose(FAST_counts_minus_background_crop,(0,2,1)),axis=2)]), laser_framerate/len(laser_digitizer_ID),timesteps=FAST_counts_minus_background_crop_time,integration=laser_int_time/1000,time_offset=FAST_counts_minus_background_crop_time[0],extvmin=0,extvmax=np.nanmean(temp[-len(temp)//60:]),xlabel='horizontal coord [pixels]', ylabel='vertical coord [pixels]',barlabel='Count increase [au]', prelude='shot ' + laser_to_analyse[-9:-4]+'\n',overlay_structure=True,include_EFIT=True,pulse_ID=laser_to_analyse[-9:-4],overlay_x_point=True,overlay_mag_axis=True,overlay_strike_points=True,overlay_separatrix=True,EFIT_output_requested=True)
	# ani.save(laser_to_analyse[:-4]+ '_FAST_count_increase.mp4', fps=5*laser_framerate/len(laser_digitizer_ID)/383, writer='ffmpeg',codec='mpeg4')
	# plt.close('all')

	print('completed FAST count rotating/cropping ' + laser_to_analyse + ' in %.3gmin from calling MASTU_pulse_process_FAST3_BB' %((tm.time()-start_processing_fast)/60))


	# reference foil properties
	# thickness = 1.4859095354482858e-06
	# emissivity = 0.9884061389741369
	# diffusivity = 1.045900223180454e-05
	# # from 2021/09/17, Laser_data_analysis3_3.py
	# thickness = 2.0531473351462095e-06
	# emissivity = 0.9999999999999
	# diffusivity = 1.0283685197530968e-05
	# # from 2022/09/29, Laser_data_analysis3_3.py
	# diffusivity = 1.3449081299985357e-05
	# thickness = 2.740001604038667e-06
	# emissivity = 0.9999999320220466

	shrink_factor_t = 1	# int(round(frames_to_average))	# I don't bin anymore because I already averaged with frames_to_average. If I bin it means that I filter twice
	# shrink_factor_x = 3	# with large time averaging this should be enough
	if laser_framerate<60:
		shrink_factor_x = 5	# increase spatial averaging if I have very low framerate
	# shrink_factor_x = 1	# Test. if error is propagated forward properly, binning should only cause a loss of information with no gain. The bayesian part should do something similar to a smoothing of the input data. checked with Chris Bowman
	if pass_number==0:	# I sample less to make the analysis faster for the first loop
		shrink_factor_x = 3
	else:
		shrink_factor_x = 1	# this seems to work well, so I make it permanent
	binning_type = 'bin' + str(np.round(frames_to_average,decimals=2)) + 'x' + str(shrink_factor_x) + 'x' + str(shrink_factor_x)


	Ptthermalconductivity=71.6 #[W/(m·K)]
	zeroC=273.15 #K / C
	sigmaSB=5.6704e-08 #[W/(m2 K4)]
	if False:
		# from 2022/10/14, Laser_data_analysis3_3.py, I separate well the digitizers now
		diffusivity =1.347180363576353e-05
		thickness =2.6854735451543735e-06	# m
		emissivity =0.9999999999

		# 2022/10/14 Laser_data_analysis3_3.py, I separate well the digitizers now
		# these quantities are ok to be relative
		sigma_diffusivity = 0.13893985595434794
		sigma_emissivity =0.08610352405403708
		sigma_thickness =0.1357454922343387
		sigma_thickness_over_diffusivity = (sigma_diffusivity**2 + sigma_thickness**2)**0.5
	elif True:
		# 2023/10/10 Laser_data_analysis3_3.py,
		# foil properties from laser 22 and laser 33, using the sum of the power from both large and small area
		diffusivity = 9.466307489300497e-06
		thickness_over_diffusivity = 0.07502143143067985	# this is the real measurement I obtain
		# thickness = thickness_over_diffusivity*diffusivity	# m
		thickness = 7.120474545364217e-07	# m
		emissivity = 0.6311559794933064

		# I don't have a measurement of anysotropy of the foil, so I use the one given us by the japanese group
		thickness_variability_japanese_measurement = 0.13257174	# np.std(foilthickness)/np.mean(foilthickness)
		emissivity_variability_japanese_measurement = 0.049993	# np.std(foilemissivity)/np.mean(foilemissivity)
		# the uncertainty from before is calculate across the whole foil, but this is not what I actually do.
		# I consider every pixel or binned pixels infividually, so the uncertainty I need to consider regards the difference between one pixel andits immediate neighbour
		# because of this the uncertainty has to be scaled down based on the binning I do.
		# the japanese precision is 1 measurement per 1cm, but in my case my spatial resolution is much better
		# uncertainty_scaling_factor = 9/(original_counts_shape[-1]/shrink_factor_x)	# 9 = np.shape(foilthickness)[-1]
		# thickness_variability_japanese_measurement /= uncertainty_scaling_factor
		# emissivity_variability_japanese_measurement /= uncertainty_scaling_factor
		# print('uncertainty_scaling_factor='+str(uncertainty_scaling_factor))

		if not std_from_pure_noise:
			# 2024/09/25 this is misleading. this is the uncertainty on the thermal properties in one point.
			# when I do the inversion, though, this uncertainty should be propagated only in a Monte Carlo way, not like this. assuming 1 combo of thermal properties,
			# the uncertainty is =0. various combo should calculated, and the total mean and uncertainty calculated in a bayesian way

			# these quantities are ok to be relative
			sigma_diffusivity = 8.366535349083484e-07/diffusivity	# the variability from the japanese measurement assumes constand diffusivity, therefore including also its variability, so i don't need to increase this
			sigma_thickness = 1.0319453471779227e-07/thickness
			if True:
				sigma_thickness_over_diffusivity = 0.009226369537711715/thickness_over_diffusivity	# this is at enuc=1
			else:	# considering that I do not know exactly the NUC plate emissivity, I take as uncertainty the result at Enuc=1 - Enuc=0.7 / Enuc=1
				sigma_thickness_over_diffusivity = 0.02/thickness_over_diffusivity	# this is at enuc=1

			if True:
				sigma_emissivity = 0.08937985599750203/emissivity
			else:	# considering that I do not know exactly the NUC plate emissivity, I take as uncertainty the result at Enuc=1 - Enuc=0.7 / Enuc=1
				sigma_emissivity = 0.15/emissivity
		else:
			# 2024/09/25 this how the uncertainty should be calculated. this is the uncertainty on the thermal properties in one point.
			# when I do the inversion, though, this uncertainty should be propagated only in a Monte Carlo way, not like this. assuming 1 combo of thermal properties,
			# the uncertainty is =0. various combo should calculated, and the total mean and uncertainty calculated in a bayesian way
			sigma_diffusivity = 0
			sigma_thickness = 0
			sigma_emissivity = 0
			sigma_thickness_over_diffusivity = 0
			thickness_variability_japanese_measurement = 0
			emissivity_variability_japanese_measurement = 0
		# I want the sigma on diffusivity to be reduced, so that the emissivity of thickness_over_diffusivity comes back to be the one I measured
		sigma_diffusivity = (max(0,sigma_thickness_over_diffusivity**2-sigma_thickness**2))**0.5
		sigma_thickness = (sigma_thickness**2+thickness_variability_japanese_measurement**2)**0.5
		sigma_emissivity = (sigma_emissivity**2 + emissivity_variability_japanese_measurement**2)**0.5
		# Note: the uncertainty that I should consider here is a randomized uncertainty among adjacent pixels in space and time.
		# The differences in foil properties point to point (neglecting pinholes, that cannot anyway be accounted for) are slowly varying
		# as they have been measured by Byron et al. over distances of 1cm. this will translate a much lower variation in terms of noise one pixel to the other.
		# The difference from Byron will translate to a systematic error, not a randomised one
	sigma_rec_diffusivity = sigma_diffusivity

	NUC_plate_emissivity = 1


	foil_properties_used = dict([])
	foil_properties_used['thickness'] = thickness
	foil_properties_used['thickness_unit'] = 'm'
	foil_properties_used['emissivity'] = emissivity
	foil_properties_used['emissivity_unit'] = 'au'
	foil_properties_used['diffusivity'] = diffusivity
	foil_properties_used['diffusivity_unit'] = 'm2/s'
	foil_properties_used['thermalconductivity'] = Ptthermalconductivity
	foil_properties_used['thermalconductivity_unit'] = 'W/(m·K)'

	foil_properties_used['sigma_emissivity'] = sigma_emissivity
	foil_properties_used['sigma_thickness'] = sigma_thickness
	foil_properties_used['sigma_rec_diffusivity'] = sigma_rec_diffusivity
	foil_properties_used['sigma_thermalconductivity'] = 0
	foil_properties_used['sigma_thickness_over_diffusivity'] = sigma_thickness_over_diffusivity

	# params_BB[:,:,:,0] *= emissivity
	# errparams_BB[:,:,:,0,:] *= emissivity
	# errparams_BB[:,:,:,:,0] *= emissivity
	params_BB_int = cp.deepcopy(params_BB)
	errparams_BB_int = cp.deepcopy(errparams_BB)
	params_BB_int[:,:,:,0] *= emissivity/NUC_plate_emissivity
	errparams_BB_int[:,:,:,0,:] *= emissivity/NUC_plate_emissivity
	errparams_BB_int[:,:,:,:,0] *= emissivity/NUC_plate_emissivity
	errparams_BB_int[:,:,:,0,0] += (params_BB_int[:,:,:,0]*sigma_emissivity)**2
	# I don't think the following lines are necessary. the proportional coeffs are multiplicative, and I added the contribution of sigma_emissivity already to the first proportional component
	# if params_BB_int.shape[-1]>2:	# the only coefficient that matters is the other proportional one
	# 	errparams_BB_int[:,:,:,0,2] += (params_BB_int[:,:,:,0]*emissivity/NUC_plate_emissivity*sigma_emissivity)
	# 	errparams_BB_int[:,:,:,2,0] += (params_BB_int[:,:,:,0]*emissivity/NUC_plate_emissivity*sigma_emissivity)
	BB_proportional *= emissivity/NUC_plate_emissivity
	# BB_proportional_std *= emissivity/NUC_plate_emissivity
	BB_proportional_std = ((BB_proportional_std*emissivity/NUC_plate_emissivity)**2 + (BB_proportional*sigma_emissivity)**2)**0.5

	averaged_params = np.mean(params,axis=(0))
	averaged_errparams = np.mean(errparams,axis=(0))
	averaged_params_BB = np.mean(params_BB_int,axis=(0))
	averaged_errparams_BB = np.mean(errparams_BB_int,axis=(0))
	averaged_BB_proportional = np.mean(BB_proportional,axis=(0))
	averaged_BB_proportional_std = np.mean(BB_proportional_std,axis=(0))
	counts = [temp_counts_minus_background[i]+temp_ref_counts[i] for i in range(len(laser_digitizer_ID))]
	# counts_std = estimate_counts_std(counts,laser_framerate)
	temperature_ref = averaged_params[:,:,-1] + averaged_params[:,:,-2] * FAST_temp_ref_counts + averaged_params[:,:,-3] * (FAST_temp_ref_counts**2)
	temperature_ref_std = (averaged_errparams[:,:,2,2] + (FAST_temp_ref_counts_std**2)*(averaged_params[:,:,1]**2) + (FAST_temp_ref_counts**2+FAST_temp_ref_counts_std**2)*averaged_errparams[:,:,1,1] + (FAST_temp_ref_counts_std**2)*(4*FAST_temp_ref_counts**2+3*FAST_temp_ref_counts_std**2)*(averaged_params[:,:,0]**2) + (FAST_temp_ref_counts**4+6*((FAST_temp_counts_minus_background+FAST_temp_ref_counts)**2)*(FAST_temp_ref_counts_std**2)+3*FAST_temp_ref_counts_std**4)*averaged_errparams[:,:,0,0] + 2*FAST_temp_ref_counts*averaged_errparams[:,:,2,1] + 2*(FAST_temp_ref_counts**2+FAST_temp_ref_counts_std**2)*averaged_errparams[:,:,2,0] + 2*(FAST_temp_ref_counts**3+FAST_temp_ref_counts*(FAST_temp_ref_counts_std**2))*averaged_errparams[:,:,1,0])**0.5

	ref_temperature = retrive_vessel_average_temp_archve(int(laser_to_analyse[-9:-4]))
	if not std_from_pure_noise:	# 2024/09/25 This is also something that should not be done here and used in a bayesian sense
		ref_temperature_std = 0.25	# coming from the fact that there is no noise during transition, so the std must be quite below 1K
	else:
		ref_temperature_std = 0

	if np.isnan(ref_temperature) and pass_number<2:	# I accept this for the first pass, but for the second I want the tue vessel temperature
		ref_temperature = 25	# np.mean(temperature_ref)	# 2023/06/27 modified because I verified that it is not trustworthy to use the camera as thermometer, it's good only for differences
		ref_temperature_std = 1	# (np.sum(np.array(temperature_ref_std)**2)**0.5 / len(np.array(temperature_ref_std).flatten()))
	elif np.isnan(ref_temperature) and pass_number>1:
		print('for the second pass I want the true vessel temperature but the reading failed\nwrong shot number of pyuda fails')
		bla = sga	# for the second pass I want the true vessel temperature but the reading failed\nwrong shot number of pyuda fails

	# this step takes care of doing the proper background substraction
	if False:
		temperature,temperature_std = count_to_temp_BB_multi_digitizer(np.array([counts]),np.array([averaged_params_BB]),np.array([averaged_errparams_BB]),[0],reference_background=np.array([temp_ref_counts]),reference_background_std=np.array([temp_ref_counts_std]),ref_temperature=ref_temperature,ref_temperature_std=ref_temperature_std,wavewlength_top=wavewlength_top,wavelength_bottom=wavelength_bottom,inttime=laser_int_time/1000)
		temperature = temperature[0]
		temperature_std = temperature_std[0]
		temperature_minus_background = temperature - ref_temperature
		temperature_minus_background_std = (temperature_std**2 + ref_temperature_std**2)**0.5
	else:
		temperature,temperature_std = count_to_temp_BB_multi_digitizer(counts,params_BB_int,errparams_BB_int,laser_digitizer_ID,counts_std=counts_std,reference_background=temp_ref_counts,reference_background_std=temp_ref_counts_std,ref_temperature=ref_temperature,ref_temperature_std=ref_temperature_std,wavewlength_top=wavewlength_top,wavelength_bottom=wavelength_bottom,inttime=laser_int_time/1000)
		# added the filter here 2022/10/01

		# 2024/09/24 with ref_temperature_std = 0 and foil properties uncertainty =0 the estimated uncertainty (std) is 0.0218K whine looling at the data itself is 0.0215(temporal)/0.0231(spatial), so very close
		temperature_minus_background_unfiltered = cp.deepcopy(temperature)
		temperature = np.array([real_mean_filter_agent(temperature[i],frames_to_average) for i in range(len(laser_digitizer_ID))])
		# it is abit weird to filter while still binning later, but the 2 can, and are, not correlated, I do it of a different amount, so it still make sense
		# I don't think it does, actually. this is just like the running binning I did develop. If I bin again I effectively filter twice

		if std_from_pure_noise:
			# 2024/09/24 after filtering of 6.1 frames the noise (temperature) reduces from 0.0215 to ~0.006 (temporal) instead of theoretical 0.00869
			# this because the noise on time is NOT gaussian but coherent (see the constant oscillation) so averaging exactly over that has a much larger effect
			# and from 0.0231 to ~0.011 (spatial)
			# I think the best is to keep the 0.00869 estimate and that's it
			temperature_std /= frames_to_average**0.5

		counts = np.array([real_mean_filter_agent(counts[i],frames_to_average) for i in range(len(laser_digitizer_ID))])

		if std_from_pure_noise:
			# 2024/09/24 I'll do the same for the counts
			counts_std /= frames_to_average**0.5

		temperature_minus_background = [temperature[i] - ref_temperature for i in range(len(laser_digitizer_ID))]
		temperature_minus_background_unfiltered = [temperature_minus_background_unfiltered[i] - ref_temperature for i in range(len(laser_digitizer_ID))]
		if False:	# 2024/10/01 currently I don't even use this, so no need to calculate it
			if not std_from_pure_noise:
				# the next operetion DOES NOT keep into account the time filtering. this will be considered all at once later with the spatial binning
				# # the following formula is wrong, the uncertainty in temperature has LESS uncertainty than the absolute temp
				# temperature_minus_background_std = [(temperature_std[i]**2 + ref_temperature_std**2)**0.5 for i in range(len(laser_digitizer_ID))]
				# # the following actually makes sense, removing the error from ref_temperature from from the std
				# temperature_minus_background_std = [(temperature_std[i]**2 - ref_temperature_std**2 - (temp_ref_counts_std[i]/(photon_flux_over_temperature_interpolator(ref_temperature)*BB_proportional[i]))**2)**0.5 for i in range(len(laser_digitizer_ID))]
				# 2024/09/24 I restructured the error estimation changing how the effects of the interpolateros is calculated, so I think I should remove that bit
				temperature_minus_background_std = [(temperature_std[i]**2 - ref_temperature_std**2)**0.5 for i in range(len(laser_digitizer_ID))]
			else:
				# 2024/09/24 if I don't consider the error in ref_temperature (as i should not if I account things properly bayesianly) then the error on the absolute is same as error on relative
				temperature_minus_background_std = cp.deepcopy(temperature_std)

	if pass_number==0:	# I move this before the binning to save time, it's formally the same
		temp = FAST_counts_minus_background_crop[::int(round(frames_to_average))]
		temp = np.array(temp)
		FAST_counts_minus_background_crop = cp.deepcopy(temp)
		temp = []
		for i in range(len(laser_digitizer_ID)):
			temp.append(temperature[i][::int(round(frames_to_average))])
		temp = np.array(temp)
		temperature = cp.deepcopy(temp)
		temp = []
		for i in range(len(laser_digitizer_ID)):
			temp.append(counts[i][::int(round(frames_to_average))])
		temp = np.array(temp)
		counts = cp.deepcopy(temp)
		temp = []
		for i in range(len(laser_digitizer_ID)):
			temp.append(temperature_minus_background[i][::int(round(frames_to_average))])
		temp = np.array(temp)
		temperature_minus_background = cp.deepcopy(temp)


	# rotation and crop
	# temperature_rot=rotate(temperature,foilrotdeg,axes=(-1,-2))
	# temperature_std_rot=rotate(temperature_std,foilrotdeg,axes=(-1,-2))
	# temperature_minus_background_rot=rotate(temperature_minus_background,foilrotdeg,axes=(-1,-2))
	# temperature_minus_background_std_rot=rotate(temperature_minus_background_std,foilrotdeg,axes=(-1,-2))
	# counts_rot=rotate(counts,foilrotdeg,axes=(-1,-2))
	# counts_std_rot=rotate(counts_std,foilrotdeg,axes=(-1,-2))
	# if not (height==max_ROI[0][1]+1 and width==max_ROI[1][1]+1):
	# 	temperature_rot*=out_of_ROI_mask
	# 	temperature_rot[np.logical_and(temperature_rot<np.nanmin(temperature),temperature_rot>np.nanmax(temperature))]=0
	# 	temperature_std_rot*=out_of_ROI_mask
	# 	temperature_std_rot[np.logical_and(temperature_std_rot<np.nanmin(temperature_std),temperature_std_rot>np.nanmax(temperature_std))]=0
	# 	temperature_minus_background_rot*=out_of_ROI_mask
	# 	temperature_minus_background_rot[np.logical_and(temperature_minus_background_rot<np.nanmin(temperature_minus_background),temperature_minus_background_rot>np.nanmax(temperature_minus_background))]=0
	# 	temperature_minus_background_std_rot*=out_of_ROI_mask
	# 	temperature_minus_background_std_rot[np.logical_and(temperature_minus_background_std_rot<np.nanmin(temperature_minus_background_std),temperature_minus_background_std_rot>np.nanmax(temperature_minus_background_std))]=0
	# 	counts_rot*=out_of_ROI_mask
	# 	counts_rot[np.logical_and(counts_rot<np.nanmin(counts),counts_rot>np.nanmax(counts))]=0
	# 	counts_std_rot*=out_of_ROI_mask
	# 	counts_std_rot[np.logical_and(counts_std_rot<np.nanmin(counts_std),counts_std_rot>np.nanmax(counts_std))]=0
	# temperature_crop = temperature_rot[:,foildw:foilup,foillx:foilrx]
	# temperature_std_crop = temperature_std_rot[:,foildw:foilup,foillx:foilrx]
	# temperature_minus_background_crop = temperature_minus_background_rot[:,foildw:foilup,foillx:foilrx]
	# temperature_minus_background_std_crop = temperature_minus_background_std_rot[:,foildw:foilup,foillx:foilrx]
	# counts_crop = counts_rot[:,foildw:foilup,foillx:foilrx]
	# counts_std_crop = counts_std_rot[:,foildw:foilup,foillx:foilrx]
	if False:
		temperature_crop = rotate_and_crop_3D(temperature,foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx)
		temperature_std_crop = rotate_and_crop_3D(temperature_std,foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx)
		temperature_minus_background_crop = rotate_and_crop_3D(temperature_minus_background,foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx)
		temperature_minus_background_std_crop = rotate_and_crop_3D(temperature_minus_background_std,foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx)
		counts_crop = rotate_and_crop_3D(counts,foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx)
		counts_std_crop = rotate_and_crop_3D(counts_std,foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx)
	else:	# the rotation reduces the spatial noise involontarily by ~18%-13%. I neglect it
		temperature_crop = [rotate_and_crop_3D(temperature[i],foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx) for i in range(len(laser_digitizer_ID))]
		temperature_std_crop = [rotate_and_crop_3D(temperature_std[i],foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx) for i in range(len(laser_digitizer_ID))]
		temperature_minus_background_crop = [rotate_and_crop_3D(temperature_minus_background[i],foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx) for i in range(len(laser_digitizer_ID))]
		temperature_minus_background_unfiltered_crop = [rotate_and_crop_3D(temperature_minus_background_unfiltered[i],foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx) for i in range(len(laser_digitizer_ID))]
		if False:	# 2024/10/01 currently I don't even use this, so no need to calculate it
			temperature_minus_background_std_crop = [rotate_and_crop_3D(temperature_minus_background_std[i],foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx) for i in range(len(laser_digitizer_ID))]
		counts_crop = [rotate_and_crop_3D(counts[i],foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx) for i in range(len(laser_digitizer_ID))]
		counts_std_crop = [rotate_and_crop_3D(counts_std[i],foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx) for i in range(len(laser_digitizer_ID))]


	# this is only for checking at disruptions. it will be heavily effected by noise
	temperature_minus_background_unfiltered_crop_dt_time = np.array(time_partial)
	temperature_minus_background_unfiltered_crop_dt_time = (temperature_minus_background_unfiltered_crop_dt_time[:,1:]+temperature_minus_background_unfiltered_crop_dt_time[:,:-1])/2
	temperature_minus_background_unfiltered_crop_dt_time = np.mean(temperature_minus_background_unfiltered_crop_dt_time,axis=0)
	# temperature_minus_background_crop_dt = np.mean(np.diff(temperature_minus_background_crop,axis=1),axis=0).astype(np.float16)
	temperature_minus_background_unfiltered_crop = np.array(temperature_minus_background_unfiltered_crop).astype(np.float32)
	temperature_minus_background_unfiltered_crop_dt = ((foil_properties_used['thickness'] * foil_properties_used['thermalconductivity'] / foil_properties_used['diffusivity']) * np.mean(np.diff(temperature_minus_background_unfiltered_crop,axis=1),axis=0)/np.median(np.diff(temperature_minus_background_unfiltered_crop_dt_time))/1000).astype(np.float16)
	temperature_minus_background_unfiltered_crop_dt[:,1:-1,1:-1] -= ((foil_properties_used['thickness'] * foil_properties_used['thermalconductivity']) * np.mean(temperature_minus_background_unfiltered_crop[:,1:,0:-2,1:-1]-2*temperature_minus_background_unfiltered_crop[:,1:,1:-1,1:-1]+temperature_minus_background_unfiltered_crop[:,1:,2:,1:-1],axis=0)/((foil_position_dict['foilhorizw']/foil_position_dict['foilhorizwpixel'])**2)/1000).astype(np.float16)
	temperature_minus_background_unfiltered_crop_dt[:,1:-1,1:-1] -= ((foil_properties_used['thickness'] * foil_properties_used['thermalconductivity']) * np.mean(temperature_minus_background_unfiltered_crop[:,1:,1:-1,0:-2]-2*temperature_minus_background_unfiltered_crop[:,1:,1:-1,1:-1]+temperature_minus_background_unfiltered_crop[:,1:,1:-1,2:],axis=0)/((foil_position_dict['foilhorizw']/foil_position_dict['foilhorizwpixel'])**2)/1000).astype(np.float16)
	sigmaSB=5.6704e-08 #[W/(m2 K4)]
	temperature_minus_background_unfiltered_crop_dt += ((2* sigmaSB* foil_properties_used['emissivity']) * np.mean((temperature_minus_background_unfiltered_crop[:,1:]+ref_temperature+273)**4 - (ref_temperature+273)**4,axis=0)/1000).astype(np.float16)
	# as it is now this is no more the actualt power  -->> in kW <<--

	# rotation and crop
	# temp_ref_counts_rot=rotate(temp_ref_counts,foilrotdeg,axes=(-1,-2))
	# temp_ref_counts_std_rot=rotate(temp_ref_counts_std,foilrotdeg,axes=(-1,-2))
	# averaged_BB_proportional_rot=rotate(averaged_BB_proportional,foilrotdeg,axes=(-1,-2))
	# averaged_BB_proportional_std_rot=rotate(averaged_BB_proportional_std,foilrotdeg,axes=(-1,-2))
	# if not (height==max_ROI[0][1]+1 and width==max_ROI[1][1]+1):
	# 	temp_ref_counts_rot*=out_of_ROI_mask
	# 	temp_ref_counts_rot[np.logical_and(temp_ref_counts_rot<np.nanmin(temp_ref_counts),temp_ref_counts_rot>np.nanmax(temp_ref_counts))]=0
	# 	temp_ref_counts_std_rot*=out_of_ROI_mask
	# 	temp_ref_counts_std_rot[np.logical_and(temp_ref_counts_std_rot<np.nanmin(temp_ref_counts_std),temp_ref_counts_std_rot>np.nanmax(temp_ref_counts_std))]=0
	# 	averaged_BB_proportional_rot*=out_of_ROI_mask
	# 	averaged_BB_proportional_rot[np.logical_and(averaged_BB_proportional_rot<np.nanmin(averaged_BB_proportional),averaged_BB_proportional_rot>np.nanmax(averaged_BB_proportional))]=0
	# 	averaged_BB_proportional_std_rot*=out_of_ROI_mask
	# 	averaged_BB_proportional_std_rot[np.logical_and(averaged_BB_proportional_std_rot<np.nanmin(averaged_BB_proportional_std),averaged_BB_proportional_std_rot>np.nanmax(averaged_BB_proportional_std))]=0
	# temp_ref_counts_crop = temp_ref_counts_rot[foildw:foilup,foillx:foilrx]
	# temp_ref_counts_std_crop = temp_ref_counts_std_rot[foildw:foilup,foillx:foilrx]
	# averaged_BB_proportional_crop = averaged_BB_proportional_rot[foildw:foilup,foillx:foilrx]
	# averaged_BB_proportional_std_crop = averaged_BB_proportional_std_rot[foildw:foilup,foillx:foilrx]
	if False:
		temp_ref_counts_crop = rotate_and_crop_2D(temp_ref_counts,foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx)
		temp_ref_counts_std_crop = rotate_and_crop_2D(temp_ref_counts_std,foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx)
		averaged_BB_proportional_crop = rotate_and_crop_2D(averaged_BB_proportional,foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx)
		averaged_BB_proportional_std_crop = rotate_and_crop_2D(averaged_BB_proportional_std,foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx)
	else:
		temp_ref_counts_crop = [rotate_and_crop_2D(temp_ref_counts[i],foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx) for i in range(len(laser_digitizer_ID))]
		temp_ref_counts_std_crop = [rotate_and_crop_2D(temp_ref_counts_std[i],foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx) for i in range(len(laser_digitizer_ID))]
		BB_proportional_crop = [rotate_and_crop_2D(BB_proportional[i],foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx) for i in range(len(laser_digitizer_ID))]
		BB_proportional_std_crop = [rotate_and_crop_2D(BB_proportional_std[i],foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx) for i in range(len(laser_digitizer_ID))]

	# shrink_factor_t = 1	# int(round(frames_to_average))	# I don't bin anymore because I already averaged with frames_to_average. If I bin it means that I filter twice
	# # shrink_factor_x = 3	# with large time averaging this should be enough
	# if laser_framerate<60:
	# 	shrink_factor_x = 5	# increase spatial averaging if I have very low framerate
	# # shrink_factor_x = 1	# Test. if error is propagated forward properly, binning should only cause a loss of information with no gain. The bayesian part should do something similar to a smoothing of the input data. checked with Chris Bowman
	# if pass_number==0:	# I sample less to make the analysis faster for the first loop
	# 	shrink_factor_x = 3
	# else:
	# 	shrink_factor_x = 1	# this seems to work well, so I make it permanent
	# binning_type = 'bin' + str(np.round(frames_to_average,decimals=2)) + 'x' + str(shrink_factor_x) + 'x' + str(shrink_factor_x)

	if False:
		FAST_counts_minus_background_crop_binned,nan_ROI_mask = proper_homo_binning_t_2D(FAST_counts_minus_background_crop,shrink_factor_t,shrink_factor_x)	# this is NOT smoothed in time
		temperature_crop_binned,nan_ROI_mask = proper_homo_binning_t_2D(temperature_crop,shrink_factor_t,shrink_factor_x)
		# temperature_std_add_crop_binned = proper_homo_binning_t_2D(temperature_crop,shrink_factor_t,shrink_factor_x,type='np.nanstd')[0]
		temperature_std_crop_binned = 1/(shrink_factor_t*shrink_factor_x**2)*(proper_homo_binning_t_2D(temperature_std_crop**2,shrink_factor_t,shrink_factor_x,type='np.nansum')[0]**0.5)
		# temperature_std_crop_binned = (temperature_std_crop_binned**2 + temperature_std_add_crop_binned**2)**0.5
		counts_crop_binned,trash = proper_homo_binning_t_2D(counts_crop,shrink_factor_t,shrink_factor_x)
		# counts_std_add_crop_binned = proper_homo_binning_t_2D(counts_crop,shrink_factor_t,shrink_factor_x,type='np.nanstd')[0]
		counts_std_crop_binned = 1/(shrink_factor_t*shrink_factor_x**2)*(proper_homo_binning_t_2D(counts_std_crop**2,shrink_factor_t,shrink_factor_x,type='np.nansum')[0]**0.5)
		# counts_std_crop_binned = (counts_std_crop_binned**2 + counts_std_add_crop_binned**2)**0.5
		temperature_minus_background_crop_binned,nan_ROI_mask = proper_homo_binning_t_2D(temperature_minus_background_crop,shrink_factor_t,shrink_factor_x)
		# temperature_minus_background_std_add_crop_binned = proper_homo_binning_t_2D(temperature_minus_background_crop,shrink_factor_t,shrink_factor_x,type='np.nanstd')[0]
		temperature_minus_background_std_crop_binned = 1/(shrink_factor_t*shrink_factor_x**2)*(proper_homo_binning_t_2D(temperature_minus_background_std_crop**2,shrink_factor_t,shrink_factor_x,type='np.nansum')[0]**0.5)
		# temperature_minus_background_std_crop_binned = (temperature_minus_background_std_crop_binned**2 + temperature_minus_background_std_add_crop_binned**2)**0.5
		# temperature_ref_crop_binned = proper_homo_binning_2D(temperature_ref_crop,shrink_factor_x)
		# temp_ref_counts_std_add_crop_binned = proper_homo_binning_2D(temp_ref_counts_crop,shrink_factor_x,type='np.nanstd')
		temp_ref_counts_std_crop_binned = 1/((shrink_factor_t**0.5)*shrink_factor_x**2)*(proper_homo_binning_2D(temp_ref_counts_std_crop**2,shrink_factor_x,type='np.nansum')**0.5)
		# temp_ref_counts_std_crop_binned = (temp_ref_counts_std_crop_binned**2  + temp_ref_counts_std_add_crop_binned**2)**0.5
		averaged_BB_proportional_crop_binned = proper_homo_binning_2D(averaged_BB_proportional_crop,shrink_factor_x)
		# averaged_BB_proportional_std_add_crop_binned = proper_homo_binning_2D(averaged_BB_proportional_crop,shrink_factor_x,type='np.nanstd')[0]
		averaged_BB_proportional_std_crop_binned = 1/(shrink_factor_x**2)*(proper_homo_binning_2D(averaged_BB_proportional_std_crop**2,shrink_factor_x,type='np.nansum')**0.5)
		# averaged_BB_proportional_std_crop_binned = (averaged_BB_proportional_std_crop_binned**2 + averaged_BB_proportional_std_add_crop_binned**2)**0.5
		time_binned = proper_homo_binning_t(FAST_counts_minus_background_crop_time,shrink_factor_t)
	else:
		FAST_counts_minus_background_crop_binned,nan_ROI_mask = proper_homo_binning_t_2D(FAST_counts_minus_background_crop,shrink_factor_t,shrink_factor_x)
		temperature_crop_binned = [proper_homo_binning_t_2D(temperature_crop[i],shrink_factor_t,shrink_factor_x)[0] for i in range(len(laser_digitizer_ID))]
		# temperature_std_add_crop_binned = proper_homo_binning_t_2D(temperature_crop,shrink_factor_t,shrink_factor_x,type='np.nanstd')[0]
		if not std_from_pure_noise:
			temperature_std_crop_binned = [1/(frames_to_average*shrink_factor_x**2)*(proper_homo_binning_t_2D(temperature_std_crop[i]**2,np.round(frames_to_average).astype(int),shrink_factor_x,type='np.nansum',running_time=True)[0]**0.5) for i in range(len(laser_digitizer_ID))]
		else:
			temperature_std_crop_binned = [1/(shrink_factor_x**2)*(proper_homo_binning_t_2D(temperature_std_crop[i]**2,1,shrink_factor_x,type='np.nansum',running_time=True)[0]**0.5) for i in range(len(laser_digitizer_ID))]
		# temperature_std_crop_binned = (temperature_std_crop_binned**2 + temperature_std_add_crop_binned**2)**0.5
		counts_crop_binned = [proper_homo_binning_t_2D(counts_crop[i],shrink_factor_t,shrink_factor_x)[0] for i in range(len(laser_digitizer_ID))]
		# counts_std_add_crop_binned = proper_homo_binning_t_2D(counts_crop,shrink_factor_t,shrink_factor_x,type='np.nanstd')[0]
		if not std_from_pure_noise:
			counts_std_crop_binned = [1/(frames_to_average*shrink_factor_x**2)*(proper_homo_binning_t_2D(counts_std_crop[i]**2,np.round(frames_to_average).astype(int),shrink_factor_x,type='np.nansum',running_time=True)[0]**0.5) for i in range(len(laser_digitizer_ID))]
		else:
			counts_std_crop_binned = [1/(shrink_factor_x**2)*(proper_homo_binning_t_2D(counts_std_crop[i]**2,1,shrink_factor_x,type='np.nansum',running_time=True)[0]**0.5) for i in range(len(laser_digitizer_ID))]
		# counts_std_crop_binned = (counts_std_crop_binned**2 + counts_std_add_crop_binned**2)**0.5
		temperature_minus_background_crop_binned = [proper_homo_binning_t_2D(temperature_minus_background_crop[i],shrink_factor_t,shrink_factor_x)[0] for i in range(len(laser_digitizer_ID))]
		# temperature_minus_background_std_add_crop_binned = proper_homo_binning_t_2D(temperature_minus_background_crop,shrink_factor_t,shrink_factor_x,type='np.nanstd')[0]
		if False:	# 2024/10/01 currently I don't even use this, so no need to calculate it
			if not std_from_pure_noise:
				temperature_minus_background_std_crop_binned = [1/(frames_to_average*shrink_factor_x**2)*(proper_homo_binning_t_2D(temperature_minus_background_std_crop[i]**2,np.round(frames_to_average).astype(int),shrink_factor_x,type='np.nansum',running_time=True)[0]**0.5) for i in range(len(laser_digitizer_ID))]
			else:
				temperature_minus_background_std_crop_binned = [1/(shrink_factor_x**2)*(proper_homo_binning_t_2D(temperature_minus_background_std_crop[i]**2,1,shrink_factor_x,type='np.nansum',running_time=True)[0]**0.5) for i in range(len(laser_digitizer_ID))]
			# temperature_minus_background_std_crop_binned = (temperature_minus_background_std_crop_binned**2 + temperature_minus_background_std_add_crop_binned**2)**0.5
		# temperature_ref_crop_binned = proper_homo_binning_2D(temperature_ref_crop,shrink_factor_x)
		# temp_ref_counts_std_add_crop_binned = proper_homo_binning_2D(temp_ref_counts_crop,shrink_factor_x,type='np.nanstd')
		temp_ref_counts_crop_binned = [proper_homo_binning_2D(temp_ref_counts_crop[i],shrink_factor_x) for i in range(len(laser_digitizer_ID))]
		temp_ref_counts_std_crop_binned = [1/(shrink_factor_x**2)*(proper_homo_binning_2D(temp_ref_counts_std_crop[i]**2,shrink_factor_x,type='np.nansum')**0.5) for i in range(len(laser_digitizer_ID))]
		# temp_ref_counts_std_crop_binned = (temp_ref_counts_std_crop_binned**2  + temp_ref_counts_std_add_crop_binned**2)**0.5
		BB_proportional_crop_binned = [proper_homo_binning_2D(BB_proportional_crop[i],shrink_factor_x) for i in range(len(laser_digitizer_ID))]
		# averaged_BB_proportional_std_add_crop_binned = proper_homo_binning_2D(averaged_BB_proportional_crop,shrink_factor_x,type='np.nanstd')[0]
		BB_proportional_std_crop_binned = [1/(shrink_factor_x**2)*(proper_homo_binning_2D(BB_proportional_std_crop[i]**2,shrink_factor_x,type='np.nansum')**0.5) for i in range(len(laser_digitizer_ID))]
		# averaged_BB_proportional_std_crop_binned = (averaged_BB_proportional_std_crop_binned**2 + averaged_BB_proportional_std_add_crop_binned**2)**0.5
		time_binned = [proper_homo_binning_t(time_partial[i],shrink_factor_t) for i in range(len(laser_digitizer_ID))]

		# 2024/09/26 when comparing np.std(counts_crop_binned) with counts_std_crop_binned for the first frames it appears that the real noise is much higher than it should.
		# this is shown to depend from a feature in the actual image, that becomes apparent when some real binning is operated (before the pixels are too close for the local variation to be significant)
		# this feature seems to be there also at later times, and it's of ~3 counts of amplitude.
		# this has non impact in the temporal derivative, but could have a minimal one on the spatial one
		# I found this in shot 48004 easily as it is quite strong there, but confermed it's exixtance also il 48001 and 45473, albeit with a smaller intensity
		# I restricted the time region used to fint t=0 to avoid the effected region, the calculation of the reference frame is at t_min+0.5<t<-0.5, so it's already ok

	if pass_number==0:	# I sample less to make the analysis smaller, to replicate the initial binning behaviour
		binning_type = 'bin' + str(int(np.round(frames_to_average))) + 'x' + str(shrink_factor_x) + 'x' + str(shrink_factor_x)
		# temp = FAST_counts_minus_background_crop_binned[::int(round(frames_to_average))]
		# temp = np.array(temp)
		# FAST_counts_minus_background_crop_binned = cp.deepcopy(temp)
		# temp = []
		# for i in range(len(laser_digitizer_ID)):
		# 	temp.append(temperature_crop_binned[i][::int(round(frames_to_average))])
		# temp = np.array(temp)
		# temperature_crop_binned = cp.deepcopy(temp)
		temp = []
		for i in range(len(laser_digitizer_ID)):
			temp.append(temperature_std_crop_binned[i][::int(round(frames_to_average))])
		temp = np.array(temp)
		temperature_std_crop_binned = cp.deepcopy(temp)
		# temp = []
		# for i in range(len(laser_digitizer_ID)):
		# 	temp.append(counts_crop_binned[i][::int(round(frames_to_average))])
		# temp = np.array(temp)
		# counts_crop_binned = cp.deepcopy(temp)
		temp = []
		for i in range(len(laser_digitizer_ID)):
			temp.append(counts_std_crop_binned[i][::int(round(frames_to_average))])
		temp = np.array(temp)
		counts_std_crop_binned = cp.deepcopy(temp)
		# temp = []
		# for i in range(len(laser_digitizer_ID)):
		# 	temp.append(temperature_minus_background_crop_binned[i][::int(round(frames_to_average))])
		# temp = np.array(temp)
		# temperature_minus_background_crop_binned = cp.deepcopy(temp)
		if False:	# 2024/10/01 currently I don't even use this, so no need to calculate it
			temp = []
			for i in range(len(laser_digitizer_ID)):
				temp.append(temperature_minus_background_std_crop_binned[i][::int(round(frames_to_average))])
			temp = np.array(temp)
			temperature_minus_background_std_crop_binned = cp.deepcopy(temp)
		temp = []
		for i in range(len(laser_digitizer_ID)):
			temp.append(time_binned[i][::int(round(frames_to_average))])
		temp = np.array(temp)
		time_binned = cp.deepcopy(temp)
	else:	# normal process
		pass
	filename_root = filename_root + '_'+binning_type

	foil_properties_used['ref_temperature'] = ref_temperature
	foil_properties_used['ref_temperature_std'] = ref_temperature_std

	foilemissivityscaled=emissivity*np.ones(np.array(FAST_counts_minus_background_crop_binned[0].shape)-2)
	foilthicknessscaled=thickness*np.ones(np.array(FAST_counts_minus_background_crop_binned[0].shape)-2)
	conductivityscaled=Ptthermalconductivity*np.ones(np.array(FAST_counts_minus_background_crop_binned[0].shape)-2)
	reciprdiffusivityscaled=(1/diffusivity)*np.ones(np.array(FAST_counts_minus_background_crop_binned[0].shape)-2)
	foilemissivityscaled_sigma=sigma_emissivity*emissivity*np.ones(np.array(FAST_counts_minus_background_crop_binned[0].shape)-2)	# absolute quantity, not relative
	foilthickness_over_diffusivityscaled_sigma=sigma_thickness_over_diffusivity*thickness/diffusivity*np.ones(np.array(FAST_counts_minus_background_crop_binned[0].shape)-2)	# absolute quantity, not relative
	conductivityscaled_sigma=0*Ptthermalconductivity*np.ones(np.array(FAST_counts_minus_background_crop_binned[0].shape)-2)	# absolute quantity, not relative
	thicknessscaled_sigma=sigma_thickness*thickness*np.ones(np.array(FAST_counts_minus_background_crop_binned[0].shape)-2)	# absolute quantity, not relative


	dx=foil_position_dict['foilhorizw']/foil_position_dict['foilhorizwpixel']*shrink_factor_x

	if False:
		dTdt,dTdt_std,d2Tdxy,d2Tdxy_std,negd2Tdxy,negd2Tdxy_std,T4_T04,T4_T04_std = calc_temp_to_power_BB_1(photon_flux_over_temperature_interpolator,temperature_minus_background_crop_binned,ref_temperature,time_binned,dx,counts_std_crop_binned,averaged_BB_proportional_crop_binned,averaged_BB_proportional_std_crop_binned,temp_ref_counts_std_crop_binned,temperature_std_crop_binned,nan_ROI_mask,ref_temperature_std=ref_temperature_std)
		BBrad,diffusion,timevariation,powernoback,BBrad_std,diffusion_std,timevariation_std,powernoback_std = calc_temp_to_power_BB_2(dTdt,dTdt_std,d2Tdxy,d2Tdxy_std,negd2Tdxy,negd2Tdxy_std,T4_T04,T4_T04_std,nan_ROI_mask,foilemissivityscaled,foilthicknessscaled,reciprdiffusivityscaled,Ptthermalconductivity)
	else:
		output1 = [calc_temp_to_power_BB_1(photon_flux_over_temperature_interpolator,temperature_minus_background_crop_binned[i],ref_temperature,time_binned[i],dx,counts_std_crop_binned[i],BB_proportional_crop_binned[i],BB_proportional_std_crop_binned[i],temp_ref_counts_std_crop_binned[i],temperature_std_crop_binned[i],nan_ROI_mask,ref_temperature_std=ref_temperature_std) for i in range(len(laser_digitizer_ID))]
		dTdt = [[],[]]
		dTdt_std = [[],[]]
		d2Tdxy = [[],[]]
		d2Tdxy_std = [[],[]]
		negd2Tdxy = [[],[]]
		negd2Tdxy_std = [[],[]]
		T4_T04 = [[],[]]
		T4_T04_std = [[],[]]
		dTdt[0],dTdt_std[0],d2Tdxy[0],d2Tdxy_std[0],negd2Tdxy[0],negd2Tdxy_std[0],T4_T04[0],T4_T04_std[0] = output1[0]
		dTdt[1],dTdt_std[1],d2Tdxy[1],d2Tdxy_std[1],negd2Tdxy[1],negd2Tdxy_std[1],T4_T04[1],T4_T04_std[1] = output1[1]
		output1 = [calc_temp_to_power_BB_2(dTdt[i],dTdt_std[i],d2Tdxy[i],d2Tdxy_std[i],negd2Tdxy[i],negd2Tdxy_std[i],T4_T04[i],T4_T04_std[i],nan_ROI_mask,foilemissivityscaled,foilthicknessscaled,reciprdiffusivityscaled,conductivityscaled,foilemissivityscaled_sigma=foilemissivityscaled_sigma,foilthickness_over_diffusivityscaled_sigma=foilthickness_over_diffusivityscaled_sigma,conductivityscaled_sigma=conductivityscaled_sigma,thicknessscaled_sigma=thicknessscaled_sigma) for i in range(len(laser_digitizer_ID))]
		BBrad = [[],[]]	# W/m2
		diffusion = [[],[]]	# W/m2
		timevariation = [[],[]]	# W/m2
		powernoback = [[],[]]	# W/m2
		BBrad_std = [[],[]]
		diffusion_std = [[],[]]
		timevariation_std = [[],[]]
		powernoback_std = [[],[]]
		BBrad[0],diffusion[0],timevariation[0],powernoback[0],BBrad_std[0],diffusion_std[0],timevariation_std[0],powernoback_std[0] = output1[0]
		BBrad[1],diffusion[1],timevariation[1],powernoback[1],BBrad_std[1],diffusion_std[1],timevariation_std[1],powernoback_std[1] = output1[1]

	# What I just calculated is the power absorbed by the foil.
	# this is DIFFERENT from the power emitted by the plasma! only the emissivity*Pplasma i absorbed by the foil
	for i in range(len(laser_digitizer_ID)):
		powernoback_std[i] = ((powernoback_std[i]/foilemissivityscaled)**2 + (powernoback[i]/(foilemissivityscaled**2)*foilemissivityscaled_sigma)**2)**0.5
		powernoback[i] /= foilemissivityscaled

	if int(laser_to_analyse[-9:-4])> 45517:	# MU02 geometry
		stand_off_length = 0.06	# m
	else:
		stand_off_length = 0.045	# m
	pinhole_offset = np.array([-0.0198,-0.0198])	# toroidal direction parallel to the place surface, z
	pinhole_radious = 0.004/2	# m

	horizontal_coord = np.arange(np.shape(powernoback[0][0])[1])
	vertical_coord = np.arange(np.shape(powernoback[0][0])[0])
	horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
	horizontal_coord = (horizontal_coord+1+0.5)*dx	# +1 because in the process of calculating the power I eliminate the first and last pixel in spatial coordinates, +0.5 do be the centre of the pixel
	vertical_coord = (vertical_coord+1+0.5)*dx
	horizontal_coord -= foil_position_dict['foilhorizw']*0.5-pinhole_offset[0]
	vertical_coord -= foil_position_dict['foilvertw']*0.5+pinhole_offset[0]
	distance_from_vertical = (horizontal_coord**2+vertical_coord**2)**0.5
	pinhole_to_foil_vertical = 0.008 + 0.003 + 0.002 + stand_off_length	# pinhole holder, washer, foil holder, standoff
	pinhole_to_pixel_distance = (pinhole_to_foil_vertical**2 + distance_from_vertical**2)**0.5

	etendue = np.ones_like(powernoback[0][0]) * (np.pi*(pinhole_radious**2)) / (pinhole_to_pixel_distance**2)	# I should include also the area of the pixel, but that is already in the w/m2 power
	# in next step i multiply for the cos of the angle from the normal of the pinhole to the pixels and from the normal of the pixel to the pinhole
	etendue *= (pinhole_to_foil_vertical/pinhole_to_pixel_distance)**2	 # cos(a)*cos(b). for pixels not directly under the pinhole both pinhole and pixel are tilted respect to the vertical, with same angle.
	brightness = 4*np.pi*np.mean(powernoback,axis=0)/etendue

	# here I prepare the matrix to correlate pinhole plate temperature to foil power
	# from mpmath import ellipe
	# ellipe_for_arrays = np.frompyfunc(ellipe,1,1)
	sigmaSB=5.6704e-08 #[W/(m2 K4)]
	rmax_pinhole_plate=0.045/2	# radious left open by the pinhole plate holder
	IRVB_head_radious = 0.180/2
	rmax_IRVB_tube = IRVB_head_radious+np.sum(pinhole_offset**2)**0.5
	dr_pinhole_plate = 0.0001
	R_IRVB_head_from_pinhole = np.arange(pinhole_radious,rmax_IRVB_tube,dr_pinhole_plate)+dr_pinhole_plate/2
	# IRVB_head_emissivity = 0.4	# completely arbitrary, similar to the one of shiny stainless steel
	IRVB_head_emissivity = 1	# the IRVB head is coated in carbon, so high emissivity
	pinhole_plate_to_foil_weight = []
	for r in R_IRVB_head_from_pinhole:
		a = distance_from_vertical**2 + r**2 + pinhole_to_foil_vertical**2
		b = 2*distance_from_vertical*r
		pinhole_plate_to_foil_weight.append( sigmaSB*emissivity *(pinhole_to_foil_vertical**2)* r*dr_pinhole_plate * 2*a * (a**2 - b**2)**0.5 / (b**2 - a**2)**2 )
	pinhole_plate_to_foil_weight = np.array(pinhole_plate_to_foil_weight)

	def calc_IRVB_head_ring_to_foil_weight():
		horizontal_coord = np.arange(np.shape(brightness[0])[1])
		vertical_coord = np.arange(np.shape(brightness[0])[0])
		horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
		horizontal_coord = (horizontal_coord+1+0.5)*dx	# +1 because in the process of calculating the power I eliminate the first and last pixel in spatial coordinates, +0.5 do be the centre of the pixel
		vertical_coord = (vertical_coord+1+0.5)*dx
		horizontal_coord -= foil_position_dict['foilhorizw']*0.5#-pinhole_offset[0]	# I do this so I take the distnce of the real centre
		vertical_coord -= foil_position_dict['foilvertw']*0.5#+pinhole_offset[0]	# I do this so I take the distnce of the real centre
		distance_from_vertical = (horizontal_coord**2+vertical_coord**2)**0.5

		dr_pinhole_plate = 0.01	# completely arbitrary
		a = distance_from_vertical**2 + IRVB_head_radious**2 + pinhole_to_foil_vertical**2
		b = 2*distance_from_vertical*IRVB_head_radious
		out = sigmaSB*emissivity *(pinhole_to_foil_vertical**2)* IRVB_head_radious*dr_pinhole_plate * 2*a * (a**2 - b**2)**0.5 / (b**2 - a**2)**2
		return out
	IRVB_head_ring_to_foil_weight = calc_IRVB_head_ring_to_foil_weight()

	if False:	# just to check the numbers
		DT_max = 2
		DT_head = 0
		DT_ring = 0
		pinhole_plate_T_slope = +100000000
		DT_head_r = DT_max + DT_head + ( -DT_max - pinhole_plate_T_slope*(rmax_pinhole_plate-pinhole_radious) ) * ((R_IRVB_head_from_pinhole-pinhole_radious)/(rmax_pinhole_plate-pinhole_radious))**2 + pinhole_plate_T_slope*(R_IRVB_head_from_pinhole-pinhole_radious)
		DT_head_r[R_IRVB_head_from_pinhole>=rmax_pinhole_plate] = DT_head
		DT_head_r[DT_head_r<DT_head] = DT_head
		DT_head_r[DT_head_r>DT_max+DT_head] = DT_max+DT_head
		IRVB_head_power_correction = np.sum((pinhole_plate_to_foil_weight.T*((DT_head_r+ref_temperature+zeroC)**4 - (ref_temperature+zeroC)**4)).T,axis=0) + IRVB_head_ring_to_foil_weight*((DT_ring+ref_temperature+zeroC)**4 - (ref_temperature+zeroC)**4)


	# temp = brightness[:,:,:int(np.shape(brightness)[2]*0.75)]
	# temp = np.sort(temp[np.max(temp,axis=(1,2)).argmax()].flatten())
	# ani,efit_reconstruction = movie_from_data(np.array([np.flip(np.transpose(brightness,(0,2,1)),axis=2)]), 1/np.median(np.diff(time_binned)),timesteps=time_binned[1:-1],integration=laser_int_time/1000,time_offset=time_binned[0],extvmin=0,xlabel='horizontal coord [pixels]', ylabel='vertical coord [pixels]',barlabel='brightness [W/m2]', prelude='shot ' + laser_to_analyse[-9:-4]+'\n'+binning_type+'\n',overlay_structure=True,include_EFIT=True,pulse_ID=laser_to_analyse[-9:-4],overlay_x_point=True,overlay_mag_axis=True,overlay_strike_points=True,overlay_separatrix=True,EFIT_output_requested=True)
	# ani.save('/home/ffederic/work/irvb/MAST-U/FAST_results/'+os.path.split(laser_to_analyse[:-4])[1]+ '_FAST_brightness.mp4', fps=5*(1/np.mean(np.diff(time_binned)))/383, writer='ffmpeg',codec='mpeg4')
	# plt.close('all')
	powernoback_output = np.mean(powernoback,axis=0)	# W/m2

	# sigma_emissivity = 0.1
	# sigma_thickness = 0.15
	# sigma_rec_diffusivity = 0.1
	# sigma_emissivity = 0.04999 # np.std(foilemissivity)/np.mean(foilemissivity)	# I use the varition on the japanese data as reference
	# sigma_thickness = 0.1325 # np.std(foilthickness)/np.mean(foilthickness)
	# sigma_rec_diffusivity = 0.1325	# I give it the same variability as thickness
	# # 2022/09/29 from the RSI paper
	# sigma_emissivity = 0.111 # np.std(foilemissivity)/np.mean(foilemissivity)	# I use the varition on the japanese data as reference
	# sigma_thickness = 0.139 # np.std(foilthickness)/np.mean(foilthickness)
	# sigma_rec_diffusivity = 0.142	# I give it the same variability as thickness
	# 2022/10/14 defined before now

	tend = get_tend(laser_to_analyse[-9:-4])+0.05	 # I add 50ms just for safety and to catch disruptions
	time_full_binned = np.mean(time_binned,axis=0)[1:-1]
	time_full_binned_crop = time_full_binned[time_full_binned<tend]
	powernoback_full_orig = np.mean(powernoback,axis=0)[time_full_binned<tend]
	# now I calculate the uncertainty completely within calc_temp_to_power_BB_2,m so i don't need to re-do it
	# try:	# in case i change something and sigma_thickness_over_diffusivity is no longer defined
	# 	sigma_powernoback_full = 0.5*(np.sum([( (diffusion[i]**2)*((diffusion_std[i]/diffusion[i])**2+sigma_thickness**2) + (timevariation[i]**2)*((timevariation_std[i]/timevariation[i])**2+sigma_thickness_over_diffusivity**2) + (BBrad[i]**2)*((BBrad_std[i]/BBrad[i])**2+sigma_emissivity**2) ) for i in range(len(laser_digitizer_ID))] ,axis=0)**0.5)
	# except:
	# 	sigma_powernoback_full = 0.5*(np.sum([( (diffusion[i]**2)*((diffusion_std[i]/diffusion[i])**2+sigma_thickness**2) + (timevariation[i]**2)*((timevariation_std[i]/timevariation[i])**2+sigma_thickness**2+sigma_rec_diffusivity**2) + (BBrad[i]**2)*((BBrad_std[i]/BBrad[i])**2+sigma_emissivity**2) ) for i in range(len(laser_digitizer_ID))] ,axis=0)**0.5)
	sigma_powernoback_full =  0.5*(np.sum(np.array(powernoback_std)**2,axis=0)**0.5).astype(np.float32)
	sigma_powernoback_output = cp.deepcopy(sigma_powernoback_full)
	brightness_sigma = 4*np.pi*sigma_powernoback_full/etendue
	sigma_powernoback_full = sigma_powernoback_full[time_full_binned<tend]

	print(np.shape(brightness))

	temp = brightness[:,:,:int(np.shape(brightness)[2]*0.75)]
	temp = np.sort(temp[np.max(temp,axis=(1,2)).argmax()].flatten())
	ani,efit_reconstruction = movie_from_data(np.array([np.flip(np.transpose(brightness,(0,2,1)),axis=2)])/1000, 1/np.median(np.diff(time_full_binned)),timesteps=time_full_binned,integration=laser_int_time/1000,time_offset=time_full_binned[0],extvmin=0,xlabel='horizontal coord [pixels]', ylabel='vertical coord [pixels]',barlabel='brightness [kW/m2]', prelude='shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\n'+binning_type+'\n',overlay_structure=True,include_EFIT=True,pulse_ID=laser_to_analyse[-9:-4],overlay_x_point=True,overlay_mag_axis=True,overlay_strike_points=True,overlay_separatrix=True,EFIT_output_requested=True)
	ani.save(filename_root+ '_FAST_brightness.mp4', fps=5*(1/np.mean(np.diff(time_full_binned)))/383, writer='ffmpeg',codec='mpeg4')
	plt.close('all')


	print('completed FAST power calculation ' + laser_to_analyse + ' in %.3gmin from calling MASTU_pulse_process_FAST3_BB' %((tm.time()-start_processing_fast)/60))

	if pass_number==1:	# temporary to speed up the inversion 04/03/2025
		time_full_binned_crop = time_full_binned_crop[::int(frames_to_average)]
		powernoback_full_orig = powernoback_full_orig[::int(frames_to_average)]
		sigma_powernoback_full = sigma_powernoback_full[::int(frames_to_average)]

	inverted_dict = dict([])
	from scipy.ndimage import geometric_transform
	import time as tm
	import pickle
	from shapely.geometry import Point
	from shapely.geometry.polygon import Polygon
	shot_number = int(laser_to_analyse[-9:-4])
	# for grid_resolution in [4, 2]:
	# for grid_resolution in [2, 4]:
	for grid_resolution in [2]:
		inverted_dict[str(grid_resolution)] = dict([])
		# grid_resolution = 8  # in cm
		foil_resolution = '187'

		foil_res = '_foil_pixel_h_' + str(foil_resolution)

		grid_type = 'core_res_' + str(grid_resolution) + 'cm'
		if int(laser_to_analyse[-9:-4])> 45517:	# MU02 geometry
			# path_sensitivity = '/home/ffederic/work/analysis_scripts/sensitivity_matrix_' + grid_type[5:] + foil_res + '_power_stand_off_0.06_pinhole_4'
			# TEST: geometry matrix calculated with measured geometry
			path_sensitivity = '/home/ffederic/work/analysis_scripts/sensitivity_matrix_' + grid_type[5:] + foil_res + '_power_stand_off_0.06_pinhole_4_MU04'
			# extra_geom_correction = '_2mmleft'
			extra_geom_correction = ''
		else:
			path_sensitivity = '/home/ffederic/work/analysis_scripts/sensitivity_matrix_' + grid_type[5:] + foil_res + '_power'
			extra_geom_correction = ''

		try:
			sensitivities = np.array((scipy.sparse.load_npz(path_sensitivity + '/sensitivity'+extra_geom_correction+'.npz')).todense())	# W/m^2 /(W/m^3/str) of emitter
			print('done sensitivities = np.array((scipy.sparse.load_npz('+path_sensitivity + '/sensitivity'+extra_geom_correction+'.npz)).todense()))')
		except:
			sensitivities = np.load(path_sensitivity + '/sensitivity'+extra_geom_correction+'.npy')
			print('sensitivities = np.load('+path_sensitivity+'/sensitivity'+extra_geom_correction+'.npy)')

		filenames = all_file_names(path_sensitivity, '.csv')[0]
		with open(os.path.join(path_sensitivity, filenames)) as csv_file:
			csv_reader = csv.reader(csv_file, delimiter=',')
			for row in csv_reader:
				if row[0] == 'foil vertical pixels ':
					pixel_v = int(row[1])
				if row[0] == 'foil horizontal pixels ':
					pixel_h = int(row[1])
				if row[0] == 'pipeline type ':
					pipeline = row[1]
				if row[0] == 'type of volume grid ':
					grid_type = row[1]
			# print(row)

		# directory = '/home/ffederic/work/cherab/cherab_mastu/cherab/mastu/bolometry/grid_construction'
		directory = '/home/ffederic/work/analysis_scripts/grid_construction'	# moved for to have all important info in the same place
		grid_file = os.path.join(directory,'{}_rectilinear_grid.pickle'.format(grid_type))
		with open(grid_file, 'rb') as f:
			grid_data_all = pickle.load(f)
		grid_laplacian = grid_data_all['laplacian']
		grid_mask = grid_data_all['mask']
		grid_data = grid_data_all['voxels']
		grid_index_2D_to_1D_map = grid_data_all['index_2D_to_1D_map']
		grid_index_1D_to_2D_map = grid_data_all['index_1D_to_2D_map']

		sensitivities_reshaped = sensitivities.reshape((pixel_v,pixel_h,len(grid_laplacian)))
		sensitivities_reshaped = np.transpose(sensitivities_reshaped , (1,0,2))	# W/m^2 /(W/m^3/str) of emitter

		if False:	# I want to visualize the grid
			sensitivities_reshaped[sensitivities_reshaped==0] = np.nan
			sensitivities_reshaped *= (0.07/187)**2 / (4*np.pi)
			plt.figure()
			plt.scatter(np.mean(grid_data,axis=1)[:,0],np.mean(grid_data,axis=1)[:,1],c=np.nansum(sensitivities_reshaped,axis=(0,1)),s=2,marker='s',cmap='rainbow',norm=LogNorm(vmin=np.nanmax(np.nansum(sensitivities_reshaped,axis=(0,1)))/1e4))
			# plt.plot(_MASTU_CORE_GRID_POLYGON[:, 0], _MASTU_CORE_GRID_POLYGON[:, 1], 'k')
			plt.plot(FULL_MASTU_CORE_GRID_POLYGON[:, 0], FULL_MASTU_CORE_GRID_POLYGON[:, 1], 'k')
			plt.colorbar()
			ax = plt.gca()
			ax.set_aspect('equal', adjustable='box')
			plt.savefig('/home/ffederic/work/irvb/0__outputs/'+os.path.split(path_sensitivity)[-1]+'.eps')

		print('mark1 in %.3gmin from calling MASTU_pulse_process_FAST3_BB' %((tm.time()-start_processing_fast)/60))

		if grid_resolution==8:
			# temp=1e-3
			temp=1e-7
			temp2=0
		elif grid_resolution==2:
			temp=0#1e-4
			temp2=0#1e-4
		elif grid_resolution==4:
			temp=0
			temp2=0

		sensitivities_reshaped_masked,grid_laplacian_masked,grid_data_masked,grid_Z_derivate_masked,grid_R_derivate_masked = reduce_voxels(sensitivities_reshaped,grid_laplacian,grid_data,sum_treshold=temp2,std_treshold = temp,chop_top_corner = False,chop_corner_close_to_baffle = False, core_radious_treshold = 1.9,extra_chop_top_corner=False,restrict_polygon=_MASTU_CORE_GRID_POLYGON)

		# this step is to adapt the matrix to the size of the foil I measure, that can be slightly different
		# binning_type = 'bin' + str(shrink_factor_t) + 'x' + str(shrink_factor_x) + 'x' + str(shrink_factor_x)
		shape = list(FAST_counts_minus_background_crop.shape[1:])
		# I want to save time furter so I want to save the matrix with the number of camera pixel reduced, as the interpolation requires a lot of time (~5 min)
		try:
			sensitivities_reshaped_masked2 = np.load(path_sensitivity + '/sensitivity'+extra_geom_correction+'_FoilShape'+str(shape)+'.npz')['sensitivities_reshaped_masked2']	# W/m^2 /(W/m^3/str) of emitter
			print('sensitivities_reshaped_masked2 loaded')
		except:
			if shape!=list(sensitivities_reshaped_masked.shape[:-1]):
				# shape.extend([len(grid_laplacian_masked)])
				# def mapping(output_coords):
				# 	return(output_coords[0]/shape[0]*pixel_h,output_coords[1]/shape[1]*pixel_v,output_coords[2])
				# sensitivities_reshaped_masked2 = geometric_transform(sensitivities_reshaped_masked,mapping,output_shape=shape)
				# new method using reliable functions that avoids by design negative values. it could cause unwanted sMoothing of the data on the foil
				from scipy.interpolate import RegularGridInterpolator
				sensitivities_interpolator = RegularGridInterpolator((np.arange(sensitivities_reshaped_masked.shape[0]), np.arange(sensitivities_reshaped_masked.shape[1]), np.arange(sensitivities_reshaped_masked.shape[-1])), sensitivities_reshaped_masked)
				output_shape_0 = np.arange(shape[0])/(shape[0]-1)*(sensitivities_reshaped_masked.shape[0]-1)
				output_shape_1 = np.arange(shape[1])/(shape[1]-1)*(sensitivities_reshaped_masked.shape[1]-1)
				output_shape_2 = np.arange(sensitivities_reshaped_masked.shape[-1])
				X, Y, Z= np.meshgrid(output_shape_0, output_shape_1, output_shape_2, indexing='ij')
				sensitivities_reshaped_masked2 = sensitivities_interpolator((X, Y, Z))	# W/m^2 /(W/m^3/str) of emitter
			else:
				sensitivities_reshaped_masked2 = cp.deepcopy(sensitivities_reshaped_masked)
			np.savez_compressed(path_sensitivity + '/sensitivity'+extra_geom_correction+'_FoilShape'+str(shape),sensitivities_reshaped_masked2=sensitivities_reshaped_masked2)
			print('sensitivities_reshaped_masked2 saved')




		print('mark2 in %.3gmin from calling MASTU_pulse_process_FAST3_BB' %((tm.time()-start_processing_fast)/60))

		sensitivities_binned = proper_homo_binning_1D_1D_1D(sensitivities_reshaped_masked2,shrink_factor_x,shrink_factor_x,1,type='np.nanmean')
		sensitivities_binned = sensitivities_binned[1:-1,1:-1]	# i need to remove 2 pixels per coordinate because this is done to calculate the lalacian
		sensitivities_binned = np.flip(sensitivities_binned,axis=1)	# it turns ou that I need to flip it	# W/m^2 /(W/m^3/str) of emitter

		print('mark3 in %.3gmin from calling MASTU_pulse_process_FAST3_BB' %((tm.time()-start_processing_fast)/60))

		# additional cropping of the foil to exlude regions without plasma LOS, the frame of the foil and gas puff
		# ROI = np.array([[0.2,0.85],[0.1,0.9]])
		# ROI = np.array([[0.05,0.95],[0.05,0.95]])
		# ROI = np.array([[0.2,0.95],[0.1,1]])
		# ROI1 = np.array([[0.03,0.82],[0.03,0.90]])	# horizontal, vertical
		# ROI2 = np.array([[0.03,0.68],[0.03,0.91]])
		ROI1 = np.array([np.array([1,52])/62,np.array([1,72])/80])	# horizontal, vertical
		ROI2 = np.array([np.array([1,41])/62,np.array([1,78])/80])
		# # 2022/10/02, no, it' create stripes on the LFS
		# ROI1 = np.array([[0.15,0.82],[0.03,0.90]])	# horizontal, vertical
		# ROI2 = np.array([[0.15,0.68],[0.03,0.91]])
		# ROI_beams = np.array([[0.,0.32],[0.42,1]])
		# # suggestion from Jack: keep the view as poloidal as I can, remove the central colon bit
		# ROI1 = np.array([[0.03,0.65],[0.03,0.90]])	# horizontal, vertical
		# ROI2 = np.array([[0.03,0.65],[0.03,0.91]])
		if int(laser_to_analyse[-9:-4])< 45514:	# difference between MU01 and MU02
			ROI_beams = np.array([[0.,0.32],[0.42,1]])
			foil_mask_type = ''
		else:
			foil_mask_type = retrive_shot_foil_mask_type(int(laser_to_analyse[-9:-4]))
			if foil_mask_type == 'large masking':
				ROI_beams = np.array([np.array([0.,10,25,25,30,0])/62,np.array([25,33,60,70,80,80])/80])	# this original borders are quite clear from 47958
			elif foil_mask_type == 'small masking':
				ROI_beams = np.array([np.array([0.,15,10,5,0])/62,np.array([25,33,45,60,70])/80])	# 26/03/2024 temporary edit to see what happens if all the possible volume is used for 49213
			elif foil_mask_type == 'small high masking':
				ROI_beams = np.array([np.array([0.,15,10,5,5,0])/62,np.array([25,33,45,60,75,80])/80])	# 26/03/2024 temporary edit to see what happens if all the possible volume is used for 49213
			elif foil_mask_type == 'standard':
				ROI_beams = np.array([np.array([0.,10,20,20,20,0])/62,np.array([25,33,60,70,80,80])/80])	# 26/03/2024 in reality the initial moments of a shot are not that impoirtant, so I can reduce the excluded area to only what is usefull after #300/400ms
			elif foil_mask_type == 'as no beam':
				ROI_beams = np.array([np.array([0.,1,0])/62,np.array([25,33,60])/80])	# 04/05/24 to analyse shots that have failed beams as they are purely ohmic
			else:
				print("this selection means that no mask was really selected, it's kind of wrong. the standard is the fall back case rather than the requested "+foil_mask_type)
				ROI_beams = np.array([np.array([0.,10,20,20,20,0])/62,np.array([25,33,60,70,80,80])/80])	# 26/03/2024 in reality the initial moments of a shot are not that impoirtant, so I can reduce the excluded area to only what is usefull after #300/400ms
			foil_mask_type = '_' + foil_mask_type
		sensitivities_binned_crop,selected_ROI,ROI1,ROI2,ROI_beams = cut_sensitivity_matrix_based_on_foil_anysotropy(sensitivities_binned,ROI1,ROI2,ROI_beams,laser_to_analyse,additional_output=True)	# W/m^2 /(W/m^3/str) of emitter

		additional_polygons_dict = dict([])
		additional_polygons_dict['time'] = np.array([0])	# in this case I plot the same polygon for the whole movie
		additional_polygons_dict['0'] = np.array([[[ROI1[0,0],ROI1[0,1],ROI1[0,1],ROI1[0,0],ROI1[0,0]],[ROI1[1,0],ROI1[1,0],ROI1[1,1],ROI1[1,1],ROI1[1,0]]]])
		additional_polygons_dict['1'] = np.array([[[ROI2[0,0],ROI2[0,1],ROI2[0,1],ROI2[0,0],ROI2[0,0]],[ROI2[1,0],ROI2[1,0],ROI2[1,1],ROI2[1,1],ROI2[1,0]]]])
		if int(laser_to_analyse[-9:-4])< 45514:	# difference between MU01 and MU02
			additional_polygons_dict['2'] = np.array([[[ROI_beams[0,0],ROI_beams[0,1],ROI_beams[0,1],ROI_beams[0,0],ROI_beams[0,0]],[ROI_beams[1,0],ROI_beams[1,0],ROI_beams[1,1],ROI_beams[1,1],ROI_beams[1,0]]]])
		else:
			additional_polygons_dict['2'] = np.round([np.concatenate([ROI_beams,[ROI_beams[0]]]).T]).astype(int)
		additional_polygons_dict['number_of_polygons'] = 3
		additional_polygons_dict['marker'] = ['--k','--k','--k']

		print('mark4 in %.3gmin from calling MASTU_pulse_process_FAST3_BB' %((tm.time()-start_processing_fast)/60))

		# temp = brightness[:,:,:int(np.shape(brightness)[2]*0.75)]
		# temp = np.sort(temp[np.max(temp,axis=(1,2)).argmax()].flatten())
		brightness_extvmax = np.nanmax(brightness[:,np.nansum(sensitivities_binned_crop,axis=-1)>0],axis=-1)
		ani,efit_reconstruction = movie_from_data(np.array([np.flip(np.transpose(brightness,(0,2,1)),axis=2)])/1000, 1/np.median(np.diff(time_full_binned)),timesteps=time_full_binned,integration=laser_int_time/1000,time_offset=time_full_binned[0],extvmin=0,extvmax=brightness_extvmax/1000,xlabel='horizontal coord [pixels]', ylabel='vertical coord [pixels]',barlabel='brightness [kW/m2]', prelude='shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\n'+binning_type+'\n',overlay_structure=True,include_EFIT=True,pulse_ID=laser_to_analyse[-9:-4],overlay_x_point=True,overlay_mag_axis=True,overlay_strike_points=True,overlay_separatrix=True,EFIT_output_requested=True,additional_polygons_dict=additional_polygons_dict)
		ani.save(filename_root+ '_FAST_brightness.mp4', fps=5*(1/np.mean(np.diff(time_full_binned)))/383, writer='ffmpeg',codec='mpeg4')
		plt.close('all')

		print('mark5 in %.3gmin from calling MASTU_pulse_process_FAST3_BB' %((tm.time()-start_processing_fast)/60))

		if only_plot_brightness:
			sgan = dsfds	# I want an error to occour, just creating the brightness video

		if grid_resolution==8:
			# temp=1e-3
			temp=1e-7
			temp2=0
		elif grid_resolution==2:
			temp=0#1e-4
			temp2=0#5e-5
			# temp2=np.sum(sensitivities_binned_crop,axis=(0,1)).max()*1e-4
		elif grid_resolution==4:
			temp=0
			temp2=0
		sensitivities_binned_crop,grid_laplacian_masked_crop,grid_data_masked_crop,grid_Z_derivate_masked_crop,grid_R_derivate_masked_crop = reduce_voxels(sensitivities_binned_crop,grid_laplacian_masked,grid_data_masked,sum_treshold=temp2,std_treshold = temp,restrict_polygon=_MASTU_CORE_GRID_POLYGON,chop_corner_close_to_baffle=False)
		# sensitivities_binned_crop,grid_laplacian_masked_crop,grid_data_masked_crop,grid_Z_derivate_masked_crop,grid_R_derivate_masked_crop = reduce_voxels(sensitivities_binned_crop,grid_laplacian_masked,grid_data_masked,sum_treshold=temp2,std_treshold = temp,chop_corner_close_to_baffle=False)

		# I add another step of filtering purely fo eliminate cells too isolated from the bulk and can let the laplacian grow
		select = np.max(grid_laplacian_masked_crop,axis=0)<2.9
		grid_laplacian_masked_crop_temp = build_laplacian(grid_data_masked_crop,cells_to_exclude=select)
		select = np.max(grid_laplacian_masked_crop_temp,axis=0)<2.9
		grid_laplacian_masked_crop_temp = build_laplacian(grid_data_masked_crop,cells_to_exclude=select)
		select = np.max(grid_laplacian_masked_crop_temp,axis=0)<2.9
		grid_laplacian_masked_crop_temp = build_laplacian(grid_data_masked_crop,cells_to_exclude=select)
		select = np.max(grid_laplacian_masked_crop_temp,axis=0)<2.9
		sensitivities_binned_crop,grid_laplacian_masked_crop,grid_data_masked_crop,grid_Z_derivate_masked_crop,grid_R_derivate_masked_crop = reduce_voxels(sensitivities_binned_crop,grid_laplacian_masked_crop,grid_data_masked_crop,sum_treshold=0,std_treshold = 0,restrict_polygon=[],chop_corner_close_to_baffle=False,cells_to_exclude=select)
		if False:	# normal path to select the cells close to the pinhole and allow them to not be bound by the limitation of the laplacian
			selected_edge_cells_for_laplacian = np.sum(sensitivities_binned_crop,axis=(0,1))>np.sum(sensitivities_binned_crop,axis=(0,1)).max()*0.2
			# I need to do this again because when I remove some cells from the laplacian I reintroduce the problem of poorly connected ones again
			grid_laplacian_masked_crop_temp = build_laplacian(grid_data_masked_crop,cells_to_exclude=selected_edge_cells_for_laplacian)
			select = np.max(grid_laplacian_masked_crop_temp,axis=0)<2.9
			grid_laplacian_masked_crop_temp = build_laplacian(grid_data_masked_crop,cells_to_exclude=select)
			select = np.max(grid_laplacian_masked_crop_temp,axis=0)<2.9
			grid_laplacian_masked_crop_temp = build_laplacian(grid_data_masked_crop,cells_to_exclude=select)
			select = np.logical_and(np.max(grid_laplacian_masked_crop_temp,axis=0)<2.9,np.logical_not(selected_edge_cells_for_laplacian))
			sensitivities_binned_crop,grid_laplacian_masked_crop,grid_data_masked_crop,grid_Z_derivate_masked_crop,grid_R_derivate_masked_crop = reduce_voxels(sensitivities_binned_crop,grid_laplacian_masked_crop,grid_data_masked_crop,sum_treshold=0,std_treshold = 0,restrict_polygon=[],chop_corner_close_to_baffle=False,cells_to_exclude=select)
			selected_edge_cells_for_laplacian = selected_edge_cells_for_laplacian[np.logical_not(select)]
		else:	# this is NOT to have any cells with this correction
			selected_edge_cells_for_laplacian = np.sum(sensitivities_binned_crop,axis=(0,1))>np.sum(sensitivities_binned_crop,axis=(0,1)).max()*0.2
			selected_edge_cells_for_laplacian = np.zeros_like(selected_edge_cells_for_laplacian)

		print('mark6 in %.3gmin from calling MASTU_pulse_process_FAST3_BB' %((tm.time()-start_processing_fast)/60))

		selected_super_x_cells = np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,0]>0.85,np.mean(grid_data_masked_crop,axis=1)[:,1]<-1.65)
		# select_foil_region_with_plasma = (np.sum(sensitivities_binned_crop,axis=-1)>1e-3)	# this does not work if you change the ROI
		if int(laser_to_analyse[-9:-4])< 45514:	# difference between MU01 and MU02
			select_foil_region_with_plasma = select_region_with_plasma(sensitivities_binned_crop,selected_ROI,outline_area_with_plasma='MU01')
		else:
			select_foil_region_with_plasma = select_region_with_plasma(sensitivities_binned_crop,selected_ROI,outline_area_with_plasma='MU02')
		selected_ROI_no_plasma = np.logical_and(selected_ROI,np.logical_not(select_foil_region_with_plasma))
		select_foil_region_with_plasma = select_foil_region_with_plasma.flatten()


		x1 = [1.55,0.25]	# r,z
		x2 = [1.1,-0.15]
		interp = interp1d([x1[0],x2[0]],[x1[1],x2[1]],fill_value="extrapolate",bounds_error=False)
		select = np.mean(grid_data_masked_crop,axis=1)[:,1]>interp(np.mean(grid_data_masked_crop,axis=1)[:,0])
		selected_central_border_cells = np.logical_and(select,np.logical_and(np.max(grid_Z_derivate_masked_crop,axis=(1))==1,np.mean(grid_data_masked_crop,axis=1)[:,1]>-0.5))

		# selected_central_column_border_cells = np.logical_and(np.logical_and(np.max(grid_R_derivate_masked_crop,axis=(1))==1,np.mean(grid_data_masked_crop,axis=1)[:,0]<0.7),np.mean(grid_data_masked_crop,axis=1)[:,1]<-0.7)
		selected_central_column_border_cells = np.logical_and(np.logical_and(np.max(grid_R_derivate_masked_crop,axis=(1))==1,np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,0]<0.6,np.abs(np.mean(grid_data_masked_crop,axis=1)[:,1])>0.5)),np.logical_not(selected_central_border_cells))
		# selected_central_column_border_cells = np.logical_and(np.logical_and(np.dot(grid_laplacian_masked_crop,selected_central_column_border_cells*np.random.random(selected_central_column_border_cells.shape))!=0,np.mean(grid_data_masked_crop,axis=1)[:,0]<0.7),np.mean(grid_data_masked_crop,axis=1)[:,1]<-0.7)
		selected_central_column_border_cells = np.dot(grid_laplacian_masked_crop,selected_central_column_border_cells*np.random.random(selected_central_column_border_cells.shape))!=0
		selected_central_column_border_cells = np.dot(grid_laplacian_masked_crop,selected_central_column_border_cells*np.random.random(selected_central_column_border_cells.shape))!=0

		selected_central_border_cells = np.logical_or(selected_central_border_cells,np.logical_and(np.dot(grid_laplacian_masked_crop,selected_central_border_cells*np.random.random(selected_central_border_cells.shape))!=0,np.mean(grid_data_masked_crop,axis=1)[:,0]>=0.33))
		selected_central_border_cells = np.logical_or(selected_central_border_cells,np.logical_and(np.dot(grid_laplacian_masked_crop,selected_central_border_cells*np.random.random(selected_central_border_cells.shape))!=0,np.mean(grid_data_masked_crop,axis=1)[:,0]>=0.33))
		selected_central_border_cells = np.logical_or(selected_central_border_cells,np.logical_and(np.dot(grid_laplacian_masked_crop,selected_central_border_cells*np.random.random(selected_central_border_cells.shape))!=0,np.mean(grid_data_masked_crop,axis=1)[:,0]>=0.33))
		selected_central_border_cells = np.logical_or(selected_central_border_cells,np.logical_and(np.dot(grid_laplacian_masked_crop,selected_central_border_cells*np.random.random(selected_central_border_cells.shape))!=0,np.mean(grid_data_masked_crop,axis=1)[:,0]>=0.33))
		selected_central_border_cells = np.logical_or(selected_central_border_cells,np.logical_and(np.dot(grid_laplacian_masked_crop,selected_central_border_cells*np.random.random(selected_central_border_cells.shape))!=0,np.mean(grid_data_masked_crop,axis=1)[:,0]>=0.33))
		selected_central_border_cells = np.logical_or(selected_central_border_cells,np.logical_and(np.dot(grid_laplacian_masked_crop,selected_central_border_cells*np.random.random(selected_central_border_cells.shape))!=0,np.mean(grid_data_masked_crop,axis=1)[:,0]>=0.33))

		# this cells are the external borer ot the domain and above the baffle.
		# I use them to apply a negative weight to any emissivity value different than zero, both positive and negative
		if False:
			# selected_edge_cells = np.logical_and(np.logical_and(np.max(grid_laplacian_masked_crop,axis=(0))<=5.5,np.mean(grid_data_masked_crop,axis=1)[:,0]>1.35),np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,1]>-1.1,np.mean(grid_data_masked_crop,axis=1)[:,1]<-0.6))
			# selected_edge_cells = np.logical_or(selected_edge_cells,np.logical_and(np.logical_and(np.logical_and(np.max(grid_laplacian_masked_crop,axis=(0))<=5.5,np.mean(grid_data_masked_crop,axis=1)[:,0]>1.05),np.mean(grid_data_masked_crop,axis=1)[:,1]>-1.5),np.mean(grid_data_masked_crop,axis=1)[:,1]<-0.6))
			selected_edge_cells = np.logical_and(np.logical_and(np.max(grid_laplacian_masked_crop,axis=(0))<=5.5,np.mean(grid_data_masked_crop,axis=1)[:,0]>1.35),np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,1]>-1.1,np.mean(grid_data_masked_crop,axis=1)[:,1]<2))
			selected_edge_cells = np.logical_or(selected_edge_cells,np.logical_and(np.logical_and(np.logical_and(np.max(grid_laplacian_masked_crop,axis=(0))<=5.5,np.mean(grid_data_masked_crop,axis=1)[:,0]>1.05),np.mean(grid_data_masked_crop,axis=1)[:,1]>-1.5),np.mean(grid_data_masked_crop,axis=1)[:,1]<2))
		else:	# this is NOT to have any cells with this correction
			selected_edge_cells = np.zeros_like(np.max(grid_laplacian_masked_crop,axis=(0))<=5.5)

		if False:
			selected_edge_cells_for_laplacian = np.logical_and(np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,0]>1.05,np.mean(grid_data_masked_crop,axis=1)[:,1]<-0.6),np.dot(grid_laplacian_masked_crop,selected_edge_cells*np.random.random(selected_edge_cells.shape))!=0)
			if grid_resolution<8:
				selected_edge_cells_for_laplacian = np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,0]>1.05,np.dot(grid_laplacian_masked_crop,selected_edge_cells_for_laplacian*np.random.random(selected_edge_cells_for_laplacian.shape))!=0)
			if grid_resolution<4:
				selected_edge_cells_for_laplacian = np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,0]>1.05,np.dot(grid_laplacian_masked_crop,selected_edge_cells_for_laplacian*np.random.random(selected_edge_cells_for_laplacian.shape))!=0)
				selected_edge_cells_for_laplacian = np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,0]>1.05,np.dot(grid_laplacian_masked_crop,selected_edge_cells_for_laplacian*np.random.random(selected_edge_cells_for_laplacian.shape))!=0)
				selected_edge_cells_for_laplacian = np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,0]>1.05,np.dot(grid_laplacian_masked_crop,selected_edge_cells_for_laplacian*np.random.random(selected_edge_cells_for_laplacian.shape))!=0)
				# selected_edge_cells_for_laplacian = np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,0]>1.05,np.dot(grid_laplacian_masked_crop,selected_edge_cells_for_laplacian*np.random.random(selected_edge_cells_for_laplacian.shape))!=0)

			# def temp_func():	# I package it only so it doesn't mess up other variables
			# 	from shapely.geometry import Point
			# 	from shapely.geometry.polygon import Polygon
			# 	select = np.zeros_like(selected_edge_cells_for_laplacian).astype(bool)
			# 	polygon = Polygon(FULL_MASTU_CORE_GRID_POLYGON)
			# 	for i_e in range(len(grid_data_masked_crop)):
			# 		if np.sum([polygon.contains(Point((grid_data_masked_crop[i_e][i__e,0],grid_data_masked_crop[i_e][i__e,1]))) for i__e in range(4)])==0:
			# 			select[i_e] = True
			# 	selected_cells_to_exclude = np.logical_or(selected_edge_cells_for_laplacian,select)
			# 	return selected_cells_to_exclude
			# selected_cells_to_exclude = temp_func()
			selected_cells_to_exclude = np.zeros_like(selected_edge_cells_for_laplacian)

		else:	# strange thing. I want less regularisation where the sensitivity is much higher than the rest of the image
			# selected_edge_cells_for_laplacian = np.sum(sensitivities_binned_crop,axis=(0,1))>np.sum(sensitivities_binned_crop,axis=(0,1)).max()*0.2
			grid_laplacian_masked_crop = build_laplacian(grid_data_masked_crop,cells_to_exclude=selected_edge_cells_for_laplacian)
			grid_Z_derivate_masked_crop = build_Z_derivate(grid_data_masked_crop,cells_to_exclude=selected_edge_cells_for_laplacian)
			grid_R_derivate_masked_crop = build_R_derivate(grid_data_masked_crop,cells_to_exclude=selected_edge_cells_for_laplacian)

			def temp_func():	# I package it only so it doesn't mess up other variables
				from shapely.geometry import Point
				from shapely.geometry.polygon import Polygon
				select = np.zeros_like(selected_edge_cells_for_laplacian).astype(bool)
				polygon = Polygon(FULL_MASTU_CORE_GRID_POLYGON)
				for i_e in range(len(grid_data_masked_crop)):
					if np.sum([polygon.contains(Point((grid_data_masked_crop[i_e][i__e,0],grid_data_masked_crop[i_e][i__e,1]))) for i__e in range(4)])==0:
						select[i_e] = True
				selected_cells_to_exclude = np.logical_or(selected_edge_cells_for_laplacian,select)
				return selected_cells_to_exclude
			selected_cells_to_exclude = temp_func()

		if decrease_smoothing_inner_leg_x_point:
			# select = np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,0]<=0.5,np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,1]<=-0.5,np.mean(grid_data_masked_crop,axis=1)[:,1]>=-1.5))
			# select = np.logical_or(select,np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,0]<=0.7,np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,1]<=-0.75,np.mean(grid_data_masked_crop,axis=1)[:,1]>=-1.25)))
			x1 = [0.7,-0.75]	# r,z
			x2 = [0.5,-0.7]
			interp = interp1d([x1[0],x2[0]],[x1[1],x2[1]],fill_value="extrapolate",bounds_error=False)
			select = np.mean(grid_data_masked_crop,axis=1)[:,1]<interp(np.mean(grid_data_masked_crop,axis=1)[:,0])
			# x1 = [0.7,-1.25]	# r,z	extra_additionan_parameter = ''
			# x2 = [0.5,-1.5]
			# x1 = [0.7,-1.2]	# r,z	extra_additionan_parameter = '1'
			# x2 = [0.5,-1.45]
			x1 = [0.7,-1.15]	# r,z	extra_additionan_parameter = '2'	# with 48144 this seems to eliminate all the localised spikes on the outer leg
			x2 = [0.5,-1.40]
			# x1 = [0.7,-1.3]	# r,z	extra_additionan_parameter = '3'
			# x2 = [0.5,-1.55]
			# x1 = [0.7,-1.35]	# r,z	extra_additionan_parameter = '4'
			# x2 = [0.5,-1.6]
			# x1 = [0.7,-1.4]	# r,z	extra_additionan_parameter = '5'
			# x2 = [0.5,-1.65]
			# x1 = [0.7,-1.4]	# r,z	extra_additionan_parameter = '6'
			# x2 = [0.5,-1.65]
			interp = interp1d([x1[0],x2[0]],[x1[1],x2[1]],fill_value="extrapolate",bounds_error=False)
			select = np.logical_and(select,np.mean(grid_data_masked_crop,axis=1)[:,1]>interp(np.mean(grid_data_masked_crop,axis=1)[:,0]))
			select = np.logical_and(select,np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,0]<=0.7,np.logical_and(np.mean(grid_data_masked_crop,axis=1)[:,1]<=-0.5,np.mean(grid_data_masked_crop,axis=1)[:,1]>=-1.6)))
			grid_laplacian_masked_crop[select] /= 3	# extra_additionan_parameter = '8'	this still gives me plenty of high resolution
			# grid_laplacian_masked_crop[select] /= 2	# extra_additionan_parameter = '7'
			# grid_laplacian_masked_crop[select] /= 4

		print('mark7 in %.3gmin from calling MASTU_pulse_process_FAST3_BB' %((tm.time()-start_processing_fast)/60))

		sensitivities_binned_crop_shape = sensitivities_binned_crop.shape
		sensitivities_binned_crop = sensitivities_binned_crop.reshape((sensitivities_binned_crop.shape[0]*sensitivities_binned_crop.shape[1],sensitivities_binned_crop.shape[2]))	# W/m^2 /(W/m^3/str) of emitter

		# here i want to find a correlation between the reduced voxels and the voxels of the original geometry matrix
		if True:
			sensitivity_matrix_equivalence = np.zeros((len(grid_data_masked),len(grid_data_masked_crop))).astype(bool)
			pos_to_equate_to = np.mean(grid_data_masked,axis=1)
			for i in range(len(grid_data_masked_crop)):
				pos = np.mean(grid_data_masked_crop[i],axis=0)
				d = (pos_to_equate_to[:,0]-pos[0])**2 + (pos_to_equate_to[:,1]-pos[1])**2
				sensitivity_matrix_equivalence[np.abs(d).argmin(),i] = True


		if shrink_factor_x > 1:
			foil_resolution = str(shrink_factor_x) + 'x' + str(shrink_factor_x)
		else:
			foil_resolution = str(shape[0])

		foil_res = '_foil_pixel_h_' + str(foil_resolution)
		path_sensitivity = '/home/ffederic/work/analysis_scripts/sensitivity_matrix_'+grid_type[5:]+foil_res+'_power'
		path_sensitivity_original = cp.deepcopy(path_sensitivity)

		# binning_type = 'bin' + str(shrink_factor_t) + 'x' + str(shrink_factor_x) + 'x' + str(shrink_factor_x)
		print('starting '+binning_type)
		# powernoback_full = saved_file_dict_short[binning_type].all()['powernoback_full']
		# powernoback_std_full = saved_file_dict_short[binning_type].all()['powernoback_std_full']

		# from here I make the new method.
		# I consider the nominal properties as central value, with:
		# emissivity -10% (from Japanese properties i have std of ~5%, but my nominal value is ~1 and emissivity cannot be >1 so I double the interval down)
		# thickness +/-15% (from Japanese properties i have std of ~15%)
		# diffusivity -10% (this is missing from the Japanese data, so I guess std ~10%)

		if False:	# I do this earlier just for convenience
			# sigma_emissivity = 0.1
			# sigma_thickness = 0.15
			# sigma_rec_diffusivity = 0.1
			sigma_emissivity = 0.04999 # np.std(foilemissivity)/np.mean(foilemissivity)	# I use the varition on the japanese data as reference
			sigma_thickness = 0.1325 # np.std(foilthickness)/np.mean(foilthickness)
			sigma_rec_diffusivity = 0.1325	# I give it the same variability as thickness

			tend = get_tend(laser_to_analyse[-9:-4])+0.05	 # I add 50ms just for safety and to catch disruptions
			time_full_binned = time_binned[1:-1]
			BBrad_full_crop = BBrad[time_full_binned<tend]
			BBrad_full_crop[:,np.logical_not(selected_ROI)] = 0
			BBrad_std_full_crop = BBrad_std[time_full_binned<tend]
			BBrad_std_full_crop[:,np.logical_not(selected_ROI)] = 0
			diffusion_full_crop = diffusion[time_full_binned<tend]
			diffusion_full_crop[:,np.logical_not(selected_ROI)] = 0
			diffusion_std_full_crop = diffusion_std[time_full_binned<tend]
			diffusion_std_full_crop[:,np.logical_not(selected_ROI)] = 0
			timevariation_full_crop = timevariation[time_full_binned<tend]
			timevariation_full_crop[:,np.logical_not(selected_ROI)] = 0
			timevariation_std_full_crop = timevariation_std[time_full_binned<tend]
			timevariation_std_full_crop[:,np.logical_not(selected_ROI)] = 0
			time_full_binned_crop = time_full_binned[time_full_binned<tend]

			powernoback_full_orig = diffusion_full_crop + timevariation_full_crop + BBrad_full_crop
			sigma_powernoback_full = ( (diffusion_full_crop**2)*((diffusion_std_full_crop/diffusion_full_crop)**2+sigma_thickness**2) + (timevariation_full_crop**2)*((timevariation_std_full_crop/timevariation_full_crop)**2+sigma_thickness**2+sigma_rec_diffusivity**2) + (BBrad_full_crop**2)*((BBrad_std_full_crop/BBrad_full_crop)**2+sigma_emissivity**2) )**0.5
		else:
			pass

		powernoback_full_orig[:,np.logical_not(selected_ROI)] = 0
		sigma_powernoback_full[:,np.logical_not(selected_ROI)] = np.nan

		grid_laplacian_masked_crop_scaled = grid_laplacian_masked_crop/((1e-2*grid_resolution)**2)
		grid_Z_derivate_masked_crop_scaled = grid_Z_derivate_masked_crop/((1e-2*grid_resolution)**1)
		grid_R_derivate_masked_crop_scaled = grid_R_derivate_masked_crop/((1e-2*grid_resolution)**1)
		reference_sigma_powernoback_all = np.nanmedian(sigma_powernoback_full[:,selected_ROI],axis=1)
		number_cells_ROI = np.sum(selected_ROI)
		number_cells_plasma = np.sum(select_foil_region_with_plasma)
		regolarisation_coeff = 1e-3	# ok for np.median(sigma_powernoback_full)=78.18681
		if grid_resolution==4:
			regolarisation_coeff = 3e-4
		elif grid_resolution==2:
			regolarisation_coeff = 1e-2	# 0.5e-2 when using the lower and actually better measure of the uncertainty

		if shrink_factor_x==1:
			# regolarisation_coeff_range /= 5
			# regolarisation_coeff /= 4
			regolarisation_coeff /= 9	# 2024/10/07 this makes a lot of sense, as /9 matches the increase in smoothing between mpass 0 and 1
			#	but it looks line a little extra smoothing is indeed required for decent solutions
			# regolarisation_coeff *= 1.2

		sigma_emissivity = 1e6	# 2e3	# this is completely arbitrary
		sigma_emissivity_2 = sigma_emissivity**2
		r_int = np.mean(grid_data_masked_crop,axis=1)[:,0]
		z_int = np.mean(grid_data_masked_crop,axis=1)[:,1]
		r_int_2 = r_int**2

		# it seems that I have some radiation on the central column that I don't know how to eliminate.
		# I can limit the r derivative there, but I don't want to do it close to the strike point.
		# if available I take the strike point position and use it to not limit the r derivative close to it
		# first I build the coordinate system

		limiter_r,limiter_z,s_lookup = calculate_tile_geometry()
		if len(s_lookup)>1:
			from mast.geom.geomTileSurfaceUtils import get_nearest_s_coordinates_mastu
			grid_data_masked_crop_s = get_nearest_s_coordinates_mastu(r_int,z_int,s_lookup,tol=5e-3)[0]

		sigma_powernoback_full[np.isnan(sigma_powernoback_full)] = 1e10
		# inverted_dict[str(grid_resolution)]['foil_power'] = powernoback_full_orig
		# inverted_dict[str(grid_resolution)]['foil_power_std'] = sigma_powernoback_full
		# inverted_dict[str(grid_resolution)]['time_full_binned_crop'] = time_full_binned_crop
		# full_saved_file_dict_FAST = dict([])
		# full_saved_file_dict_FAST['inverted_dict'] = inverted_dict
		# np.savez_compressed(laser_to_analyse[:-4]+'_FAST',**full_saved_file_dict_FAST)
		selected_ROI_internal = selected_ROI.flatten()
		not_selected_super_x_cells = np.logical_not(selected_super_x_cells)
		inverted_data = []
		inverted_data_sigma = []
		inverted_data_excluded = []
		inverted_data_excluded_sigma = []
		inverted_data_covariance = []
		inverted_data_likelihood = []
		inverted_data_info = []
		inverted_data_plasma_region_offset = []
		inverted_data_homogeneous_offset = []
		DT_max_all = []
		DT_head_all = []
		DT_ring_all = []
		pinhole_plate_T_slope_all = []
		DT_head_r_all = []
		IRVB_head_power_correction_all = []
		fitted_foil_power = []
		fitted_foil_power_excluded = []
		foil_power = []
		foil_power_std = []
		foil_power_residuals = []
		fit_error = []
		chi_square_all = []
		regolarisation_coeff_all = []
		time_per_iteration = []
		iterations_per_iteration = []
		score_x_all = []
		score_y_all = []
		regolarisation_coeff_range_all = []
		Lcurve_curvature_all = []
		x_optimal_ext = []
		likelihood_logging_ext = []
		derivate_logging_ext = []

		plt.figure(10,figsize=(20, 10))
		plt.title('L-curve evolution\nlight=early, dark=late')
		plt.figure(11,figsize=(20, 10))
		plt.title('L-curve curvature evolution\nlight=early, dark=late')
		first_guess = []
		first_guess_all = []
		x_optimal_all_guess = []
		# regolarisation_coeff_upper_limit = 10**-0.2
		fraction_of_L_curve_for_fit = 0.2	# 0.08
		regolarisation_coeff_upper_limit = 0.1
		regolarisation_coeff_lower_limit = 4e-4 #2e-4
		if laser_framerate<60:
			regolarisation_coeff /= 2	# 2024/11/29 now it seems too smooth at 50Hz
			regolarisation_coeff_upper_limit = 0.2
			regolarisation_coeff_lower_limit = 8e-4

		# regularisation coefficients list
		# regolarisation_coeff_edge = 10
		# regolarisation_coeff_edge_multiplier = 100
		regolarisation_coeff_central_border_Z_derivate_multiplier = 200	#0
		regolarisation_coeff_central_column_border_R_derivate_multiplier = 30	#0	#100
		regolarisation_coeff_edge_laplacian_multiplier = 0
		regolarisation_coeff_divertor_multiplier = 1
		regolarisation_coeff_non_negativity_multiplier = 200
		regolarisation_coeff_offsets_multiplier = 0#1e-10
		# regolarisation_coeff_edge_laplacian = 0.01#0.001#0.02
		regolarisation_coeff_edge = 0# 500 while writing the thesis ~13/12/2022 I realised this is not necessary

		# if EFIT is available, I find the strike point and allow any emissivity gradient in R in its vicinity
		tollerance_for_strike_point = 0.15

		target_chi_square = sensitivities_binned_crop.shape[1]	# obtained doing a scan of the regularisation coefficient. this was the result for regolarisation_coeff~1e-3
		target_chi_square_sigma = 200	# this should be tight, because for such a high number of degrees of freedom things should average very well
		extra_additionan_parameter = '2'	# this can be used for developement, is I want some temporary differentiation. it should be otherwise be = ''

		important_parameters_list = [regolarisation_coeff_central_border_Z_derivate_multiplier,regolarisation_coeff_central_column_border_R_derivate_multiplier,regolarisation_coeff_edge_laplacian_multiplier,regolarisation_coeff_divertor_multiplier,regolarisation_coeff_non_negativity_multiplier,regolarisation_coeff_offsets_multiplier,regolarisation_coeff_edge,tollerance_for_strike_point,target_chi_square_sigma,int(decrease_smoothing_inner_leg_x_point)]
		filename_root_add = '_GridRes'+str(grid_resolution)+'cm_'+'ParList'+str(important_parameters_list)+extra_additionan_parameter+foil_mask_type
		print(filename_root+filename_root_add)
		homogeneous_scaling=1e-4*(3/shrink_factor_x)
		# if use_pinhole_plate_over_temperature:
		# 	homogeneous_scaling=1e-5

		print('start inversion process at %.3gmin from calling MASTU_pulse_process_FAST3_BB' %((tm.time()-start_processing_fast)/60))

		# for i_t in range(pass_number*16,len(time_full_binned_crop)):
		for i_t in range(len(time_full_binned_crop)):
			time_start = tm.time()

			print('starting t=%.4gms' %(time_full_binned_crop[i_t]*1e3))
			# plt.figure()
			# plt.imshow(powernoback_full_orig[i_t])
			# plt.colorbar()
			# plt.pause(0.01)
			#
			# plt.figure()
			# plt.imshow(sigma_powernoback_full[i_t])
			# plt.colorbar()
			# plt.pause(0.01)

			powernoback = powernoback_full_orig[i_t].flatten()
			sigma_powernoback = sigma_powernoback_full[i_t].flatten()
			reference_sigma_powernoback = reference_sigma_powernoback_all[i_t]
			# sigma_powernoback = np.ones_like(powernoback)*10
			sigma_powernoback_2 = sigma_powernoback**2

			if use_pinhole_plate_over_temperature:
				guess = np.array((np.random.random(sensitivities_binned_crop.shape[1])*1e2).tolist() + [0,0] + [1,1,1,-10])
			else:
				guess = np.array((np.random.random(sensitivities_binned_crop.shape[1])*1e2).tolist() + [0,0])

			if len(first_guess_all) == 0:
				pass
			else:
				guess = cp.deepcopy(first_guess_all[-1])
				if len(first_guess_all)>1 and time_full_binned_crop[i_t]>0.1:	# if I have the two previous solutions I can project it linearly based on the previous 2 time points. it doesn't work in too dynamic phases, it seems
					guess[:sensitivities_binned_crop.shape[1]] += (first_guess_all[-1]-first_guess_all[-2])[:sensitivities_binned_crop.shape[1]]
				guess[:sensitivities_binned_crop.shape[1]] = np.maximum(0,guess[:sensitivities_binned_crop.shape[1]])

			# if EFIT is available, I find the strike point and allow any emissivity gradient in R in its vicinity
			selected_central_column_border_cells_int = cp.deepcopy(selected_central_column_border_cells)
			if efit_reconstruction!=None:# and len(s_lookup)>1:
				i_t_efit = np.abs(time_full_binned_crop[i_t] - efit_reconstruction.time).argmin()

				# I can simply use the distance r,z from the strike point, I don't really need to use the linear distance from it
				# strike_point_s = get_nearest_s_coordinates_mastu(efit_reconstruction.strikepointR[i_t_efit],efit_reconstruction.strikepointZ[i_t_efit],s_lookup,tol=5e-3)[0]

				for i_ in range(len(efit_reconstruction.strikepointR[i_t_efit])):
					selected_central_column_border_cells_int[(r_int-efit_reconstruction.strikepointR[i_t_efit][i_])**2 + (z_int+np.abs(efit_reconstruction.strikepointZ[i_t_efit][i_]))**2 < tollerance_for_strike_point**2] = False

			prob_and_gradient,calc_hessian = define_fitting_functions(homogeneous_scaling,regolarisation_coeff_divertor_multiplier,regolarisation_coeff_central_column_border_R_derivate_multiplier,regolarisation_coeff_central_border_Z_derivate_multiplier,regolarisation_coeff_edge_laplacian_multiplier,sensitivities_binned_crop,selected_ROI_internal,select_foil_region_with_plasma,grid_laplacian_masked_crop_scaled,not_selected_super_x_cells,selected_edge_cells_for_laplacian,selected_super_x_cells,selected_central_column_border_cells_int,selected_central_border_cells,regolarisation_coeff_non_negativity_multiplier,selected_edge_cells,r_int,regolarisation_coeff_edge,regolarisation_coeff_offsets_multiplier,number_cells_ROI,reference_sigma_powernoback,number_cells_plasma,r_int_2,grid_R_derivate_masked_crop_scaled,grid_Z_derivate_masked_crop_scaled, rmax_pinhole_plate=rmax_pinhole_plate,R_IRVB_head_from_pinhole=R_IRVB_head_from_pinhole,pinhole_plate_to_foil_weight=pinhole_plate_to_foil_weight,IRVB_head_ring_to_foil_weight=IRVB_head_ring_to_foil_weight,ref_temperature=ref_temperature,use_pinhole_plate_over_temperature=use_pinhole_plate_over_temperature)

			if False:	# only for testinf the prob_and_gradient function
				args = [powernoback,sigma_powernoback,sigma_emissivity,regolarisation_coeff,sigma_powernoback**2,sigma_emissivity**2]
				target = 1297
				scale = 1e-2
				# guess = np.array((np.random.random(sensitivities_binned_crop.shape[1]+2)*1e2).tolist() + [1,1,0,-10])
				# guess = np.array((np.ones(sensitivities_binned_crop.shape[1]+2)*1e2).tolist() + [10,10,0,-10])
				# guess[target] *=10
				# guess = np.array((np.random.random(sensitivities_binned_crop.shape[1]+2)*1e2).tolist() )
				# DT_max = emissivity_plus[-4]
				# DT_head = emissivity_plus[-3]
				# DT_ring = emissivity_plus[-2]
				# pinhole_plate_T_slope = emissivity_plus[-1]

				# guess[target] = 1e5
				temp1 = prob_and_gradient(guess,*args)
				print(temp1[0])
				guess[target] +=scale
				temp2 = prob_and_gradient(guess,*args)
				guess[target] += -2*scale
				temp3 = prob_and_gradient(guess,*args)
				guess[target] += scale
				print('calculated derivated of %.7g vs true of %.7g' %(temp1[1][target],((temp2[0]-temp3[0])/(2*scale))))
			else:
				pass


			if pass_number<2:
				regolarisation_coeff_range = np.array([regolarisation_coeff])
			else:
				# regolarisation_coeff_range = 10**np.linspace(1,-6,num=120)
				# regolarisation_coeff_range = 10**np.linspace(1,-5,num=102)
				# regolarisation_coeff_range = 10**np.linspace(0.5,-5,num=94)
				# regolarisation_coeff_range = 10**np.linspace(0.,-5,num=85)
				# regolarisation_coeff_range = 10**np.linspace(-0.5,-5,num=77)
				regolarisation_coeff_range = 10**np.arange(-0.5,-7,-0.06)
				# regolarisation_coeff_range = 10**np.linspace(0.5,-4.5,num=85)
				# if laser_framerate<60:
				# 	regolarisation_coeff_range = 10**np.linspace(0.5,-4,num=76)
			if pass_number<1:
				# pgtol = 4e-7
				factr=1e5
				# modified 2024/10/01
				pgtol = 8e-6
				maxiter=110	# more than pgtol, this seems to be rigorous at limiting pointless iterations
				if time_full_binned_crop[i_t]>0.2:
					maxiter=150
			else:
				# pgtol = 4e-7
				pgtol = 4e-7
				factr=1e2
				# maxiter=15000
				# modified 2024/10/01
				maxiter=250	# more than pgtol, this seems to be rigorous at limiting pointless iterations

			if (override_second_pass and pass_number==1) or (override_third_pass and pass_number==2) or pass_number==0:
				# x_optimal_all,recompose_voxel_emissivity_all,y_opt_all,opt_info_all,voxels_centre,recompose_voxel_emissivity_excluded_all = loop_fit_over_regularisation(prob_and_gradient,regolarisation_coeff_range,guess,grid_data_masked_crop,powernoback,sigma_powernoback,sigma_emissivity,x_optimal_all_guess=x_optimal_all_guess,factr=1e10,excluded_cells = selected_edge_cells_for_laplacian)
				x_optimal_all,recompose_voxel_emissivity_all,y_opt_all,opt_info_all,voxels_centre,recompose_voxel_emissivity_excluded_all,likelihood_logging_all,derivate_logging_all = loop_fit_over_regularisation(prob_and_gradient,regolarisation_coeff_range,guess,grid_data_masked_crop,powernoback,sigma_powernoback,sigma_emissivity,excluded_cells = selected_edge_cells_for_laplacian,fix_pinhole_emis_zero=fix_pinhole_emis_zero,pgtol=pgtol,factr=factr,maxiter=maxiter,iprint=0,m=10)#,factr=1e8)#,iprint=2,pgtol=5e-5)

				# if first_guess == []:
				x_optimal_all_guess = cp.deepcopy(x_optimal_all)	# W/m^3/str of emitter
				first_guess = x_optimal_all[0]

				regolarisation_coeff_range = np.flip(regolarisation_coeff_range,axis=0)
				x_optimal_all = np.flip(x_optimal_all,axis=0)
				recompose_voxel_emissivity_all = np.flip(recompose_voxel_emissivity_all,axis=0)	# W/m^3
				recompose_voxel_emissivity_excluded_all = np.flip(recompose_voxel_emissivity_excluded_all,axis=0)
				y_opt_all = np.flip(y_opt_all,axis=0)
				opt_info_all = np.flip(opt_info_all,axis=0)

				IRVB_head_power_correction_all_int = 0
				if use_pinhole_plate_over_temperature:
					DT_head_r = []
					IRVB_head_power_correction_all_int = []
					for i in range(len(x_optimal_all)):
						DT_max,DT_head,DT_ring,pinhole_plate_T_slope = x_optimal_all[i][-4:]*homogeneous_scaling
						# pinhole_plate_T_slope = x_optimal_all[i][-1]
						temp1,temp2 = calc_IRVB_head_power_correction(DT_max,DT_head,DT_ring,pinhole_plate_T_slope,rmax_pinhole_plate,pinhole_radious,R_IRVB_head_from_pinhole,pinhole_plate_to_foil_weight,ref_temperature,IRVB_head_ring_to_foil_weight)
						IRVB_head_power_correction_all_int.append( temp2.flatten())
					IRVB_head_power_correction_all_int = np.array(IRVB_head_power_correction_all_int)

				# score_x = np.sum(((np.dot(sensitivities_binned_crop,x_optimal_all[:,:-2].T).T  + (np.array([selected_ROI_internal.tolist()]*len(x_optimal_all)).T*x_optimal_all[:,-1]).T*homogeneous_scaling + (np.array([select_foil_region_with_plasma.tolist()]*len(x_optimal_all)).T*x_optimal_all[:,-2]).T*homogeneous_scaling  - powernoback) ** 2) / (sigma_powernoback**2),axis=1)
				# score_y = np.sum(((np.dot(grid_laplacian_masked_crop_scaled,(np.logical_not(selected_edge_cells_for_laplacian)*x_optimal_all[:,:-2]).T).T) ** 2) / (sigma_emissivity**2),axis=1)
				score_x = np.sum(((np.dot(sensitivities_binned_crop,x_optimal_all[:,:len(grid_data_masked_crop)].T).T  + (np.array([selected_ROI_internal.tolist()]*len(x_optimal_all)).T*x_optimal_all[:,len(grid_data_masked_crop)+1]).T*homogeneous_scaling + (np.array([select_foil_region_with_plasma.tolist()]*len(x_optimal_all)).T*x_optimal_all[:,len(grid_data_masked_crop)]).T*homogeneous_scaling + IRVB_head_power_correction_all_int - powernoback) ** 2) / (sigma_powernoback**2),axis=1)
				score_y = np.sum(((np.dot(grid_laplacian_masked_crop_scaled,(np.logical_not(selected_edge_cells_for_laplacian)*x_optimal_all[:,:len(grid_data_masked_crop)]).T).T) ** 2) / (sigma_emissivity**2),axis=1)

				score_x_all.append(score_x)
				score_y_all.append(score_y)
				regolarisation_coeff_range_all.append(regolarisation_coeff_range)
			else:	# this is useful if I have already the scores and I use a different method to find the best regularisation
				temp = np.load(laser_to_analyse[:-4]+'_FAST'+'.npz')
				temp.allow_pickle=True
				temp = dict(temp)
				# temp['second_pass'] = temp['second_pass'].all()
				if pass_number==1:
					temp = temp['second_pass'].all()['inverted_dict']
				else:
					temp = temp['third_pass'].all()['inverted_dict']
				score_x = temp[str(grid_resolution)]['score_x_all'][i_t]
				score_y = temp[str(grid_resolution)]['score_y_all'][i_t]
				regolarisation_coeff_range = temp[str(grid_resolution)]['regolarisation_coeff_range_all'][i_t]
				x_optimal_all = np.zeros_like(regolarisation_coeff_range)
				score_x_all.append(score_x)
				score_y_all.append(score_y)
				regolarisation_coeff_range_all.append(regolarisation_coeff_range)

				trash,trash,trash,trash,trash,trash,trash,trash,regolarisation_coeff,trash,trash,trash,trash,trash = find_optimal_regularisation_minimal(score_x,score_y,regolarisation_coeff_range,x_optimal_all,regolarisation_coeff_upper_limit=regolarisation_coeff_upper_limit,regolarisation_coeff_lower_limit=regolarisation_coeff_lower_limit,fraction_of_L_curve_for_fit = fraction_of_L_curve_for_fit)
				x_optimal_all,recompose_voxel_emissivity_all,y_opt_all,opt_info_all,voxels_centre,recompose_voxel_emissivity_excluded_all,likelihood_logging_all,derivate_logging_all = loop_fit_over_regularisation(prob_and_gradient,np.array([regolarisation_coeff]),guess,grid_data_masked_crop,powernoback,sigma_powernoback,sigma_emissivity,excluded_cells = selected_edge_cells_for_laplacian,fix_pinhole_emis_zero=fix_pinhole_emis_zero,pgtol=pgtol,factr=factr,maxiter=maxiter)#,factr=1e8)

				x_optimal_all_guess = cp.deepcopy(x_optimal_all)	# W/m^3/str of emitter
				first_guess = x_optimal_all[0]

				x_optimal_all = np.array(list(x_optimal_all)*len(score_x))
				recompose_voxel_emissivity_all = np.array(list(recompose_voxel_emissivity_all)*len(score_x))	# W/m^3
				y_opt_all = np.array(list(y_opt_all)*len(score_x))
				opt_info_all = np.array(list(opt_info_all)*len(score_x))
				recompose_voxel_emissivity_excluded_all = np.array(list(recompose_voxel_emissivity_excluded_all)*len(score_x))

			first_guess_all.append(first_guess)
			iterations_per_iteration.append([val['nit'] for val in opt_info_all])

			IRVB_head_power_correction = 0
			if pass_number<2:
				recompose_voxel_emissivity,x_optimal,regolarisation_coeff,y_opt,opt_info,recompose_voxel_emissivity_excluded = recompose_voxel_emissivity_all[0],x_optimal_all[0],regolarisation_coeff_range[0],y_opt_all[0],opt_info_all[0],recompose_voxel_emissivity_excluded_all[0]
				if pass_number==1 and False:
					plt.figure()
					plt.imshow(recompose_voxel_emissivity,'rainbow')
					plt.colorbar()
					plt.savefig(filename_root+filename_root_add+'_'+ 't=%.4gms' %(time_full_binned_crop[i_t]*1e3) +'.eps')
					plt.close()

				if use_pinhole_plate_over_temperature:
					IRVB_head_power_correction = IRVB_head_power_correction_all_int[0]
				likelihood_logging,derivate_logging = likelihood_logging_all[0],derivate_logging_all[0]
				iterations_per_iteration[-1] = iterations_per_iteration[-1][0]
			else:
				score_y,score_x,score_y_record_rel,score_x_record_rel,curvature_range,Lcurve_curvature,recompose_voxel_emissivity,x_optimal,points_removed,regolarisation_coeff,regolarisation_coeff_range,y_opt,opt_info,curvature_range_left_all,curvature_range_right_all,peaks,best_index,recompose_voxel_emissivity_excluded = find_optimal_regularisation(score_x,score_y,regolarisation_coeff_range,x_optimal_all,recompose_voxel_emissivity_all,y_opt_all,opt_info_all,recompose_voxel_emissivity_excluded_all,regolarisation_coeff_upper_limit=regolarisation_coeff_upper_limit,regolarisation_coeff_lower_limit=regolarisation_coeff_lower_limit,fraction_of_L_curve_for_fit = fraction_of_L_curve_for_fit)
				if use_pinhole_plate_over_temperature:
					IRVB_head_power_correction = IRVB_head_power_correction_all[best_index]
				likelihood_logging,derivate_logging = likelihood_logging_all[best_index],derivate_logging_all[best_index]
				iterations_per_iteration[-1] = np.nansum(iterations_per_iteration[-1])

				plt.figure(10)
				# plt.plot(score_x,score_y,color=str(0.9-i_t/(len(time_full_binned_crop)/0.9)))
				plt.plot(np.log(score_x_all[-1]),np.log(score_y_all[-1]),color=str(0.9-i_t/(len(time_full_binned_crop)/0.9)))
				plt.plot(score_x,score_y,'+',color=str(0.9-i_t/(len(time_full_binned_crop)/0.9)))
				plt.plot(score_x[best_index],score_y[best_index],'o',color='b')
				plt.plot(score_x[peaks],score_y[peaks],'o',color=str(0.9-i_t/(len(time_full_binned_crop)/0.9)),fillstyle='none',markersize=10)
				plt.plot(score_x[np.abs(regolarisation_coeff_range-regolarisation_coeff_upper_limit).argmin()],score_y[np.abs(regolarisation_coeff_range-regolarisation_coeff_upper_limit).argmin()],'s',color='r')
				plt.plot(score_x[np.abs(regolarisation_coeff_range-regolarisation_coeff_lower_limit).argmin()],score_y[np.abs(regolarisation_coeff_range-regolarisation_coeff_lower_limit).argmin()],'s',color='r')
				plt.xlabel('log ||Gm-d||2')
				plt.ylabel('log ||Laplacian(m)||2')
				plt.title('shot ' + laser_to_analyse[-9:-4]+' '+scenario+'L-curve evolution\nlight=early, dark=late\ntime_per_iteration [s] '+str(np.round(time_per_iteration).astype(int)))
				plt.savefig(filename_root+filename_root_add+'_L_curve_evolution.eps')
				plt.figure(11)
				plt.plot(regolarisation_coeff_range,Lcurve_curvature,color=str(0.9-i_t/(len(time_full_binned_crop)/0.9)))
				plt.plot(regolarisation_coeff_range,Lcurve_curvature,'+',color=str(0.9-i_t/(len(time_full_binned_crop)/0.9)))
				plt.plot(regolarisation_coeff_range[best_index],Lcurve_curvature[best_index],'o',color='b')
				plt.plot(regolarisation_coeff_range[peaks],Lcurve_curvature[peaks],'o',color=str(0.9-i_t/(len(time_full_binned_crop)/0.9)),fillstyle='none',markersize=10)
				plt.axvline(x=regolarisation_coeff_upper_limit,color='r')
				plt.axvline(x=regolarisation_coeff_lower_limit,color='r')
				plt.semilogx()
				plt.xlabel('regularisation coeff')
				plt.ylabel('L-curve turvature')
				plt.title('shot ' + laser_to_analyse[-9:-4]+' '+scenario+'L-curve curvature evolution\nlight=early, dark=late\nfraction_of_L_curve_for_fit = '+str(fraction_of_L_curve_for_fit)+'\ncurvature_range_left_all = '+str(curvature_range_left_all)+'\ncurvature_range_right_all = '+str(curvature_range_right_all))
				plt.savefig(filename_root+filename_root_add+'_L_curve_curvature_evolution.eps')


			# foil_power_guess = np.dot(sensitivities_binned_crop,x_optimal[:-2])+x_optimal[-2]*select_foil_region_with_plasma*homogeneous_scaling+x_optimal[-1]*selected_ROI_internal*homogeneous_scaling
			foil_power_guess = np.dot(sensitivities_binned_crop,x_optimal[:len(grid_data_masked_crop)])+x_optimal[len(grid_data_masked_crop)]*select_foil_region_with_plasma*homogeneous_scaling+x_optimal[len(grid_data_masked_crop)+1]*selected_ROI_internal*homogeneous_scaling + IRVB_head_power_correction
			foil_power_error = powernoback - foil_power_guess
			chi_square = np.sum((foil_power_error/sigma_powernoback)**2)
			print('chi_square '+str(chi_square))

			# regolarisation_coeff_edge = regolarisation_coeff*regolarisation_coeff_edge_multiplier
			regolarisation_coeff_central_border_Z_derivate = regolarisation_coeff*regolarisation_coeff_central_border_Z_derivate_multiplier
			regolarisation_coeff_central_column_border_R_derivate = regolarisation_coeff*regolarisation_coeff_central_column_border_R_derivate_multiplier
			# regolarisation_coeff_edge_laplacian = regolarisation_coeff*regolarisation_coeff_edge_laplacian_multiplier
			regolarisation_coeff_divertor = regolarisation_coeff*regolarisation_coeff_divertor_multiplier

			if False:	# only for visualisation
				plt.figure(figsize=(12,13))
				# plt.scatter(np.mean(grid_data_masked_crop,axis=1)[:,0],np.mean(grid_data_masked_crop,axis=1)[:,1],c=x_optimal,s=100,marker='s',cmap='rainbow')
				plt.imshow(np.flip(np.flip(np.flip(np.transpose(recompose_voxel_emissivity,(1,0)),axis=1),axis=1),axis=0),extent=[grid_data_masked_crop[:,:,0].min(),grid_data_masked_crop[:,:,0].max(),grid_data_masked_crop[:,:,1].min(),grid_data_masked_crop[:,:,1].max()])
				plt.plot(_MASTU_CORE_GRID_POLYGON[:, 0], _MASTU_CORE_GRID_POLYGON[:, 1], 'k')
				temp = np.abs(efit_reconstruction.time-time_full_binned_crop[i_t]).argmin()
				for i in range(len(all_time_sep_r[temp])):
					plt.plot(r_fine[all_time_sep_r[temp][i]],z_fine[all_time_sep_z[temp][i]],'--b')
				plt.plot(efit_reconstruction.lower_xpoint_r[temp],efit_reconstruction.lower_xpoint_z[temp],'xr')
				plt.plot(efit_reconstruction.strikepointR[temp],efit_reconstruction.strikepointZ[temp],'xr')
				plt.title('sigma_emissivity %.3g\nregolarisation_coeff %.3g\nregolarisation_coeff_edge %.3g\nregolarisation_coeff_central_border_Z_derivate %.3g\nregolarisation_coeff_central_column_border_R_derivate %.3g' %(sigma_emissivity,regolarisation_coeff,regolarisation_coeff_edge,regolarisation_coeff_central_border_Z_derivate,regolarisation_coeff_central_column_border_R_derivate))
				plt.colorbar().set_label('emissivity [W/m3]')
				plt.ylim(top=0.5)
				# plt.savefig(path_power_output + '/'+ str(shot_number)+'_'+binning_type+'_gridres'+str(grid_resolution)+'cm_example19.eps')
				plt.pause(0.01)
			else:
				pass

			inverted_data.append(recompose_voxel_emissivity)	# this is WITHOUT the excluded part	# W/m^3
			inverted_data_excluded.append(recompose_voxel_emissivity_excluded)	# W/m^3
			x_optimal_ext.append(x_optimal)	# W/m^3/str of emitter
			inverted_data_likelihood.append(y_opt)
			inverted_data_info.append(opt_info)
			IRVB_head_power_correction = 0
			DT_head_r = 0
			if use_pinhole_plate_over_temperature:
				DT_max_all.append(x_optimal[-4]*homogeneous_scaling)
				DT_head_all.append(x_optimal[-3]*homogeneous_scaling)
				DT_ring_all.append(x_optimal[-2]*homogeneous_scaling)
				pinhole_plate_T_slope_all.append(x_optimal[-1]*homogeneous_scaling)
				DT_head_r,IRVB_head_power_correction = calc_IRVB_head_power_correction(DT_max_all[-1],DT_head_all[-1],DT_ring_all[-1],pinhole_plate_T_slope_all[-1],rmax_pinhole_plate,pinhole_radious,R_IRVB_head_from_pinhole,pinhole_plate_to_foil_weight,ref_temperature,IRVB_head_ring_to_foil_weight)
			DT_head_r_all.append(DT_head_r)
			IRVB_head_power_correction_all.append(IRVB_head_power_correction)
			# inverted_data_plasma_region_offset.append(x_optimal[-2]*homogeneous_scaling)
			# inverted_data_homogeneous_offset.append(x_optimal[-1]*homogeneous_scaling)
			# fitted_foil_power.append((np.dot(sensitivities_binned_crop,x_optimal[:-2])+x_optimal[-2]*select_foil_region_with_plasma*homogeneous_scaling+x_optimal[-1]*selected_ROI_internal*homogeneous_scaling).reshape(powernoback_full_orig[i_t].shape))
			# fitted_foil_power_excluded.append((np.dot(sensitivities_binned_crop,x_optimal[:-2]*selected_edge_cells_for_laplacian)).reshape(powernoback_full_orig[i_t].shape))
			inverted_data_plasma_region_offset.append(x_optimal[len(grid_data_masked_crop)]*homogeneous_scaling)
			inverted_data_homogeneous_offset.append(x_optimal[len(grid_data_masked_crop)+1]*homogeneous_scaling)
			# fitted_foil_power.append((np.dot(sensitivities_binned_crop,x_optimal[:len(grid_data_masked_crop)])+x_optimal[len(grid_data_masked_crop)]*select_foil_region_with_plasma*homogeneous_scaling+x_optimal[len(grid_data_masked_crop)+1]*selected_ROI_internal*homogeneous_scaling).reshape(powernoback_full_orig[i_t].shape) + IRVB_head_power_correction)
			# I want a formula that takes my emissivities and applies them over the entire foil
			fitted_foil_power.append((np.dot(sensitivities_binned.reshape((len(select_foil_region_with_plasma),sensitivities_binned.shape[-1])),np.dot(sensitivity_matrix_equivalence,x_optimal[:len(grid_data_masked_crop)]))+x_optimal[len(grid_data_masked_crop)]*select_foil_region_with_plasma*homogeneous_scaling+x_optimal[len(grid_data_masked_crop)+1]*selected_ROI_internal*homogeneous_scaling).reshape(powernoback_full_orig[i_t].shape) + IRVB_head_power_correction)
			fitted_foil_power_excluded.append((np.dot(sensitivities_binned_crop,x_optimal[:len(grid_data_masked_crop)]*selected_edge_cells_for_laplacian) +x_optimal[len(grid_data_masked_crop)]*select_foil_region_with_plasma*homogeneous_scaling+x_optimal[len(grid_data_masked_crop)+1]*selected_ROI_internal*homogeneous_scaling).reshape(powernoback_full_orig[i_t].shape) + IRVB_head_power_correction)
			foil_power.append(powernoback_full_orig[i_t])
			foil_power_std.append(sigma_powernoback_full[i_t])
			foil_power_residuals.append(powernoback_full_orig[i_t]-fitted_foil_power[-1])
			print('powernoback_full_orig')
			print(np.shape(powernoback_full_orig))
			print('selected_ROI')
			print(np.shape(selected_ROI))
			print('fitted_foil_power')
			print(np.shape(fitted_foil_power))
			fit_error.append(np.sum(((powernoback_full_orig[i_t][selected_ROI]-fitted_foil_power[-1][selected_ROI]))**2)**0.5/np.sum(selected_ROI))
			chi_square_all.append(chi_square)
			regolarisation_coeff_all.append(regolarisation_coeff)
			if pass_number>1:
				for value in points_removed:
					Lcurve_curvature = np.concatenate([Lcurve_curvature[:value],[np.nan],Lcurve_curvature[value:]])
				Lcurve_curvature_all.append(Lcurve_curvature)
			likelihood_logging_ext.append(likelihood_logging)
			derivate_logging_ext.append(derivate_logging)

			# args = [powernoback,sigma_powernoback,sigma_emissivity,regolarisation_coeff,sigma_powernoback**2,sigma_emissivity**2]
			# hessian=calc_hessian(x_optimal,*args)	# W/m^3
			# covariance = np.linalg.inv(hessian)
			# trash,recompose_voxel_sigma,recompose_voxel_excluded_sigma = translate_emissivity_profile_with_homo_temp(np.mean(grid_data_masked_crop,axis=1,dtype=np.float32),(np.diag(covariance).astype(np.float32))**0.5,np.mean(grid_data_masked_crop,axis=1),cells_to_exclude=selected_edge_cells_for_laplacian)
			# inverted_data_sigma.append(recompose_voxel_sigma)	# W/m^3
			# inverted_data_excluded_sigma.append(recompose_voxel_excluded_sigma)	# W/m^3
			# inverted_data_covariance.append(covariance)	# W/m^3

			# inverted_dict[str(grid_resolution)]['score_x_all'] = score_x_all
			# inverted_dict[str(grid_resolution)]['score_y_all'] = score_y_all
			# inverted_dict[str(grid_resolution)]['regolarisation_coeff_range_all'] = regolarisation_coeff_range_all
			# inverted_dict[str(grid_resolution)]['Lcurve_curvature_all'] = Lcurve_curvature_all
			# inverted_dict[str(grid_resolution)]['foil_power_residuals'] = foil_power_residuals
			# inverted_dict[str(grid_resolution)]['fitted_foil_power'] = fitted_foil_power
			# full_saved_file_dict_FAST = dict([])
			# full_saved_file_dict_FAST['inverted_dict'] = inverted_dict
			# np.savez_compressed(laser_to_analyse[:-4]+'_FAST',**full_saved_file_dict_FAST)

			# 2024/10/01 added to show the evolution of convergence and to monitor live what's happening
			alpha = 0.7
			# colors_smooth = np.array([np.linspace(0,1,num=number_of_curves_to_plot),np.linspace(1,0,num=number_of_curves_to_plot),[0]*number_of_curves_to_plot]).T
			# colors_smooth = np.linspace(0,0.9,num=int(np.ceil(len(time_full_binned_crop)/4))).astype(str)
			colors_smooth = np.linspace(0,0.9,num=5).astype(str)
			colors_smooth = colors_smooth.tolist()+[val[0]+val[1] for val in np.array([['C']*30,np.arange(10).tolist()+np.arange(10).tolist()+np.arange(10).tolist()]).T]
			fig, ax = plt.subplots( 2,1,figsize=(20, 20), squeeze=False,sharex=True)
			fig.suptitle('shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\ntime spent per iteration (total %.3gmin)\npgtol=%.3g, factr=%.3g, maxiter=%.3g, homo scale=%.3g' %(np.sum(time_per_iteration)/60,pgtol,factr,maxiter,homogeneous_scaling))
			repeating_frequency = 4
			if len(time_full_binned_crop)>60:
				repeating_frequency = int(len(time_full_binned_crop)/(4*15))	# this way I skip a number of steps, to limit the number of lines plotted
				fig.suptitle('shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\ntime spent per iteration (total %.3gmin)\npgtol=%.3g, factr=%.3g, maxiter=%.3g\neach linestyle repetition is worth ' %(np.sum(time_per_iteration)/60,pgtol,factr,maxiter) +str(repeating_frequency)+' timesteps instead of 4')
			# plt.figure()
			for i_i_t,temp_t in enumerate(time_full_binned_crop):
				try:
					if i_i_t%repeating_frequency==0:
						ax[0,0].plot(likelihood_logging_ext[i_i_t],'-',color=colors_smooth[i_i_t//repeating_frequency],label = '%.3gs' %(temp_t),linewidth=1,alpha=alpha)
						ax[1,0].plot(derivate_logging_ext[i_i_t],'-',color=colors_smooth[i_i_t//repeating_frequency],linewidth=1,alpha=alpha)
					elif i_i_t%repeating_frequency==1:
						ax[0,0].plot(likelihood_logging_ext[i_i_t],'-.',color=colors_smooth[i_i_t//repeating_frequency],label = '%.3gs' %(temp_t),linewidth=1,alpha=alpha)
						ax[1,0].plot(derivate_logging_ext[i_i_t],'-.',color=colors_smooth[i_i_t//repeating_frequency],linewidth=1,alpha=alpha)
					elif i_i_t%repeating_frequency==2:
						ax[0,0].plot(likelihood_logging_ext[i_i_t],':',color=colors_smooth[i_i_t//repeating_frequency],label = '%.3gs' %(temp_t),linewidth=1,alpha=alpha)
						ax[1,0].plot(derivate_logging_ext[i_i_t],':',color=colors_smooth[i_i_t//repeating_frequency],linewidth=1,alpha=alpha)
					elif i_i_t%repeating_frequency==3:
						ax[0,0].plot(likelihood_logging_ext[i_i_t],'--',color=colors_smooth[i_i_t//repeating_frequency],label = '%.3gs' %(temp_t),linewidth=1,alpha=alpha)
						ax[1,0].plot(derivate_logging_ext[i_i_t],'--',color=colors_smooth[i_i_t//repeating_frequency],linewidth=1,alpha=alpha)
				except:
					pass
			ax[0,0].legend(loc='best', fontsize='xx-small',ncol=3)
			ax[0,0].set_ylabel('likelihood [au]')
			ax[1,0].set_ylabel('likel gradient [au]')
			# ax[1,0].set_xlabel('distance from target [m]')
			ax[1,0].set_xlabel('iteration [au]')
			ax[0,0].grid()
			ax[1,0].grid()
			ax[0,0].semilogy()
			ax[1,0].semilogy()
			plt.savefig(filename_root+filename_root_add+'_FAST_convergence_evolution.eps')
			plt.close()

			time_per_iteration.append(tm.time()-time_start)

			plt.figure(figsize=(20, 10))
			plt.plot(time_full_binned_crop[:len(time_per_iteration)],time_per_iteration)
			task = np.array([gna['task'] for gna in np.array(inverted_data_info)])
			for i__ in range(len(np.unique(task))):
				plt.plot(time_full_binned_crop[:len(time_per_iteration)][task==np.unique(task)[i__]],np.array(time_per_iteration)[task==np.unique(task)[i__]],'o',color='C'+str(i__),label='<- '+np.unique(task)[i__])
			# plt.semilogy()
			plt.legend(loc='upper left', fontsize='x-small')
			plt.ylabel('time [s]')
			plt.grid()
			plt.ylim(bottom=0)
			plt.twinx()
			if pass_number==0:
				plt.plot([time_full_binned_crop.min(),0.2,0.2,time_full_binned_crop.max()],[110,110,150,150],'--k')
			if pass_number==1:
				plt.plot([time_full_binned_crop.min(),0.2,0.2,time_full_binned_crop.max()],[250,250,250,250],'--k')
			plt.plot(time_full_binned_crop[:len(iterations_per_iteration)],iterations_per_iteration,'--',label='->')
			plt.ylim(bottom=0)
			plt.legend(loc='upper right', fontsize='x-small')
			plt.ylabel('iterations [au]')
			plt.title('shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\ntime spent per iteration (sum %.3gmin)\npgtol=%.3g, factr=%.3g, maxiter=%.3g, homo scale=%.3g' %(np.sum(time_per_iteration)/60,pgtol,factr,maxiter,homogeneous_scaling))
			plt.xlabel('time [s]')
			plt.savefig(filename_root+filename_root_add+'_FAST_time_trace.eps')
			plt.close()

		print('ended inversion process at %.3gmin from calling MASTU_pulse_process_FAST3_BB' %((tm.time()-start_processing_fast)/60))

		inverted_data = np.array(inverted_data)	# W/m^3
		inverted_data_excluded = np.array(inverted_data_excluded)	# W/m^3
		inverted_data_likelihood = -np.array(inverted_data_likelihood)
		inverted_data_plasma_region_offset = np.array(inverted_data_plasma_region_offset)
		inverted_data_homogeneous_offset = np.array(inverted_data_homogeneous_offset)
		if use_pinhole_plate_over_temperature:
			DT_max_all = np.array(DT_max_all)
			DT_head_all = np.array(DT_head_all)
			DT_ring_all = np.array(DT_ring_all)
			pinhole_plate_T_slope_all = np.array(pinhole_plate_T_slope_all)
		DT_head_r_all = np.array(DT_head_r_all)
		IRVB_head_power_correction_all = np.array(IRVB_head_power_correction_all)
		inverted_data_info = np.array(inverted_data_info)
		x_optimal_ext = np.array(x_optimal_ext)	# W/m^3/str of emitter
		fit_error = np.array(fit_error)
		chi_square_all = np.array(chi_square_all)
		regolarisation_coeff_all = np.array(regolarisation_coeff_all)
		time_per_iteration = np.array(time_per_iteration)
		iterations_per_iteration = np.array(iterations_per_iteration)
		fitted_foil_power = np.array(fitted_foil_power)
		fitted_foil_power_excluded = np.array(fitted_foil_power_excluded)
		foil_power = np.array(foil_power)
		foil_power_std = np.array(foil_power_std)
		foil_power_residuals = np.array(foil_power_residuals)
		fitted_brightness = 4*np.pi*(fitted_foil_power-fitted_foil_power_excluded)/etendue

		timeout = 20*60	# 20 minutes
		# timeout = 0	# TEMPORARY to process them all
		while efit_reconstruction==None and timeout>0:
			try:
				EFIT_path_default = '/common/uda-scratch/lkogan/efitpp_eshed'
				efit_reconstruction = mclass(EFIT_path_default+'/epm0'+laser_to_analyse[-9:-4]+'.nc',pulse_ID=laser_to_analyse[-9:-4])

			except:
				print('EFIT missing, waiting 1 min')
			tm.sleep(60)
			timeout -= 60

		if efit_reconstruction!=None:

			# temp = brightness[:,:,:int(np.shape(brightness)[2]*0.75)]
			# temp = np.sort(temp[np.max(temp,axis=(1,2)).argmax()].flatten())
			ani,efit_reconstruction = movie_from_data(np.array([np.flip(np.transpose(brightness,(0,2,1)),axis=2)])/1000, 1/np.median(np.diff(time_full_binned)),timesteps=time_full_binned,integration=laser_int_time/1000,time_offset=time_full_binned[0],extvmin=0,extvmax=brightness_extvmax/1000,xlabel='horizontal coord [pixels]', ylabel='vertical coord [pixels]',barlabel='brightness [kW/m2]', prelude='shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\n'+binning_type+'\n',overlay_structure=True,include_EFIT=True,pulse_ID=laser_to_analyse[-9:-4],overlay_x_point=True,overlay_mag_axis=True,overlay_strike_points=True,overlay_separatrix=True,EFIT_output_requested=True,efit_reconstruction=efit_reconstruction,additional_polygons_dict=additional_polygons_dict)
			ani.save(filename_root+ '_FAST_brightness.mp4', fps=5*(1/np.mean(np.diff(time_full_binned)))/383, writer='ffmpeg',codec='mpeg4')
			plt.close('all')

		print('calculation of the uncertainty')
		# I had to split it because it cakes a long time, and I want the inversions earlier
		for i_t in range(len(time_full_binned_crop)):
			time_start = tm.time()

			print('starting t=%.4gms' %(time_full_binned_crop[i_t]*1e3))

			powernoback = powernoback_full_orig[i_t].flatten()
			sigma_powernoback = sigma_powernoback_full[i_t].flatten()
			reference_sigma_powernoback = reference_sigma_powernoback_all[i_t]
			# sigma_powernoback = np.ones_like(powernoback)*10
			sigma_powernoback_2 = sigma_powernoback**2

			# if EFIT is available, I find the strike point and allow any emissivity gradient in R in its vicinity
			selected_central_column_border_cells_int = cp.deepcopy(selected_central_column_border_cells)
			if efit_reconstruction!=None:# and len(s_lookup)>1:
				i_t_efit = np.abs(time_full_binned_crop[i_t] - efit_reconstruction.time).argmin()

				# I can simply use the distance r,z from the strike point, I don't really need to use the linear distance from it
				# strike_point_s = get_nearest_s_coordinates_mastu(efit_reconstruction.strikepointR[i_t_efit],efit_reconstruction.strikepointZ[i_t_efit],s_lookup,tol=5e-3)[0]

				for i_ in range(len(efit_reconstruction.strikepointR[i_t_efit])):
					selected_central_column_border_cells_int[(r_int-efit_reconstruction.strikepointR[i_t_efit][i_])**2 + (z_int+np.abs(efit_reconstruction.strikepointZ[i_t_efit][i_]))**2 < tollerance_for_strike_point**2] = False

			prob_and_gradient,calc_hessian = define_fitting_functions(homogeneous_scaling,regolarisation_coeff_divertor_multiplier,regolarisation_coeff_central_column_border_R_derivate_multiplier,regolarisation_coeff_central_border_Z_derivate_multiplier,regolarisation_coeff_edge_laplacian_multiplier,sensitivities_binned_crop,selected_ROI_internal,select_foil_region_with_plasma,grid_laplacian_masked_crop_scaled,not_selected_super_x_cells,selected_edge_cells_for_laplacian,selected_super_x_cells,selected_central_column_border_cells_int,selected_central_border_cells,regolarisation_coeff_non_negativity_multiplier,selected_edge_cells,r_int,regolarisation_coeff_edge,regolarisation_coeff_offsets_multiplier,number_cells_ROI,reference_sigma_powernoback,number_cells_plasma,r_int_2,grid_R_derivate_masked_crop_scaled,grid_Z_derivate_masked_crop_scaled, rmax_pinhole_plate=rmax_pinhole_plate,R_IRVB_head_from_pinhole=R_IRVB_head_from_pinhole,pinhole_plate_to_foil_weight=pinhole_plate_to_foil_weight,IRVB_head_ring_to_foil_weight=IRVB_head_ring_to_foil_weight,ref_temperature=ref_temperature,use_pinhole_plate_over_temperature=use_pinhole_plate_over_temperature)

			args = [powernoback,sigma_powernoback,sigma_emissivity,regolarisation_coeff_all[i_t],sigma_powernoback**2,sigma_emissivity**2]
			hessian=calc_hessian(x_optimal_ext[i_t],*args)	# W/m^3
			covariance = np.linalg.inv(hessian.astype(np.float64))
			eigenvalues, eigenvectors = np.linalg.eigh(covariance)
			# from Svensson2008, pag 20
			# eigenvalues[eigenvalues<0] = 0
			# U, s, Vh = np.linalg.svd(covariance)
			# s[s<0] = 0
			# covariance = U @ np.diag(s) @ (Vh)
			eigenvalues[eigenvalues<0] = 0
			covariance = eigenvectors @ np.diag(eigenvalues) @ (eigenvectors.T)

			trash,recompose_voxel_sigma,recompose_voxel_excluded_sigma = translate_emissivity_profile_with_homo_temp(np.mean(grid_data_masked_crop,axis=1,dtype=np.float32),(np.diag(covariance).astype(np.float32))**0.5,np.mean(grid_data_masked_crop,axis=1),cells_to_exclude=selected_edge_cells_for_laplacian)
			inverted_data_sigma.append(recompose_voxel_sigma)	# W/m^3
			inverted_data_excluded_sigma.append(recompose_voxel_excluded_sigma)	# W/m^3
			inverted_data_covariance.append(covariance)	# W/m^3
		inverted_data_sigma = np.array(inverted_data_sigma)	# W/m^3
		inverted_data_excluded_sigma = np.array(inverted_data_excluded_sigma)	# W/m^3
		inverted_data_covariance = np.array(inverted_data_covariance,dtype=np.float32)

		if efit_reconstruction!=None:
			all_time_sep_r,all_time_sep_z,r_fine,z_fine = efit_reconstruction_to_separatrix_on_foil(efit_reconstruction)
			all_time_strike_points_location = return_all_time_strike_points_location_radial(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine)
			outer_leg_tot_rad_power_all = []
			inner_leg_tot_rad_power_all = []
			core_tot_rad_power_all = []
			sxd_tot_rad_power_all = []
			x_point_tot_rad_power_all = []
			outer_leg_tot_rad_power_sigma_all = []
			inner_leg_tot_rad_power_sigma_all = []
			core_tot_rad_power_sigma_all = []
			sxd_tot_rad_power_sigma_all = []
			x_point_tot_rad_power_sigma_all = []
			for i_t in range(len(time_full_binned_crop)):
				x_optimal_no_offsets_and_plate = x_optimal_ext[i_t][:len(grid_data_masked_crop)]
				inverted_data_covariance_ = inverted_data_covariance[i_t][:len(grid_data_masked_crop),:len(grid_data_masked_crop)]
				temp = np.abs(efit_reconstruction.time-time_full_binned_crop[i_t]).argmin()
				xpoint_r = efit_reconstruction.lower_xpoint_r[temp]
				xpoint_z = efit_reconstruction.lower_xpoint_z[temp]
				z_,r_ = np.meshgrid(np.unique(voxels_centre[:,1]),np.unique(voxels_centre[:,0]))

				temp = cp.deepcopy(inverted_data[i_t])	# W/m^3
				temp[z_>xpoint_z] = 0
				temp[r_<xpoint_r] = 0
				outer_leg_tot_rad_power = np.nansum(temp*2*np.pi*r_*((grid_resolution*0.01)**2))
				temp_sigma = np.ones_like(x_optimal_no_offsets_and_plate)
				temp_sigma[np.mean(grid_data_masked_crop,axis=1)[:,1]>xpoint_z] = 0
				temp_sigma[np.mean(grid_data_masked_crop,axis=1)[:,0]<xpoint_r] = 0
				# outer_leg_tot_rad_power_sigma = np.nansum((temp_sigma*2*np.pi*r_*((grid_resolution*0.01)**2))**2)**0.5
				temp = ((temp_sigma!=0)*2*np.pi*np.mean(grid_data_masked_crop,axis=1)[:,0]*((grid_resolution*0.01)**2))
				outer_leg_tot_rad_power_sigma = np.nansum(((inverted_data_covariance_*temp).T*temp).T)**0.5

				temp = cp.deepcopy(inverted_data[i_t])	# W/m^3
				temp[z_>xpoint_z] = 0
				temp[r_>xpoint_r] = 0
				inner_leg_tot_rad_power = np.nansum(temp*2*np.pi*r_*((grid_resolution*0.01)**2))
				temp_sigma = np.ones_like(x_optimal_no_offsets_and_plate)
				temp_sigma[np.mean(grid_data_masked_crop,axis=1)[:,1]>xpoint_z] = 0
				temp_sigma[np.mean(grid_data_masked_crop,axis=1)[:,0]>xpoint_r] = 0
				# inner_leg_tot_rad_power_sigma = np.nansum((temp_sigma*2*np.pi*r_*((grid_resolution*0.01)**2))**2)**0.5
				temp = (temp_sigma!=0)*2*np.pi*np.mean(grid_data_masked_crop,axis=1)[:,0]*((grid_resolution*0.01)**2)
				inner_leg_tot_rad_power_sigma = np.nansum(((inverted_data_covariance_*temp).T*temp).T)**0.5

				temp = cp.deepcopy(inverted_data[i_t])	# W/m^3
				temp[z_<xpoint_z] = 0
				temp[z_>0] = 0
				core_tot_rad_power = np.nansum(temp*2*np.pi*r_*((grid_resolution*0.01)**2))
				temp_sigma = np.ones_like(x_optimal_no_offsets_and_plate)
				temp_sigma[np.mean(grid_data_masked_crop,axis=1)[:,1]<xpoint_z] = 0
				temp_sigma[np.mean(grid_data_masked_crop,axis=1)[:,1]>0] = 0
				# core_tot_rad_power_sigma = np.nansum((temp_sigma*2*np.pi*r_*((grid_resolution*0.01)**2))**2)**0.5
				temp = (temp_sigma!=0)*2*np.pi*np.mean(grid_data_masked_crop,axis=1)[:,0]*((grid_resolution*0.01)**2)
				core_tot_rad_power_sigma = np.nansum(((inverted_data_covariance_*temp).T*temp).T)**0.5

				temp = cp.deepcopy(inverted_data[i_t])	# W/m^3
				# temp[z_>-1.5] = 0
				# temp[r_<0.8] = 0
				# temp[z_>-1.550] = 0	# this might be reworked one the phantom scans are done
				# temp[r_<0.800] = 0	# this might be reworked one the phantom scans are done
				MU01_sxd_region_delimiter = return_MU01_sxd_region_delimiter()
				temp[z_>=MU01_sxd_region_delimiter(r_)] = 0	# delimiter as used in the thesis
				sxd_tot_rad_power = np.nansum(temp*2*np.pi*r_*((grid_resolution*0.01)**2))
				temp_sigma = np.ones_like(x_optimal_no_offsets_and_plate)
				# temp_sigma[z_>-1.5] = 0
				# temp_sigma[r_<0.8] = 0
				# temp_sigma[np.mean(grid_data_masked_crop,axis=1)[:,1]>-1.550] = 0	# this might be reworked one the phantom scans are done
				# temp_sigma[np.mean(grid_data_masked_crop,axis=1)[:,0]<0.800] = 0	# this might be reworked one the phantom scans are done
				temp_sigma[np.mean(grid_data_masked_crop,axis=1)[:,1]>=MU01_sxd_region_delimiter(np.mean(grid_data_masked_crop,axis=1)[:,0])] = 0	# delimiter as used in the thesis
				# sxd_tot_rad_power_sigma = np.nansum((temp_sigma*2*np.pi*r_*((grid_resolution*0.01)**2))**2)**0.5
				temp = (temp_sigma!=0)*2*np.pi*np.mean(grid_data_masked_crop,axis=1)[:,0]*((grid_resolution*0.01)**2)
				sxd_tot_rad_power_sigma = np.nansum(((inverted_data_covariance_*temp).T*temp).T)**0.5

				temp = cp.deepcopy(inverted_data[i_t])	# W/m^3
				temp[((z_-xpoint_z)**2+(r_-xpoint_r)**2)**0.5>x_point_region_radious] = 0
				x_point_tot_rad_power = np.nansum(temp*2*np.pi*r_*((grid_resolution*0.01)**2))
				temp_sigma = np.ones_like(x_optimal_no_offsets_and_plate)
				temp_sigma[((np.mean(grid_data_masked_crop,axis=1)[:,1]-xpoint_z)**2+(np.mean(grid_data_masked_crop,axis=1)[:,0]-xpoint_r)**2)**0.5>x_point_region_radious] = 0
				# x_point_tot_rad_power_sigma = np.nansum((temp_sigma*2*np.pi*r_*((grid_resolution*0.01)**2))**2)**0.5
				temp = (temp_sigma!=0)*2*np.pi*np.mean(grid_data_masked_crop,axis=1)[:,0]*((grid_resolution*0.01)**2)
				x_point_tot_rad_power_sigma = np.nansum(((inverted_data_covariance_*temp).T*temp).T)**0.5

				outer_leg_tot_rad_power_all.append(outer_leg_tot_rad_power)
				inner_leg_tot_rad_power_all.append(inner_leg_tot_rad_power)
				core_tot_rad_power_all.append(core_tot_rad_power)
				sxd_tot_rad_power_all.append(sxd_tot_rad_power)
				x_point_tot_rad_power_all.append(x_point_tot_rad_power)
				outer_leg_tot_rad_power_sigma_all.append(outer_leg_tot_rad_power_sigma)
				inner_leg_tot_rad_power_sigma_all.append(inner_leg_tot_rad_power_sigma)
				core_tot_rad_power_sigma_all.append(core_tot_rad_power_sigma)
				sxd_tot_rad_power_sigma_all.append(sxd_tot_rad_power_sigma)
				x_point_tot_rad_power_sigma_all.append(x_point_tot_rad_power_sigma)
			outer_leg_tot_rad_power_all = np.array(outer_leg_tot_rad_power_all)
			inner_leg_tot_rad_power_all = np.array(inner_leg_tot_rad_power_all)
			core_tot_rad_power_all = np.array(core_tot_rad_power_all)
			sxd_tot_rad_power_all = np.array(sxd_tot_rad_power_all)
			x_point_tot_rad_power_all = np.array(x_point_tot_rad_power_all)
			outer_leg_tot_rad_power_sigma_all = np.array(outer_leg_tot_rad_power_sigma_all)
			inner_leg_tot_rad_power_sigma_all = np.array(inner_leg_tot_rad_power_sigma_all)
			core_tot_rad_power_sigma_all = np.array(core_tot_rad_power_sigma_all)
			sxd_tot_rad_power_sigma_all = np.array(sxd_tot_rad_power_sigma_all)
			x_point_tot_rad_power_sigma_all = np.array(x_point_tot_rad_power_sigma_all)

			plt.figure(figsize=(15, 10))
			plt.errorbar(time_full_binned_crop,outer_leg_tot_rad_power_all/1e3,yerr=outer_leg_tot_rad_power_sigma_all/1e3,label='outer_leg',capsize=5)
			plt.errorbar(time_full_binned_crop,sxd_tot_rad_power_all/1e3,yerr=sxd_tot_rad_power_sigma_all/1e3,label='sxd',capsize=5)
			plt.errorbar(time_full_binned_crop,inner_leg_tot_rad_power_all/1e3,yerr=inner_leg_tot_rad_power_sigma_all/1e3,label='inner_leg',capsize=5)
			plt.errorbar(time_full_binned_crop,core_tot_rad_power_all/1e3,yerr=core_tot_rad_power_sigma_all/1e3,label='core',capsize=5)
			plt.errorbar(time_full_binned_crop,x_point_tot_rad_power_all/1e3,yerr=x_point_tot_rad_power_sigma_all/1e3,label='x_point (dist<%.3gm)' %(x_point_region_radious),capsize=5)
			plt.errorbar(time_full_binned_crop,(outer_leg_tot_rad_power_all+inner_leg_tot_rad_power_all+core_tot_rad_power_all)/1e3,yerr=((outer_leg_tot_rad_power_sigma_all**2+inner_leg_tot_rad_power_sigma_all**2+core_tot_rad_power_sigma_all**2)**0.5)/1e3,label='tot',capsize=5)
			plt.title('shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\nradiated power in the lower half of the machine')
			plt.ylim(bottom=0,top=median_filter((outer_leg_tot_rad_power_all+inner_leg_tot_rad_power_all+core_tot_rad_power_all),size=[max(1,len(core_tot_rad_power_all)//8)*2+1],mode='constant',cval=0).max()/1e3)	# arbitrary limit to see better if there is a disruption at the end of the shot
			plt.legend(loc='best', fontsize='x-small')
			plt.xlabel('time [s]')
			plt.ylabel('power [kW]')
			plt.grid()
			plt.savefig(filename_root+filename_root_add+'_FAST_tot_rad_power.eps')
			plt.close()

			inverted_dict[str(grid_resolution)]['outer_leg_tot_rad_power_all'] = outer_leg_tot_rad_power_all
			inverted_dict[str(grid_resolution)]['inner_leg_tot_rad_power_all'] = inner_leg_tot_rad_power_all
			inverted_dict[str(grid_resolution)]['core_tot_rad_power_all'] = core_tot_rad_power_all
			inverted_dict[str(grid_resolution)]['sxd_tot_rad_power_all'] = sxd_tot_rad_power_all
			inverted_dict[str(grid_resolution)]['x_point_tot_rad_power_all'] = x_point_tot_rad_power_all
			inverted_dict[str(grid_resolution)]['outer_leg_tot_rad_power_sigma_all'] = outer_leg_tot_rad_power_sigma_all
			inverted_dict[str(grid_resolution)]['inner_leg_tot_rad_power_sigma_all'] = inner_leg_tot_rad_power_sigma_all
			inverted_dict[str(grid_resolution)]['core_tot_rad_power_sigma_all'] = core_tot_rad_power_sigma_all
			inverted_dict[str(grid_resolution)]['sxd_tot_rad_power_sigma_all'] = sxd_tot_rad_power_sigma_all
			inverted_dict[str(grid_resolution)]['x_point_tot_rad_power_sigma_all'] = x_point_tot_rad_power_sigma_all
			inverted_dict[str(grid_resolution)]['x_point_region_radious'] = x_point_region_radious


		path_power_output = os.path.split(laser_to_analyse)[0] + '/' + str(shot_number)
		if not os.path.exists(path_power_output):
			os.makedirs(path_power_output)

		plt.figure(figsize=(20, 10))
		plt.plot(time_full_binned_crop,inverted_data_likelihood)
		# plt.semilogy()
		plt.title('shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\nFit log likelihood\npgtol=%.3g, factr=%.3g, maxiter=%.3g, homo scale=%.3g' %(pgtol,factr,maxiter,homogeneous_scaling))
		plt.xlabel('time [s]')
		plt.ylabel('log likelihoog [au]')
		plt.grid()
		plt.savefig(filename_root+filename_root_add+'_FAST_likelihood.eps')
		plt.close()

		plt.figure(figsize=(20, 10))
		plt.plot(time_full_binned_crop,chi_square_all)
		plt.plot(time_full_binned_crop,np.ones_like(time_full_binned_crop)*target_chi_square,'--k')
		# plt.semilogy()
		if False:
			plt.title('shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\nchi square obtained vs requested\nfixed regularisation of '+str(regolarisation_coeff))
		else:
			plt.title('shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\nchi square obtained vs requested\nflexible regolarisation coefficient')
		plt.xlabel('time [s]')
		plt.ylabel('chi square [au]')
		plt.grid()
		plt.savefig(filename_root+filename_root_add+'_FAST_chi_square.eps')
		plt.close()

		plt.figure(figsize=(20, 10))
		plt.plot(time_full_binned_crop,regolarisation_coeff_all)
		# plt.semilogy()
		plt.title('shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\nregolarisation coefficient obtained')
		plt.semilogy()
		plt.xlabel('time [s]')
		plt.ylabel('regolarisation coefficient [au]')
		plt.grid()
		plt.savefig(filename_root+filename_root_add+'_FAST_regolarisation_coeff.eps')
		plt.close()

		plt.figure(figsize=(20, 10))
		plt.plot(time_full_binned_crop,fit_error)
		# plt.semilogy()
		plt.title('shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\nFit error ( sum((image-fit)^2)^0.5/num pixels )')
		plt.xlabel('time [s]')
		plt.ylabel('average fit error [W/m2]')
		plt.grid()
		plt.savefig(filename_root+filename_root_add+'_FAST_fit_error.eps')
		plt.close()

		# plt.figure(figsize=(20, 10))
		# plt.plot(time_full_binned_crop,inverted_data_plasma_region_offset,label='plasma region')
		# plt.plot(time_full_binned_crop,inverted_data_homogeneous_offset,label='whole foil')
		# plt.plot(time_full_binned_crop,np.nanmax(powernoback_full_orig,axis=(1,2)),'--',label='whole foil max')
		# plt.plot(time_full_binned_crop,np.nanmean(powernoback_full_orig,axis=(1,2)),'--',label='whole foil mean')
		# plt.title('Offsets to match foil power')
		# plt.legend(loc='best', fontsize='x-small')
		# plt.xlabel('time [s]')
		# plt.ylabel('power density [W/m2]')
		# # plt.semilogy()
		# plt.grid()
		# plt.savefig(path_power_output + '/'+ str(shot_number)+'_'+binning_type+'_gridres'+str(grid_resolution)+'cm_FAST_offsets.eps')
		# plt.close()

		fig, ax = plt.subplots( 4,1,figsize=(20, 18), squeeze=False, sharex=True)
		fig.suptitle('shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\nOffsets to match foil power\npgtol=%.3g, factr=%.3g, maxiter=%.3g, homo scale=%.3g' %(pgtol,factr,maxiter,homogeneous_scaling))
		ax[0,0].plot(time_full_binned_crop,inverted_data_plasma_region_offset,label='plasma region')
		ax[0,0].plot(time_full_binned_crop,inverted_data_homogeneous_offset,label='whole foil')
		ax[0,0].plot(time_full_binned_crop,np.mean(fitted_foil_power_excluded,axis=(-1,-2)),label='averaged\noffs+excl+head')
		if use_pinhole_plate_over_temperature:
			IRVB_head_power_correction_pinhole_plate_all = []
			IRVB_head_power_correction_head_all = []
			IRVB_head_power_correction_ring_all = []
			for i_t in range(len(time_full_binned_crop)):
				DT_head_r = DT_head_r_all[i_t]
				DT_ring = DT_ring_all[i_t]
				IRVB_head_power_correction_pinhole_plate = np.sum(((pinhole_plate_to_foil_weight.T*((DT_head_r+ref_temperature+zeroC)**4 - (ref_temperature+zeroC)**4)).T)[R_IRVB_head_from_pinhole<rmax_pinhole_plate],axis=0)
				IRVB_head_power_correction_head = np.sum(((pinhole_plate_to_foil_weight.T*((DT_head_r+ref_temperature+zeroC)**4 - (ref_temperature+zeroC)**4)).T)[R_IRVB_head_from_pinhole>=rmax_pinhole_plate],axis=0)
				IRVB_head_power_correction_ring = IRVB_head_ring_to_foil_weight*((DT_ring+ref_temperature+zeroC)**4 - (ref_temperature+zeroC)**4)
				IRVB_head_power_correction_pinhole_plate_all.append(np.mean(IRVB_head_power_correction_pinhole_plate))
				IRVB_head_power_correction_head_all.append(np.mean(IRVB_head_power_correction_head))
				IRVB_head_power_correction_ring_all.append(np.mean(IRVB_head_power_correction_ring))
			a1 = ax[0,0].plot(time_full_binned_crop,IRVB_head_power_correction_pinhole_plate_all,label='averaged pinhole plate')
			a2 = ax[0,0].plot(time_full_binned_crop,IRVB_head_power_correction_head_all,label='averaged IRVB head')
			a3 = ax[0,0].plot(time_full_binned_crop,IRVB_head_power_correction_ring_all,label='averaged IRVB head ring')
			ax[1,0].plot(time_full_binned_crop,DT_max_all,label='DT_max',color = a1[0].get_color())
			ax[1,0].plot(time_full_binned_crop,DT_head_all,label='DT_head',color = a2[0].get_color())
			ax[1,0].plot(time_full_binned_crop,DT_ring_all,label='DT_ring',color = a3[0].get_color())
			ax[1,0].legend(loc='best', fontsize='x-small')
			ax[2,0].plot(time_full_binned_crop,pinhole_plate_T_slope_all,label='pinhole_plate_T_slope')
			ax[2,0].legend(loc='best', fontsize='x-small')
		ax[3,0].plot(time_full_binned_crop,np.nanmax(powernoback_full_orig,axis=(1,2)),'--',label='whole foil max')
		ax[3,0].plot(time_full_binned_crop,np.nanmean(powernoback_full_orig,axis=(1,2)),'--',label='whole foil mean')
		ax[3,0].plot(time_full_binned_crop,np.mean(fitted_foil_power_excluded,axis=(-1,-2)),label='averaged\noffs+excl+head')
		ax[3,0].set_ylim(bottom=1)
		ax[0,0].grid()
		ax[1,0].grid()
		ax[2,0].grid()
		ax[3,0].grid()
		ax[0,0].set_ylabel('power density [W/m2]')
		ax[1,0].set_ylabel('Temperature [C]')
		ax[2,0].set_ylabel('slope [C/m]')
		ax[3,0].set_ylabel('power density [W/m2]')
		ax[3,0].set_xlabel('time [s]')
		ax[0,0].legend(loc='best', fontsize='x-small')
		ax[3,0].legend(loc='best', fontsize='x-small')
		ax[3,0].semilogy()
		plt.savefig(filename_root+filename_root_add+'_FAST_offsets.eps')
		plt.close()

		# it's necessary here to limit the range for when the regularisation goes really low
		if pass_number >1:	# This effects only the plotting
			# from shapely.geometry import Point
			from shapely.geometry.polygon import Polygon
			select_good_voxels = np.zeros((len(grid_data_masked_crop))).astype(bool)
			polygon = Polygon(FULL_MASTU_CORE_GRID_POLYGON)
			select_good_voxels = select_cells_inside_polygon(polygon,[np.unique(np.mean(grid_data_masked_crop,axis=1)[:,0]),np.unique(np.mean(grid_data_masked_crop,axis=1)[:,1])])
			# select_good_voxels = select_good_voxels.reshape(np.shape(inverted_data[0]))
			sample = cp.deepcopy(inverted_data[:,select_good_voxels])
			extvmin = np.nanmin(sample,axis=1)*0
			extvmax = np.nanmax(sample,axis=1)
		else:
			extvmin = 'auto'
			extvmax = 'auto'

		if efit_reconstruction!=None:

			additional_points_dict,radiator_xpoint_distance_all,radiator_above_xpoint_all,radiator_magnetic_radious_all,radiator_baricentre_magnetic_radious_all,radiator_baricentre_above_xpoint_all = find_radiator_location(inverted_data,np.unique(voxels_centre[:,0]),np.unique(voxels_centre[:,1]),time_full_binned_crop,efit_reconstruction)

			inverted_dict[str(grid_resolution)]['radiator_location_all'] = additional_points_dict['0']
			inverted_dict[str(grid_resolution)]['radiator_baricentre_location_all'] = additional_points_dict['1']
			inverted_dict[str(grid_resolution)]['radiator_xpoint_distance_all'] = radiator_xpoint_distance_all
			inverted_dict[str(grid_resolution)]['radiator_above_xpoint_all'] = radiator_above_xpoint_all
			inverted_dict[str(grid_resolution)]['radiator_magnetic_radious_all'] = radiator_magnetic_radious_all

			fig, ax = plt.subplots( 2,1,figsize=(8, 12), squeeze=False,sharex=True)
			ax[0,0].plot(time_full_binned_crop,radiator_magnetic_radious_all)
			ax[0,0].plot(time_full_binned_crop,radiator_baricentre_magnetic_radious_all,'--')
			ax[0,0].set_ylim(top=min(np.nanmax(radiator_magnetic_radious_all),1.1),bottom=max(np.nanmin(radiator_magnetic_radious_all),0.9))
			ax[1,0].plot(time_full_binned_crop,radiator_above_xpoint_all)
			ax[1,0].plot(time_full_binned_crop,radiator_baricentre_above_xpoint_all,'--')
			fig.suptitle('shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\nLocation of the x-point radiator\n"--"=baricentre r=20cm around x-point')
			ax[0,0].set_ylabel('normalised psi [au]')
			ax[0,0].grid()
			ax[1,0].set_xlabel('time [s]')
			ax[1,0].set_ylabel('position above x-point [m]')
			ax[1,0].grid()
			plt.savefig(filename_root+filename_root_add+'_FAST_x_point_location.eps')
			plt.close()

			extent = [grid_data_masked_crop[:,:,0].min(), grid_data_masked_crop[:,:,0].max(), grid_data_masked_crop[:,:,1].min(), grid_data_masked_crop[:,:,1].max()]
			image_extent = [grid_data_masked_crop[:,:,0].min(), grid_data_masked_crop[:,:,0].max(), grid_data_masked_crop[:,:,1].min(), grid_data_masked_crop[:,:,1].max()]
			additional_each_frame_label_description = ['reg coeff=']*len(inverted_data)
			additional_each_frame_label_number = np.array(regolarisation_coeff_all)
			ani,trash = movie_from_data_radial_profile(np.array([np.flip(np.transpose(inverted_data,(0,2,1)),axis=2)])/1000, 1/(np.mean(np.diff(time_full_binned_crop))), extent = extent, image_extent=image_extent,timesteps=time_full_binned_crop,integration=laser_int_time/1000,barlabel='Emissivity [kW/m3]',xlabel='R [m]', ylabel='Z [m]', prelude='shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\n'+binning_type+'\n'+'sigma_emissivity %.3g\nregolarisation_coeff_edge %.3g\nregolarisation_coeff_central_border_Z_derivate_multiplier %.3g\nregolarisation_coeff_central_column_border_R_derivate_multiplier %.3g\nregolarisation_coeff_edge_laplacian_multiplier %.3g\nregolarisation_coeff_divertor_multiplier %.3g\nregolarisation_coeff_non_negativity_multiplier %.3g\ngrid resolution %.3g\n' %(sigma_emissivity,regolarisation_coeff_edge,regolarisation_coeff_central_border_Z_derivate_multiplier,regolarisation_coeff_central_column_border_R_derivate_multiplier,regolarisation_coeff_edge_laplacian_multiplier,regolarisation_coeff_divertor_multiplier,regolarisation_coeff_non_negativity_multiplier,grid_resolution) ,overlay_structure=True,include_EFIT=True,EFIT_output_requested=True,efit_reconstruction=efit_reconstruction,pulse_ID=laser_to_analyse[-9:-4],overlay_x_point=True,overlay_mag_axis=True,overlay_strike_points=True,overlay_separatrix=True,additional_points_dict=additional_points_dict,additional_each_frame_label_description=additional_each_frame_label_description,additional_each_frame_label_number=additional_each_frame_label_number,extvmin=extvmin,extvmax=extvmax)#,extvmin=0,extvmax=4e4)

		else:

			# ani = movie_from_data(np.array([np.flip(np.transpose(recompose_voxel_emissivity,(0,2,1)),axis=2)]), 1/(np.mean(np.diff(time_full_binned_crop))),integration=laser_int_time/1000,barlabel='Emissivity [W/m3]')#,extvmin=0,extvmax=4e4)
			extent = [grid_data_masked_crop[:,:,0].min(), grid_data_masked_crop[:,:,0].max(), grid_data_masked_crop[:,:,1].min(), grid_data_masked_crop[:,:,1].max()]
			image_extent = [grid_data_masked_crop[:,:,0].min(), grid_data_masked_crop[:,:,0].max(), grid_data_masked_crop[:,:,1].min(), grid_data_masked_crop[:,:,1].max()]
			additional_each_frame_label_description = ['reg coeff=']*len(inverted_data)
			additional_each_frame_label_number = np.array(regolarisation_coeff_all)
			ani,trash = movie_from_data_radial_profile(np.array([np.flip(np.transpose(inverted_data,(0,2,1)),axis=2)])/1000, 1/(np.mean(np.diff(time_full_binned_crop))), extent = extent, image_extent=image_extent,timesteps=time_full_binned_crop,integration=laser_int_time/1000,barlabel='Emissivity [kW/m3]',xlabel='R [m]', ylabel='Z [m]', prelude='shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\n'+binning_type+'\n'+'sigma_emissivity %.3g\nregolarisation_coeff_edge %.3g\nregolarisation_coeff_central_border_Z_derivate_multiplier %.3g\nregolarisation_coeff_central_column_border_R_derivate_multiplier %.3g\nregolarisation_coeff_edge_laplacian_multiplier %.3g\nregolarisation_coeff_divertor_multiplier %.3g\nregolarisation_coeff_non_negativity_multiplier %.3g\ngrid resolution %.3g\n' %(sigma_emissivity,regolarisation_coeff_edge,regolarisation_coeff_central_border_Z_derivate_multiplier,regolarisation_coeff_central_column_border_R_derivate_multiplier,regolarisation_coeff_edge_laplacian_multiplier,regolarisation_coeff_divertor_multiplier,regolarisation_coeff_non_negativity_multiplier,grid_resolution) ,overlay_structure=True,include_EFIT=True,EFIT_output_requested=True,efit_reconstruction=efit_reconstruction,pulse_ID=laser_to_analyse[-9:-4],overlay_x_point=True,overlay_mag_axis=True,overlay_strike_points=True,overlay_separatrix=True,additional_each_frame_label_description=additional_each_frame_label_description,additional_each_frame_label_number=additional_each_frame_label_number,extvmin=extvmin,extvmax=extvmax)#,extvmin=0,extvmax=4e4)
		ani.save(filename_root+filename_root_add+'_FAST_reconstruct_emissivity_bayesian'+extra_geom_correction+'.mp4', fps=5*(1/(np.mean(np.diff(time_full_binned_crop))))/383, writer='ffmpeg',codec='mpeg4')
		plt.close()


		inverted_dict[str(grid_resolution)]['binning_type'] = binning_type
		inverted_dict[str(grid_resolution)]['inverted_data'] = inverted_data
		inverted_dict[str(grid_resolution)]['inverted_data_sigma'] = inverted_data_sigma.astype(np.float32)
		inverted_dict[str(grid_resolution)]['inverted_data_excluded'] = inverted_data_excluded.astype(np.float32)
		inverted_dict[str(grid_resolution)]['inverted_data_excluded_sigma'] = inverted_data_excluded_sigma.astype(np.float16)
		if False:
			inverted_dict[str(grid_resolution)]['inverted_data_covariance'] = inverted_data_covariance.astype(np.float32)	# I cannot do this, as it requires too much memory
		elif False:	# here I rescale the covariences to fit in a float16
			inverted_data_covariance_scaling_factor = np.max(np.abs(inverted_data_covariance),axis=(2))/(np.round(np.finfo(np.float16).max*0.999))
			inverted_data_covariance_scaled = ((inverted_data_covariance.T / inverted_data_covariance_scaling_factor.T).T).astype(np.float16)
			inverted_dict[str(grid_resolution)]['inverted_data_covariance_scaling_factor'] = inverted_data_covariance_scaling_factor
			inverted_dict[str(grid_resolution)]['inverted_data_covariance_scaled'] = inverted_data_covariance_scaled
		else:	# I could not do that either, as I loose too much resolution. I'll try to separate the logarithm and an the signs
			inverted_data_covariance_is_positive_packed = np.packbits((inverted_data_covariance>0).flatten())
			inverted_data_covariance_log = np.log(np.abs(inverted_data_covariance)).astype(np.float16)
			if False:
				inverted_dict[str(grid_resolution)]['inverted_data_covariance_is_positive_packed'] = inverted_data_covariance_is_positive_packed
				inverted_dict[str(grid_resolution)]['inverted_data_covariance_log'] = inverted_data_covariance_log
			else:	# as suggested by Kevion I put the uncertainty in another file, so the file size is smaller and they open faster
				covariance_dict = dict([])
				covariance_dict['inverted_data_covariance_is_positive_packed'] = inverted_data_covariance_is_positive_packed
				covariance_dict['inverted_data_covariance_log'] = inverted_data_covariance_log
		inverted_dict[str(grid_resolution)]['inverted_data_likelihood'] = inverted_data_likelihood
		inverted_dict[str(grid_resolution)]['inverted_data_info'] = inverted_data_info
		inverted_dict[str(grid_resolution)]['select_foil_region_with_plasma'] = select_foil_region_with_plasma
		inverted_dict[str(grid_resolution)]['inverted_data_plasma_region_offset'] = inverted_data_plasma_region_offset
		inverted_dict[str(grid_resolution)]['inverted_data_homogeneous_offset'] = inverted_data_homogeneous_offset
		if use_pinhole_plate_over_temperature:
			inverted_dict[str(grid_resolution)]['DT_max_all'] = DT_max_all
			inverted_dict[str(grid_resolution)]['DT_head_all'] = DT_head_all
			inverted_dict[str(grid_resolution)]['DT_head_all'] = DT_head_all
			inverted_dict[str(grid_resolution)]['pinhole_plate_T_slope_all'] = pinhole_plate_T_slope_all
		inverted_dict[str(grid_resolution)]['DT_head_r_all'] = DT_head_r_all
		inverted_dict[str(grid_resolution)]['IRVB_head_power_correction_all'] = IRVB_head_power_correction_all
		inverted_dict[str(grid_resolution)]['x_optimal_ext'] = x_optimal_ext
		inverted_dict[str(grid_resolution)]['grid_data_masked_crop'] = grid_data_masked_crop
		inverted_dict[str(grid_resolution)]['time_full_binned_crop'] = time_full_binned_crop
		inverted_dict[str(grid_resolution)]['fitted_foil_power'] = fitted_foil_power.astype(np.float32)
		inverted_dict[str(grid_resolution)]['fitted_foil_power_excluded'] = fitted_foil_power_excluded.astype(np.float32)
		inverted_dict[str(grid_resolution)]['fitted_brightness'] = fitted_brightness.astype(np.float32)
		inverted_dict[str(grid_resolution)]['brightness_sigma'] = brightness_sigma.astype(np.float16)
		inverted_dict[str(grid_resolution)]['full_foil_power'] = powernoback_output[time_full_binned<tend].astype(np.float32)
		inverted_dict[str(grid_resolution)]['foil_power'] = foil_power.astype(np.float32)
		inverted_dict[str(grid_resolution)]['foil_power_std'] = sigma_powernoback_output.astype(np.float32)
		inverted_dict[str(grid_resolution)]['foil_power_std_filtered'] = foil_power_std.astype(np.float16)
		inverted_dict[str(grid_resolution)]['foil_power_residuals'] = foil_power_residuals.astype(np.float32)
		inverted_dict[str(grid_resolution)]['fit_error'] = fit_error.astype(np.float32)
		inverted_dict[str(grid_resolution)]['chi_square_all'] = chi_square_all
		inverted_dict[str(grid_resolution)]['geometry'] = dict([])
		inverted_dict[str(grid_resolution)]['geometry']['R'] = np.unique(voxels_centre[:,0])
		inverted_dict[str(grid_resolution)]['geometry']['Z'] = np.unique(voxels_centre[:,1])
		inverted_dict[str(grid_resolution)]['score_x_all'] = score_x_all
		inverted_dict[str(grid_resolution)]['score_y_all'] = score_y_all
		inverted_dict[str(grid_resolution)]['regolarisation_coeff_range_all'] = regolarisation_coeff_range_all
		inverted_dict[str(grid_resolution)]['Lcurve_curvature_all'] = Lcurve_curvature_all
		# inverted_dict[str(grid_resolution)]['sensitivities_binned_crop'] = sensitivities_binned_crop
		inverted_dict[str(grid_resolution)]['regolarisation_coeff_all'] = regolarisation_coeff_all
		inverted_dict[str(grid_resolution)]['original_counts_shape'] = original_counts_shape
		inverted_dict[str(grid_resolution)]['foil_properties_used'] = foil_properties_used
		inverted_dict[str(grid_resolution)]['params_BB_used'] = params_BB_int
		inverted_dict[str(grid_resolution)]['errparams_BB_used'] = errparams_BB_int
		inverted_dict[str(grid_resolution)]['temperature_crop_binned'] = np.mean(temperature_crop_binned,axis=0).astype(np.float16)
		inverted_dict[str(grid_resolution)]['filename_root'] = filename_root
		inverted_dict[str(grid_resolution)]['filename_root_add'] = filename_root_add

		if efit_reconstruction!=None:

			inversion_R = np.unique(voxels_centre[:,0])
			inversion_Z = np.unique(voxels_centre[:,1])
			local_mean_emis_all,local_power_all,leg_length_interval_all,leg_length_all,data_length,leg_resolution = track_outer_leg_radiation(inverted_data,inversion_R,inversion_Z,time_full_binned_crop,efit_reconstruction)

			try:
				if False:
					fig, ax = plt.subplots( 1,2,figsize=(15, 10), squeeze=False,sharey=True)
					temp = np.array(local_power_all)
					temp[np.isnan(temp)] = 0
					im1 = ax[0,0].imshow(temp,'rainbow',origin='lower',extent=[(0-0.5)*leg_resolution,(data_length+0.5)*leg_resolution,time_full_binned_crop[0]-np.diff(time_full_binned_crop)[0]/2,time_full_binned_crop[-1]+np.diff(time_full_binned_crop)[-1]/2],aspect=10,vmin=np.min(temp[:-4]),vmax=generic_filter(temp,np.mean,size=[max(5,temp.shape[0]//10*2),max(5,temp.shape[1]//10*2)]).max())#,vmax=np.max(temp[:-4]))
					ax[0,0].plot(leg_length_all,time_full_binned_crop,'--k')
					ax[0,0].set_aspect(3)
					temp = np.array(local_mean_emis_all)
					temp[np.isnan(temp)] = 0
					im2 = ax[0,1].imshow(temp,'rainbow',origin='lower',extent=[(0-0.5)*leg_resolution,(data_length+0.5)*leg_resolution,time_full_binned_crop[0]-np.diff(time_full_binned_crop)[0]/2,time_full_binned_crop[-1]+np.diff(time_full_binned_crop)[-1]/2],aspect=10,vmin=np.min(temp[:-4]),vmax=generic_filter(temp,np.mean,size=[max(5,temp.shape[0]//10*2),max(5,temp.shape[1]//10*2)]).max())#,vmax=np.max(temp[:-4]))
					ax[0,1].plot(leg_length_all,time_full_binned_crop,'--k')
					ax[0,1].set_aspect(3)
					fig.suptitle('shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\ntracking radiation on the outer leg\naveraged/summed %.3gcm above and below the separatrix'%(leg_resolution*100))
					ax[0,0].set_xlabel('distance from the strike point [m]')
					ax[0,0].grid()
					ax[0,0].set_ylabel('time [s]')
					plt.colorbar(im1,ax=ax[0,0]).set_label('Integrated power [W]')
					# ax[0,0].colorbar().set_label('Integrated power [W]')
					ax[0,1].set_xlabel('distance from the strike point [m]')
					ax[0,1].grid()
					# ax[0,1].colorbar().set_label('Emissivity [W/m3]')
					ax[0,0].set_xlim(right=median_filter(leg_length_all,size=[max(5,len(leg_length_all)//5*2)]).max())
					ax[0,1].set_xlim(right=median_filter(leg_length_all,size=[max(5,len(leg_length_all)//5*2)]).max())
					plt.colorbar(im2,ax=ax[0,1]).set_label('Averaged emissivity [W/m3]')
					plt.savefig(filename_root+filename_root_add+'_outer_leg_radiation_tracking.eps')
					plt.close()

					number_of_curves_to_plot = 12
					alpha = 0.9
					# colors_smooth = np.array([np.linspace(0,1,num=number_of_curves_to_plot),np.linspace(1,0,num=number_of_curves_to_plot),[0]*number_of_curves_to_plot]).T
					colors_smooth = np.linspace(0,0.9,num=int(np.ceil(number_of_curves_to_plot/3))).astype(str)
					select = np.unique(np.round(np.linspace(0,len(local_mean_emis_all)-1,num=number_of_curves_to_plot))).astype(int)
					fig, ax = plt.subplots( 2,1,figsize=(10, 20), squeeze=False,sharex=True)
					fig.suptitle('shot ' + laser_to_analyse[-9:-4]+' '+scenario+'\ntracking radiation on the outer leg')
					# plt.figure()
					for i_i_t,i_t in enumerate(select):
						# ax[0,0].plot(np.cumsum(leg_length_interval_all[i_t]),local_mean_emis_all[i_t],color=colors_smooth[i_i_t],label = '%.3gs' %(time_full_binned_crop[i_t]),linewidth=3)
						# ax[1,0].plot(np.cumsum(leg_length_interval_all[i_t]),local_power_all[i_t],color=colors_smooth[i_i_t],linewidth=3)
						to_plot_x = np.array(leg_length_interval_all[i_t])
						to_plot_y1 = np.array(local_mean_emis_all[i_t])
						to_plot_y1 = to_plot_y1[to_plot_x>0]
						to_plot_y2 = np.array(local_power_all[i_t])
						to_plot_y2 = to_plot_y2[to_plot_x>0]
						to_plot_x = to_plot_x[to_plot_x>0]
						to_plot_x = np.flip(np.sum(to_plot_x)-(np.cumsum(to_plot_x)-np.array(to_plot_x)/2),axis=0)
						to_plot_y1 = np.flip(to_plot_y1,axis=0)
						to_plot_y2 = np.flip(to_plot_y2,axis=0)
						if i_i_t%3==0:
							ax[0,0].plot(to_plot_x,to_plot_y1,'-',color=colors_smooth[i_i_t//3],label = '%.3gs' %(time_full_binned_crop[i_t]),linewidth=2,alpha=alpha)
							ax[1,0].plot(to_plot_x,to_plot_y2,'-',color=colors_smooth[i_i_t//3],linewidth=2,alpha=alpha)
						elif i_i_t%3==1:
							ax[0,0].plot(to_plot_x,to_plot_y1,'-.',color=colors_smooth[i_i_t//3],label = '%.3gs' %(time_full_binned_crop[i_t]),linewidth=2,alpha=alpha)
							ax[1,0].plot(to_plot_x,to_plot_y2,'-.',color=colors_smooth[i_i_t//3],linewidth=2,alpha=alpha)
						elif i_i_t%3==2:
							ax[0,0].plot(to_plot_x,to_plot_y1,':',color=colors_smooth[i_i_t//3],label = '%.3gs' %(time_full_binned_crop[i_t]),linewidth=2,alpha=alpha)
							ax[1,0].plot(to_plot_x,to_plot_y2,':',color=colors_smooth[i_i_t//3],linewidth=2,alpha=alpha)
					ax[0,0].legend(loc='best', fontsize='x-small')
					ax[0,0].set_ylabel('average emissivity [W/m3]')
					ax[1,0].set_ylabel('local radiated power [W]')
					# ax[1,0].set_xlabel('distance from target [m]')
					ax[1,0].set_xlabel('distance from x-point [m]')
					ax[0,0].grid()
					ax[1,0].grid()
					plt.savefig(filename_root+filename_root_add+'_outer_leg_radiation_tracking_2.png')
					plt.close()
				else:	# this is exactly the same, but more compact and i can use it as an independent piece of code more easily
					peak_location,midpoint_location = plot_leg_radiation_tracking(inverted_data,inversion_R,inversion_Z,time_full_binned_crop,local_mean_emis_all,local_power_all,leg_length_interval_all,leg_length_all,data_length,leg_resolution,filename_root,filename_root_add,laser_to_analyse,scenario,which_leg='outer')

			except Exception as e:
				logging.exception('with error: ' + str(e))
				print('failed to print\n'+'/home/ffederic/work/irvb/MAST-U/FAST_results/'+os.path.split(laser_to_analyse[:-4])[1]+'_'+binning_type+'_gridres'+str(grid_resolution)+'cm_outer_leg_radiation_tracking.eps')

			inverted_dict[str(grid_resolution)]['local_outer_leg_power'] = local_power_all
			inverted_dict[str(grid_resolution)]['local_outer_leg_mean_emissivity'] = local_mean_emis_all
			inverted_dict[str(grid_resolution)]['leg_outer_length_all'] = leg_length_all
			inverted_dict[str(grid_resolution)]['leg_outer_length_interval_all'] = leg_length_interval_all
			inverted_dict[str(grid_resolution)]['leg_outer_peak_location'] = peak_location
			inverted_dict[str(grid_resolution)]['leg_outer_midpoint_location'] = midpoint_location

			local_mean_emis_all,local_power_all,leg_length_interval_all,leg_length_all,data_length,leg_resolution = track_inner_leg_radiation(inverted_data,inversion_R,inversion_Z,time_full_binned_crop,efit_reconstruction)

			try:
				peak_location,midpoint_location = plot_leg_radiation_tracking(inverted_data,inversion_R,inversion_Z,time_full_binned_crop,local_mean_emis_all,local_power_all,leg_length_interval_all,leg_length_all,data_length,leg_resolution,filename_root,filename_root_add,laser_to_analyse,scenario,which_leg='inner')

			except Exception as e:
				logging.exception('with error: ' + str(e))
				print('failed to print\n'+'/home/ffederic/work/irvb/MAST-U/FAST_results/'+os.path.split(laser_to_analyse[:-4])[1]+'_'+binning_type+'_gridres'+str(grid_resolution)+'cm_inner_leg_radiation_tracking.eps')

			inverted_dict[str(grid_resolution)]['local_inner_leg_power'] = local_power_all
			inverted_dict[str(grid_resolution)]['local_inner_leg_mean_emissivity'] = local_mean_emis_all
			inverted_dict[str(grid_resolution)]['leg_inner_length_all'] = leg_length_all
			inverted_dict[str(grid_resolution)]['leg_inner_length_interval_all'] = leg_length_interval_all
			inverted_dict[str(grid_resolution)]['leg_inner_peak_location'] = peak_location
			inverted_dict[str(grid_resolution)]['leg_inner_midpoint_location'] = midpoint_location

	print('ended MASTU_pulse_process_FAST3_BB at %.3gmin from calling MASTU_pulse_process_FAST3_BB' %((tm.time()-start_processing_fast)/60))

	# # temporary
	# plt.figure()
	# plt.imshow(inverted_data[15])
	# plt.title('shot ' + laser_to_analyse[-9:-4]+' '+'\ntemp')
	# plt.savefig(filename_root+filename_root_add+'_temp1.eps')
	# plt.close()

	if disruption_check:
		return foilrotdeg,out_of_ROI_mask,foildw,foilup,foillx,foilrx,FAST_counts_minus_background_crop_binned,time_full_binned,powernoback_output,brightness,binning_type,inverted_dict,covariance_dict,temperature_minus_background_unfiltered_crop_dt,temperature_minus_background_unfiltered_crop_dt_time
	else:
		return foilrotdeg,out_of_ROI_mask,foildw,foilup,foillx,foilrx,FAST_counts_minus_background_crop_binned,time_full_binned,powernoback_output,brightness,binning_type,inverted_dict,covariance_dict

def calc_IRVB_head_power_correction(DT_max,DT_head,DT_ring,pinhole_plate_T_slope,rmax_pinhole_plate,pinhole_radious,R_IRVB_head_from_pinhole,pinhole_plate_to_foil_weight,ref_temperature,IRVB_head_ring_to_foil_weight):
	zeroC=273.15 #K / C
	DT_head_r = DT_max + DT_head + ( -DT_max - pinhole_plate_T_slope*(rmax_pinhole_plate-pinhole_radious) ) * ((R_IRVB_head_from_pinhole-pinhole_radious)/(rmax_pinhole_plate-pinhole_radious))**2 + pinhole_plate_T_slope*(R_IRVB_head_from_pinhole-pinhole_radious)
	DT_head_r[R_IRVB_head_from_pinhole>=rmax_pinhole_plate] = DT_head
	DT_head_r[DT_head_r<DT_head] = DT_head
	DT_head_r[DT_head_r>DT_max+DT_head] = DT_max+DT_head
	IRVB_head_power_correction = np.sum((pinhole_plate_to_foil_weight.T*((DT_head_r+ref_temperature+zeroC)**4 - (ref_temperature+zeroC)**4)).T,axis=0) + IRVB_head_ring_to_foil_weight*((DT_ring+ref_temperature+zeroC)**4 - (ref_temperature+zeroC)**4)
	return DT_head_r,IRVB_head_power_correction

def define_fitting_functions(homogeneous_scaling,regolarisation_coeff_divertor_multiplier,regolarisation_coeff_central_column_border_R_derivate_multiplier,regolarisation_coeff_central_border_Z_derivate_multiplier,regolarisation_coeff_edge_laplacian_multiplier,sensitivities_binned_crop,selected_ROI_internal,select_foil_region_with_plasma,grid_laplacian_masked_crop_scaled,not_selected_super_x_cells,selected_edge_cells_for_laplacian,selected_super_x_cells,selected_central_column_border_cells,selected_central_border_cells,regolarisation_coeff_non_negativity_multiplier,selected_edge_cells,r_int,regolarisation_coeff_edge,regolarisation_coeff_offsets_multiplier,number_cells_ROI,reference_sigma_powernoback,number_cells_plasma,r_int_2,grid_R_derivate_masked_crop_scaled,grid_Z_derivate_masked_crop_scaled, rmax_pinhole_plate=0,R_IRVB_head_from_pinhole=[],pinhole_plate_to_foil_weight=[],IRVB_head_ring_to_foil_weight=[],ref_temperature=0,use_pinhole_plate_over_temperature=True):

	# added 2024/10/02 to allow logging
	global likelihood_logging,derivate_logging

	zeroC=273.15 #K / C
	edge_cells_for_laplacian_are_present = (np.sum(selected_edge_cells_for_laplacian)>0) or (regolarisation_coeff_edge_laplacian_multiplier==0)
	edge_cells_are_present = (np.sum(selected_edge_cells)>0) or (regolarisation_coeff_edge==0)
	central_column_border_R_derivate_correction_present = regolarisation_coeff_central_column_border_R_derivate_multiplier!=0
	central_border_Z_derivate_correction_present = regolarisation_coeff_central_border_Z_derivate_multiplier!=0

	def prob_and_gradient(emissivity_plus,*args):
		# added 2024/10/02 to allow logging
		global likelihood_logging,derivate_logging

		# time_start = tm.time()
		# emissivity_plus = emissivity_plus
		powernoback = args[0]
		sigma_powernoback = args[1]
		sigma_emissivity = args[2]
		regolarisation_coeff = args[3]
		sigma_powernoback_2 = args[4]
		sigma_emissivity_2 = args[5]
		IRVB_head_power_correction = 0
		if use_pinhole_plate_over_temperature:
			DT_max = emissivity_plus[-4]*homogeneous_scaling	# scaling added such that all variables have the same order of magnitude
			DT_head = emissivity_plus[-3]*homogeneous_scaling	# scaling added such that all variables have the same order of magnitude
			DT_ring = emissivity_plus[-2]*homogeneous_scaling	# scaling added such that all variables have the same order of magnitude
			pinhole_plate_T_slope = emissivity_plus[-1]*homogeneous_scaling	# scaling added such that all variables have the same order of magnitude
			# DT_head_r = DT_max + DT_head + ( -DT_max - pinhole_plate_T_slope*(rmax_pinhole_plate-pinhole_radious) ) * ((R_IRVB_head_from_pinhole-pinhole_radious)/(rmax_pinhole_plate-pinhole_radious))**2 + pinhole_plate_T_slope*(R_IRVB_head_from_pinhole-pinhole_radious)
			# DT_head_r[R_IRVB_head_from_pinhole>=rmax_pinhole_plate] = DT_head
			# DT_head_r[DT_head_r<DT_head] = DT_head
			# DT_head_r[DT_head_r>DT_max+DT_head] = DT_max+DT_head
			# IRVB_head_power_correction = np.sum((pinhole_plate_to_foil_weight.T*((DT_head_r+ref_temperature+zeroC)**4 - (ref_temperature+zeroC)**4)).T,axis=0) + IRVB_head_ring_to_foil_weight*((DT_ring+ref_temperature+zeroC)**4 - (ref_temperature+zeroC)**4)
			DT_head_r,IRVB_head_power_correction = calc_IRVB_head_power_correction(DT_max,DT_head,DT_ring,pinhole_plate_T_slope,rmax_pinhole_plate,pinhole_radious,R_IRVB_head_from_pinhole,pinhole_plate_to_foil_weight,ref_temperature,IRVB_head_ring_to_foil_weight)
			IRVB_head_power_correction = IRVB_head_power_correction.flatten()
			emissivity_plus = emissivity_plus[:-4]

		homogeneous_offset = emissivity_plus[-1]*homogeneous_scaling	# scaling added such that all variables have the same order of magnitude
		homogeneous_offset_plasma = emissivity_plus[-2]*homogeneous_scaling	# scaling added such that all variables have the same order of magnitude
		regolarisation_coeff_divertor = regolarisation_coeff*regolarisation_coeff_divertor_multiplier
		regolarisation_coeff_central_column_border_R_derivate = regolarisation_coeff*regolarisation_coeff_central_column_border_R_derivate_multiplier
		regolarisation_coeff_central_border_Z_derivate = regolarisation_coeff*regolarisation_coeff_central_border_Z_derivate_multiplier
		regolarisation_coeff_edge_laplacian = regolarisation_coeff*regolarisation_coeff_edge_laplacian_multiplier
		regolarisation_coeff_offsets = regolarisation_coeff*regolarisation_coeff_offsets_multiplier
		# regolarisation_coeff_edge = regolarisation_coeff*regolarisation_coeff_edge_multiplier
		# print(homogeneous_offset,homogeneous_offset_plasma)
		emissivity = emissivity_plus[:-2]
		# emissivity[emissivity==0] = 1e-10
		# foil_power_guess = np.dot(sensitivities_binned_crop,emissivity) + selected_ROI_internal*homogeneous_offset + homogeneous_offset_plasma*select_foil_region_with_plasma
		foil_power_error = powernoback - (np.dot(sensitivities_binned_crop,emissivity) + selected_ROI_internal*homogeneous_offset + homogeneous_offset_plasma*select_foil_region_with_plasma + IRVB_head_power_correction)
		# emissivity_laplacian = np.dot(grid_laplacian_masked_crop_scaled,emissivity)
		emissivity_laplacian_ovver_sigma = np.dot(grid_laplacian_masked_crop_scaled,emissivity)/sigma_emissivity
		emissivity_laplacian_ovver_sigma_not_selected_super_x_cells = emissivity_laplacian_ovver_sigma*np.logical_and(not_selected_super_x_cells,np.logical_not(selected_edge_cells_for_laplacian))
		emissivity_laplacian_ovver_sigma_selected_super_x_cells = emissivity_laplacian_ovver_sigma*np.logical_and(selected_super_x_cells,np.logical_not(selected_edge_cells_for_laplacian))
		if edge_cells_for_laplacian_are_present:
			emissivity_laplacian_ovver_sigma_selected_edge_cells_for_laplacian = emissivity_laplacian_ovver_sigma*selected_edge_cells_for_laplacian
		if central_column_border_R_derivate_correction_present:
			R_derivate = np.dot(grid_R_derivate_masked_crop_scaled,emissivity)
			R_derivate_selected_central_column_border_cells = R_derivate*selected_central_column_border_cells
		if central_border_Z_derivate_correction_present:
			Z_derivate = np.dot(grid_Z_derivate_masked_crop_scaled,emissivity)
			Z_derivate_selected_central_border_cells = Z_derivate*selected_central_border_cells
		# print(tm.time()-time_start)
		# time_start = tm.time()

		likelihood_power_fit = 0.5*np.sum((foil_power_error/sigma_powernoback)**2)
		likelihood_emissivity_pos = (regolarisation_coeff_non_negativity_multiplier**2)*np.sum((np.minimum(0.,emissivity*np.logical_not(selected_edge_cells))*r_int/sigma_emissivity*1)**2)	# I added a weight on the redious, becaus the power increase with radious and a negative voxel at high r is more important that one at low r
		likelihood_emissivity_laplacian = (regolarisation_coeff**2)* np.sum(((emissivity_laplacian_ovver_sigma_not_selected_super_x_cells)**2))
		likelihood_emissivity_laplacian_superx = (regolarisation_coeff_divertor**2)* np.sum(((emissivity_laplacian_ovver_sigma_selected_super_x_cells)**2))
		if edge_cells_for_laplacian_are_present:
			likelihood_emissivity_edge_laplacian = (regolarisation_coeff_edge_laplacian**2)* np.sum(((emissivity_laplacian_ovver_sigma_selected_edge_cells_for_laplacian)**2))
		else:
			likelihood_emissivity_edge_laplacian = 0
		if edge_cells_are_present:
			likelihood_emissivity_edge = (regolarisation_coeff_edge**2)*np.sum((emissivity*selected_edge_cells/sigma_emissivity)**2)
		else:
			likelihood_emissivity_edge = 0
		if central_column_border_R_derivate_correction_present:
			likelihood_emissivity_central_column_border_R_derivate = (regolarisation_coeff_central_column_border_R_derivate**2)* np.sum((R_derivate_selected_central_column_border_cells/sigma_emissivity)**2)
		else:
			likelihood_emissivity_central_column_border_R_derivate = 0
		if central_border_Z_derivate_correction_present:
			likelihood_emissivity_central_border_Z_derivate = (regolarisation_coeff_central_border_Z_derivate**2)* np.sum((Z_derivate_selected_central_border_cells/sigma_emissivity)**2)
		else:
			likelihood_emissivity_central_border_Z_derivate = 0
		likelihood = likelihood_power_fit + likelihood_emissivity_pos + likelihood_emissivity_laplacian + likelihood_emissivity_edge + likelihood_emissivity_laplacian_superx + likelihood_emissivity_central_column_border_R_derivate + likelihood_emissivity_central_border_Z_derivate + likelihood_emissivity_edge_laplacian
		likelihood_homogeneous_offset = regolarisation_coeff_offsets*number_cells_ROI*(homogeneous_offset/reference_sigma_powernoback)**2
		likelihood_homogeneous_offset_plasma = regolarisation_coeff_offsets*number_cells_plasma*(homogeneous_offset_plasma/reference_sigma_powernoback)**2
		likelihood += likelihood_homogeneous_offset + likelihood_homogeneous_offset_plasma
		# print(tm.time()-time_start)
		# time_start = tm.time()

		temp = foil_power_error/sigma_powernoback_2
		likelihood_power_fit_derivate = np.concatenate((-2*0.5*np.dot(temp,sensitivities_binned_crop),[-2*0.5*np.sum(temp*select_foil_region_with_plasma)*homogeneous_scaling,-2*0.5*np.sum(temp*selected_ROI_internal)*homogeneous_scaling]))
		if use_pinhole_plate_over_temperature:
			likelihood_power_fit_derivate = np.concatenate((likelihood_power_fit_derivate,[-2*0.5*np.sum(temp*np.sum(((pinhole_plate_to_foil_weight.T*4*((DT_head_r+ref_temperature+zeroC)**3)*(1-((R_IRVB_head_from_pinhole-pinhole_radious)/(rmax_pinhole_plate-pinhole_radious))**2)).T)[R_IRVB_head_from_pinhole<rmax_pinhole_plate],axis=0).flatten())*homogeneous_scaling]))
			likelihood_power_fit_derivate = np.concatenate((likelihood_power_fit_derivate,[-2*0.5*np.sum(temp*np.sum(((pinhole_plate_to_foil_weight.T*4*((DT_head_r+ref_temperature+zeroC)**3)*(1)).T),axis=0).flatten())*homogeneous_scaling]))
			likelihood_power_fit_derivate = np.concatenate((likelihood_power_fit_derivate,[-2*0.5*np.sum(temp*(IRVB_head_ring_to_foil_weight*4*((DT_ring+ref_temperature+zeroC)**3)*(1)).flatten())*homogeneous_scaling]))
			likelihood_power_fit_derivate = np.concatenate((likelihood_power_fit_derivate,[-2*0.5*np.sum(temp*np.sum(((pinhole_plate_to_foil_weight.T*4*((DT_head_r+ref_temperature+zeroC)**3)*(0-(rmax_pinhole_plate-pinhole_radious) * ((R_IRVB_head_from_pinhole-pinhole_radious)/(rmax_pinhole_plate-pinhole_radious))**2 + (R_IRVB_head_from_pinhole-pinhole_radious))).T)[R_IRVB_head_from_pinhole<rmax_pinhole_plate],axis=0).flatten())*homogeneous_scaling]))

		likelihood_emissivity_pos_derivate = 2*(regolarisation_coeff_non_negativity_multiplier**2)*np.minimum(0.,emissivity*np.logical_not(selected_edge_cells))*r_int_2/sigma_emissivity_2*1

		# likelihood_emissivity_laplacian_derivate = 2*(regolarisation_coeff**2) * np.dot(emissivity_laplacian_not_selected_super_x_cells , grid_laplacian_masked_crop_scaled) / (sigma_emissivity**2)
		# likelihood_emissivity_laplacian_derivate_superx = 2*(regolarisation_coeff_divertor**2) * np.dot(emissivity_laplacian_selected_super_x_cells , grid_laplacian_masked_crop_scaled) / (sigma_emissivity**2)
		# likelihood_emissivity_edge_laplacian_derivate = 2*(regolarisation_coeff_edge_laplacian**2) * np.dot(emissivity_laplacian_selected_edge_cells_for_laplacian , grid_laplacian_masked_crop_scaled) / (sigma_emissivity**2)
		if edge_cells_for_laplacian_are_present:
			likelihood_emissivity_laplacian_derivate_all = 2* np.dot( (regolarisation_coeff**2)*emissivity_laplacian_ovver_sigma_not_selected_super_x_cells + (regolarisation_coeff_edge_laplacian**2)*emissivity_laplacian_ovver_sigma_selected_edge_cells_for_laplacian + (regolarisation_coeff_divertor**2)*emissivity_laplacian_ovver_sigma_selected_super_x_cells , grid_laplacian_masked_crop_scaled) / (sigma_emissivity)
		else:
			likelihood_emissivity_laplacian_derivate_all = 2* np.dot( (regolarisation_coeff**2)*emissivity_laplacian_ovver_sigma_not_selected_super_x_cells + (regolarisation_coeff_divertor**2)*emissivity_laplacian_ovver_sigma_selected_super_x_cells , grid_laplacian_masked_crop_scaled) / (sigma_emissivity)

		# likelihood_emissivity_laplacian_derivate_all = 2* ( (regolarisation_coeff**2)*emissivity_laplacian_not_selected_super_x_cells + (regolarisation_coeff_edge_laplacian**2)*emissivity_laplacian_selected_edge_cells_for_laplacian + (regolarisation_coeff_divertor**2)*emissivity_laplacian_selected_super_x_cells * np.diag(grid_laplacian_masked_crop_scaled)) / (sigma_emissivity**2)
		if edge_cells_are_present:
			likelihood_emissivity_edge_derivate = 2*(regolarisation_coeff_edge**2)*emissivity*selected_edge_cells/sigma_emissivity_2
		else:
			likelihood_emissivity_edge_derivate = 0
		if central_column_border_R_derivate_correction_present:
			likelihood_emissivity_central_column_border_R_derivate_derivate = 2*(regolarisation_coeff_central_column_border_R_derivate**2)*np.dot(R_derivate_selected_central_column_border_cells,grid_R_derivate_masked_crop_scaled)/sigma_emissivity_2
		else:
			likelihood_emissivity_central_column_border_R_derivate_derivate = 0
		if central_border_Z_derivate_correction_present:
			likelihood_emissivity_central_border_Z_derivate_derivate = 2*(regolarisation_coeff_central_border_Z_derivate**2)*np.dot(Z_derivate_selected_central_border_cells,grid_Z_derivate_masked_crop_scaled)/sigma_emissivity_2
		else:
			likelihood_emissivity_central_border_Z_derivate_derivate = 0
		likelihood_derivate = likelihood_emissivity_pos_derivate + likelihood_emissivity_laplacian_derivate_all + likelihood_emissivity_edge_derivate + likelihood_emissivity_central_column_border_R_derivate_derivate + likelihood_emissivity_central_border_Z_derivate_derivate
		likelihood_homogeneous_offset_derivate = 2*regolarisation_coeff_offsets_multiplier*number_cells_ROI*homogeneous_offset*homogeneous_scaling/(reference_sigma_powernoback**2)
		likelihood_homogeneous_offset_plasma_derivate = 2*regolarisation_coeff_offsets_multiplier*number_cells_plasma*homogeneous_offset_plasma*homogeneous_scaling/(reference_sigma_powernoback**2)
		if use_pinhole_plate_over_temperature:
			likelihood_derivate = np.concatenate((likelihood_derivate,[likelihood_homogeneous_offset_plasma_derivate,likelihood_homogeneous_offset_derivate,0,0,0,0])) + likelihood_power_fit_derivate
		else:
			likelihood_derivate = np.concatenate((likelihood_derivate,[likelihood_homogeneous_offset_plasma_derivate,likelihood_homogeneous_offset_derivate])) + likelihood_power_fit_derivate
		# print(tm.time()-time_start)
		# time_start = tm.time()

		# 2024/10/02 section added to attempt a free logging of the convergence
		try:
			likelihood_logging.append(likelihood)
		except Exception as e:
			logging.exception('with error: ' + str(e))
			pass
		try:
			derivate_logging.append(np.linalg.norm(likelihood_derivate))
		except Exception as e:
			logging.exception('with error: ' + str(e))
			pass

		return likelihood,likelihood_derivate

	def calc_hessian_old(emissivity_plus,*args):	# this is wrong because in reality the double partial derivative is different from the product of the single derivative, as I wrongly assumed here
		print('this has to change with: gaussian weight of the foil fit divided by 2 AND the matrix built with equation 2.21 in the thesis (proper double derivative)')
		# time_start = tm.time()
		# emissivity_plus = emissivity_plus
		powernoback = args[0]
		sigma_powernoback = args[1]
		sigma_emissivity = args[2]
		regolarisation_coeff = args[3]
		sigma_powernoback_2 = args[4]
		sigma_emissivity_2 = args[5]
		IRVB_head_power_correction = 0
		if use_pinhole_plate_over_temperature:
			DT_max = emissivity_plus[-4]*homogeneous_scaling
			DT_head = emissivity_plus[-3]*homogeneous_scaling
			DT_ring = emissivity_plus[-2]*homogeneous_scaling
			pinhole_plate_T_slope = emissivity_plus[-1]
			DT_head_r,IRVB_head_power_correction = calc_IRVB_head_power_correction(DT_max,DT_head,DT_ring,pinhole_plate_T_slope,rmax_pinhole_plate,pinhole_radious,R_IRVB_head_from_pinhole,pinhole_plate_to_foil_weight,ref_temperature,IRVB_head_ring_to_foil_weight)
			IRVB_head_power_correction = IRVB_head_power_correction.flatten()
			emissivity_plus = emissivity_plus[:-4]

		homogeneous_offset = emissivity_plus[-1]*homogeneous_scaling	# scaling added such that all variables have the same order of magnitude
		homogeneous_offset_plasma = emissivity_plus[-2]*homogeneous_scaling	# scaling added such that all variables have the same order of magnitude
		# print(homogeneous_offset,homogeneous_offset_plasma)
		emissivity = emissivity_plus[:-2]
		# emissivity[emissivity==0] = 1e-10
		# foil_power_guess = np.dot(sensitivities_binned_crop,emissivity) + selected_ROI_internal*homogeneous_offset + homogeneous_offset_plasma*select_foil_region_with_plasma
		# foil_power_error = powernoback - (np.dot(sensitivities_binned_crop,emissivity) + selected_ROI_internal*homogeneous_offset + homogeneous_offset_plasma*select_foil_region_with_plasma)
		foil_power_error = powernoback - (np.dot(sensitivities_binned_crop,emissivity) + selected_ROI_internal*homogeneous_offset + homogeneous_offset_plasma*select_foil_region_with_plasma + IRVB_head_power_correction)
		emissivity_laplacian = np.dot(grid_laplacian_masked_crop_scaled,emissivity)
		emissivity_laplacian_not_selected_super_x_cells = emissivity_laplacian*not_selected_super_x_cells
		emissivity_laplacian_selected_super_x_cells = emissivity_laplacian*selected_super_x_cells
		emissivity_laplacian_selected_edge_cells_for_laplacian = emissivity_laplacian*selected_edge_cells_for_laplacian
		if regolarisation_coeff_central_column_border_R_derivate!=0:
			R_derivate = np.dot(grid_R_derivate_masked_crop_scaled,emissivity)
			R_derivate_selected_central_column_border_cells = R_derivate*selected_central_column_border_cells
		if regolarisation_coeff_central_border_Z_derivate!=0:
			Z_derivate = np.dot(grid_Z_derivate_masked_crop_scaled,emissivity)
			Z_derivate_selected_central_border_cells = Z_derivate*selected_central_border_cells
		# print(tm.time()-time_start)
		# time_start = tm.time()

		# based on https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/DAVIES1/rd_bhatt_cvonline/node9.html#SECTION00041000000000000000
		likelihood_power_fit_derivate = 0.5*np.dot(sensitivities_binned_crop.T*sigma_powernoback_2,sensitivities_binned_crop)
		if use_pinhole_plate_over_temperature:
			temp = np.zeros((np.shape(sensitivities_binned_crop)[1]+2+4,np.shape(sensitivities_binned_crop)[1]+2+4))
			temp[-4,-4] = 1
			temp[-3,-3] = 1
			temp[-2,-2] = 1
			temp[-1,-1] = 1
		else:
			temp = np.zeros((np.shape(sensitivities_binned_crop)[1]+2,np.shape(sensitivities_binned_crop)[1]+2))
		temp[:sensitivities_binned_crop.shape[1],:sensitivities_binned_crop.shape[1]] = likelihood_power_fit_derivate
		temp[sensitivities_binned_crop.shape[1]:-4,:sensitivities_binned_crop.shape[1]] = np.array([np.sum(-0.5*(sensitivities_binned_crop.T/sigma_powernoback_2*select_foil_region_with_plasma).T,axis=0)*homogeneous_scaling,np.sum(-(sensitivities_binned_crop.T/sigma_powernoback_2*selected_ROI_internal).T,axis=0)*homogeneous_scaling])
		temp[:sensitivities_binned_crop.shape[1],sensitivities_binned_crop.shape[1]:-4] = np.array([np.sum(-0.5*(sensitivities_binned_crop.T/sigma_powernoback_2*select_foil_region_with_plasma).T,axis=0)*homogeneous_scaling,np.sum(-(sensitivities_binned_crop.T/sigma_powernoback_2*selected_ROI_internal).T,axis=0)*homogeneous_scaling]).T
		temp[sensitivities_binned_crop.shape[1],sensitivities_binned_crop.shape[1]] = -np.sum(select_foil_region_with_plasma/sigma_powernoback_2*select_foil_region_with_plasma)*homogeneous_scaling
		temp[sensitivities_binned_crop.shape[1]+1,sensitivities_binned_crop.shape[1]+1] = -np.sum(selected_ROI_internal/sigma_powernoback_2*selected_ROI_internal)*homogeneous_scaling
		temp[sensitivities_binned_crop.shape[1]+1,sensitivities_binned_crop.shape[1]] = -np.sum(selected_ROI_internal/sigma_powernoback_2*select_foil_region_with_plasma)*homogeneous_scaling
		temp[sensitivities_binned_crop.shape[1],sensitivities_binned_crop.shape[1]+1] = -np.sum(selected_ROI_internal/sigma_powernoback_2*select_foil_region_with_plasma)*homogeneous_scaling
		likelihood_power_fit_derivate = cp.deepcopy(temp)

		likelihood_emissivity_pos_derivate = (regolarisation_coeff_non_negativity_multiplier**2)*np.diag((emissivity<0)*np.logical_not(selected_edge_cells)*r_int_2/sigma_emissivity_2*1)

		# likelihood_emissivity_laplacian_derivate = 2*(regolarisation_coeff**2) * np.dot(emissivity_laplacian_not_selected_super_x_cells , grid_laplacian_masked_crop_scaled) / (sigma_emissivity**2)
		# likelihood_emissivity_laplacian_derivate_superx = 2*(regolarisation_coeff_divertor**2) * np.dot(emissivity_laplacian_selected_super_x_cells , grid_laplacian_masked_crop_scaled) / (sigma_emissivity**2)
		# likelihood_emissivity_edge_laplacian_derivate = 2*(regolarisation_coeff_edge_laplacian**2) * np.dot(emissivity_laplacian_selected_edge_cells_for_laplacian , grid_laplacian_masked_crop_scaled) / (sigma_emissivity**2)
		likelihood_emissivity_laplacian_derivate_all = np.dot(grid_laplacian_masked_crop_scaled*( (regolarisation_coeff**2)*not_selected_super_x_cells + (regolarisation_coeff_edge_laplacian**2)*selected_edge_cells_for_laplacian + (regolarisation_coeff_divertor**2)*selected_super_x_cells) , grid_laplacian_masked_crop_scaled) / (sigma_emissivity**2)

		likelihood_emissivity_edge_derivate = (regolarisation_coeff_edge**2)*np.diag(selected_edge_cells*r_int_2/sigma_emissivity_2*1)
		if regolarisation_coeff_central_column_border_R_derivate==0:
			likelihood_emissivity_central_column_border_R_derivate_derivate = 0
		else:
			likelihood_emissivity_central_column_border_R_derivate_derivate = (regolarisation_coeff_central_column_border_R_derivate**2)*np.dot( grid_R_derivate_masked_crop_scaled*selected_central_column_border_cells ,grid_R_derivate_masked_crop_scaled)/sigma_emissivity_2
		if regolarisation_coeff_central_border_Z_derivate==0:
			likelihood_emissivity_central_border_Z_derivate_derivate = 0
		else:
			likelihood_emissivity_central_border_Z_derivate_derivate = (regolarisation_coeff_central_border_Z_derivate**2)*np.dot( grid_Z_derivate_masked_crop_scaled*selected_central_border_cells ,grid_Z_derivate_masked_crop_scaled)/sigma_emissivity_2
		likelihood_derivate = likelihood_emissivity_pos_derivate + likelihood_emissivity_laplacian_derivate_all + likelihood_emissivity_edge_derivate + likelihood_emissivity_central_column_border_R_derivate_derivate + likelihood_emissivity_central_border_Z_derivate_derivate
		likelihood_homogeneous_offset_derivate = regolarisation_coeff_offsets_multiplier*number_cells_ROI*homogeneous_scaling/(reference_sigma_powernoback**2)
		likelihood_homogeneous_offset_plasma_derivate = regolarisation_coeff_offsets_multiplier*number_cells_plasma*homogeneous_scaling/(reference_sigma_powernoback**2)
		likelihood_power_fit_derivate[:sensitivities_binned_crop.shape[1],:sensitivities_binned_crop.shape[1]]+=likelihood_derivate
		likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+1,sensitivities_binned_crop.shape[1]+1] += likelihood_homogeneous_offset_derivate
		likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1],sensitivities_binned_crop.shape[1]] += likelihood_homogeneous_offset_plasma_derivate
		return likelihood_power_fit_derivate

	def calc_hessian(emissivity_plus,*args):
		# version created with the proper double derivatives of the penalty to all variables. at the moment this does not include empirical bayes
		# time_start = tm.time()
		# emissivity_plus = emissivity_plus
		powernoback = args[0]
		sigma_powernoback = args[1]
		sigma_emissivity = args[2]
		regolarisation_coeff = args[3]
		sigma_powernoback_2 = args[4]
		sigma_emissivity_2 = args[5]
		IRVB_head_power_correction = 0
		if use_pinhole_plate_over_temperature:
			DT_max = emissivity_plus[-4]*homogeneous_scaling
			DT_head = emissivity_plus[-3]*homogeneous_scaling
			DT_ring = emissivity_plus[-2]*homogeneous_scaling
			pinhole_plate_T_slope = emissivity_plus[-1]*homogeneous_scaling
			DT_head_r,IRVB_head_power_correction = calc_IRVB_head_power_correction(DT_max,DT_head,DT_ring,pinhole_plate_T_slope,rmax_pinhole_plate,pinhole_radious,R_IRVB_head_from_pinhole,pinhole_plate_to_foil_weight,ref_temperature,IRVB_head_ring_to_foil_weight)
			IRVB_head_power_correction = IRVB_head_power_correction.flatten()
			emissivity_plus = emissivity_plus[:-4]

		homogeneous_offset = emissivity_plus[-1]*homogeneous_scaling	# scaling added such that all variables have the same order of magnitude
		homogeneous_offset_plasma = emissivity_plus[-2]*homogeneous_scaling	# scaling added such that all variables have the same order of magnitude
		regolarisation_coeff_divertor = regolarisation_coeff*regolarisation_coeff_divertor_multiplier
		regolarisation_coeff_central_column_border_R_derivate = regolarisation_coeff*regolarisation_coeff_central_column_border_R_derivate_multiplier
		regolarisation_coeff_central_border_Z_derivate = regolarisation_coeff*regolarisation_coeff_central_border_Z_derivate_multiplier
		regolarisation_coeff_edge_laplacian = regolarisation_coeff*regolarisation_coeff_edge_laplacian_multiplier
		regolarisation_coeff_offsets = regolarisation_coeff*regolarisation_coeff_offsets_multiplier
		# regolarisation_coeff_edge = regolarisation_coeff*regolarisation_coeff_edge_multiplier
		# print(homogeneous_offset,homogeneous_offset_plasma)
		emissivity = emissivity_plus[:-2]
		# emissivity[emissivity==0] = 1e-10
		# foil_power_guess = np.dot(sensitivities_binned_crop,emissivity) + selected_ROI_internal*homogeneous_offset + homogeneous_offset_plasma*select_foil_region_with_plasma
		# foil_power_error = powernoback - (np.dot(sensitivities_binned_crop,emissivity) + selected_ROI_internal*homogeneous_offset + homogeneous_offset_plasma*select_foil_region_with_plasma)
		foil_power_error = powernoback - (np.dot(sensitivities_binned_crop,emissivity) + selected_ROI_internal*homogeneous_offset + homogeneous_offset_plasma*select_foil_region_with_plasma + IRVB_head_power_correction)
		# print(tm.time()-time_start)
		# time_start = tm.time()

		if use_pinhole_plate_over_temperature:
			likelihood_power_fit_derivate = np.zeros((np.shape(sensitivities_binned_crop)[1]+2+4,np.shape(sensitivities_binned_crop)[1]+2+4)).astype(np.float32)
		else:
			likelihood_power_fit_derivate = np.zeros((np.shape(sensitivities_binned_crop)[1]+2,np.shape(sensitivities_binned_crop)[1]+2)).astype(np.float32)
		# I need to add all the contributions to the error double partial derivative
		# foil fit error, emissivities
		# temp = (sensitivities_binned_crop[selected_ROI_internal].astype(np.float32)).T
		# for i in range(np.shape(sensitivities_binned_crop)[1]):
		# 	likelihood_power_fit_derivate[i,:sensitivities_binned_crop.shape[1]] += np.sum((temp*temp[i])/sigma_powernoback_2[selected_ROI_internal],axis=-1)
		likelihood_power_fit_derivate[:sensitivities_binned_crop.shape[1]][:,:sensitivities_binned_crop.shape[1]] += ((sensitivities_binned_crop.T/sigma_powernoback) @ (sensitivities_binned_crop.T/sigma_powernoback).T)
		# foil fit error, offsets
		likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1],:sensitivities_binned_crop.shape[1]] += np.sum(sensitivities_binned_crop.T/sigma_powernoback_2*select_foil_region_with_plasma,axis=-1)*homogeneous_scaling
		likelihood_power_fit_derivate[:sensitivities_binned_crop.shape[1],sensitivities_binned_crop.shape[1]] += np.sum(sensitivities_binned_crop.T/sigma_powernoback_2*select_foil_region_with_plasma,axis=-1)*homogeneous_scaling
		likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1],sensitivities_binned_crop.shape[1]] += np.sum(1/sigma_powernoback_2*select_foil_region_with_plasma,axis=-1)*homogeneous_scaling*homogeneous_scaling
		likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+1,:sensitivities_binned_crop.shape[1]] += np.sum(sensitivities_binned_crop.T/sigma_powernoback_2,axis=-1)*homogeneous_scaling
		likelihood_power_fit_derivate[:sensitivities_binned_crop.shape[1],sensitivities_binned_crop.shape[1]+1] += np.sum(sensitivities_binned_crop.T/sigma_powernoback_2,axis=-1)*homogeneous_scaling
		likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+1,sensitivities_binned_crop.shape[1]] += np.sum(1/sigma_powernoback_2*select_foil_region_with_plasma,axis=-1)*homogeneous_scaling*homogeneous_scaling
		likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1],sensitivities_binned_crop.shape[1]+1] += np.sum(1/sigma_powernoback_2*select_foil_region_with_plasma,axis=-1)*homogeneous_scaling*homogeneous_scaling
		likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+1,sensitivities_binned_crop.shape[1]+1] += np.sum(1/sigma_powernoback_2,axis=-1)*homogeneous_scaling*homogeneous_scaling
		if use_pinhole_plate_over_temperature:
			dT_DT_max = 1 - ((R_IRVB_head_from_pinhole-pinhole_radious)/(rmax_pinhole_plate-pinhole_radious))**2
			dT_DT_head = 1
			dT_DT_ring = 1
			dT_pinhole_plate_T_slope = - ((R_IRVB_head_from_pinhole-pinhole_radious)**2/(rmax_pinhole_plate-pinhole_radious)) + (R_IRVB_head_from_pinhole-pinhole_radious)
			T_head_r = DT_head_r+ref_temperature+zeroC
			# foil fit error, dT_DT_max
			select_pinhole_plate = R_IRVB_head_from_pinhole<rmax_pinhole_plate
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2,:sensitivities_binned_crop.shape[1]] += np.sum(sensitivities_binned_crop.T * (1/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_max)[:,:,select_pinhole_plate],axis=-1).T.flatten()),axis=-1)*homogeneous_scaling
			likelihood_power_fit_derivate[:sensitivities_binned_crop.shape[1],sensitivities_binned_crop.shape[1]+2] += np.sum(sensitivities_binned_crop.T * (1/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_max)[:,:,select_pinhole_plate],axis=-1).T.flatten()),axis=-1)*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2,sensitivities_binned_crop.shape[1]] += np.sum(select_foil_region_with_plasma * (1/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_max)[:,:,select_pinhole_plate],axis=-1).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1],sensitivities_binned_crop.shape[1]+2] += np.sum(select_foil_region_with_plasma * (1/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_max)[:,:,select_pinhole_plate],axis=-1).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2,sensitivities_binned_crop.shape[1]+1] += np.sum( (1/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_max)[:,:,select_pinhole_plate],axis=-1).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+1,sensitivities_binned_crop.shape[1]+2] += np.sum( (1/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_max)[:,:,select_pinhole_plate],axis=-1).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2,sensitivities_binned_crop.shape[1]+2] += np.sum( (1/sigma_powernoback_2 * (np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_max)[:,:,select_pinhole_plate],axis=-1).T.flatten())**2))*homogeneous_scaling*homogeneous_scaling - np.sum(foil_power_error/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*3*(T_head_r**2)*(dT_DT_max**2) )[:,:,select_pinhole_plate],axis=-1).T.flatten() )*homogeneous_scaling*homogeneous_scaling
			# foil fit error, dT_DT_head
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+1,:sensitivities_binned_crop.shape[1]] += np.sum(sensitivities_binned_crop.T * (1/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_head)[:,:,:],axis=-1).T.flatten()),axis=-1)*homogeneous_scaling
			likelihood_power_fit_derivate[:sensitivities_binned_crop.shape[1],sensitivities_binned_crop.shape[1]+2+1] += np.sum(sensitivities_binned_crop.T * (1/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_head)[:,:,:],axis=-1).T.flatten()),axis=-1)*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+1,sensitivities_binned_crop.shape[1]] += np.sum(select_foil_region_with_plasma * (1/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_head)[:,:,:],axis=-1).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1],sensitivities_binned_crop.shape[1]+2+1] += np.sum(select_foil_region_with_plasma * (1/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_head)[:,:,:],axis=-1).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+1,sensitivities_binned_crop.shape[1]+1] += np.sum( (1/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_head)[:,:,:],axis=-1).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+1,sensitivities_binned_crop.shape[1]+2+1] += np.sum( (1/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_head)[:,:,:],axis=-1).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+1,sensitivities_binned_crop.shape[1]+2] += np.sum( (1/sigma_powernoback_2 * (np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_max)[:,:,select_pinhole_plate],axis=-1)*np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_head)[:,:,:],axis=-1)).T.flatten()))*homogeneous_scaling*homogeneous_scaling - np.sum(foil_power_error/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*3*(T_head_r**2)*(dT_DT_max*dT_DT_head))[:,:,select_pinhole_plate],axis=-1).T.flatten() )*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2,sensitivities_binned_crop.shape[1]+2+1] += np.sum( (1/sigma_powernoback_2 * (np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_max)[:,:,select_pinhole_plate],axis=-1)*np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_head)[:,:,:],axis=-1)).T.flatten()))*homogeneous_scaling*homogeneous_scaling - np.sum(foil_power_error/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*3*(T_head_r**2)*(dT_DT_max*dT_DT_head))[:,:,select_pinhole_plate],axis=-1).T.flatten() )*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+1,sensitivities_binned_crop.shape[1]+2+1] += np.sum( (1/sigma_powernoback_2 * (np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_head)[:,:,:],axis=-1).T.flatten())**2))*homogeneous_scaling*homogeneous_scaling - np.sum(foil_power_error/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*3*(T_head_r**2)*(dT_DT_head**2))[:,:,:],axis=-1).T.flatten() )*homogeneous_scaling*homogeneous_scaling
			# foil fit error, dT_DT_ring
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+2,:sensitivities_binned_crop.shape[1]] += np.sum(sensitivities_binned_crop.T * (1/sigma_powernoback_2 * (IRVB_head_ring_to_foil_weight.T*4*((DT_ring+ref_temperature+zeroC)**3)*dT_DT_ring).T.flatten()),axis=-1)*homogeneous_scaling
			likelihood_power_fit_derivate[:sensitivities_binned_crop.shape[1],sensitivities_binned_crop.shape[1]+2+2] += np.sum(sensitivities_binned_crop.T * (1/sigma_powernoback_2 * (IRVB_head_ring_to_foil_weight.T*4*((DT_ring+ref_temperature+zeroC)**3)*dT_DT_ring).T.flatten()),axis=-1)*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+2,sensitivities_binned_crop.shape[1]] += np.sum(select_foil_region_with_plasma * (1/sigma_powernoback_2 * (IRVB_head_ring_to_foil_weight.T*4*((DT_ring+ref_temperature+zeroC)**3)*dT_DT_ring).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1],sensitivities_binned_crop.shape[1]+2+2] += np.sum(select_foil_region_with_plasma * (1/sigma_powernoback_2 * (IRVB_head_ring_to_foil_weight.T*4*((DT_ring+ref_temperature+zeroC)**3)*dT_DT_ring).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+2,sensitivities_binned_crop.shape[1]+1] += np.sum( (1/sigma_powernoback_2 * (IRVB_head_ring_to_foil_weight.T*4*((DT_ring+ref_temperature+zeroC)**3)*dT_DT_ring).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+1,sensitivities_binned_crop.shape[1]+2+2] += np.sum( (1/sigma_powernoback_2 * (IRVB_head_ring_to_foil_weight.T*4*((DT_ring+ref_temperature+zeroC)**3)*dT_DT_ring).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+2,sensitivities_binned_crop.shape[1]+2] += np.sum( (1/sigma_powernoback_2 * (np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_max)[:,:,select_pinhole_plate],axis=-1)* (IRVB_head_ring_to_foil_weight.T*4*((DT_ring+ref_temperature+zeroC)**3)*dT_DT_ring)).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2,sensitivities_binned_crop.shape[1]+2+2] += np.sum( (1/sigma_powernoback_2 * (np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_max)[:,:,select_pinhole_plate],axis=-1)* (IRVB_head_ring_to_foil_weight.T*4*((DT_ring+ref_temperature+zeroC)**3)*dT_DT_ring)).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+2,sensitivities_binned_crop.shape[1]+2+1] += np.sum( (1/sigma_powernoback_2 * (np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_head)[:,:,:],axis=-1)* (IRVB_head_ring_to_foil_weight.T*4*((DT_ring+ref_temperature+zeroC)**3)*dT_DT_ring)).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+1,sensitivities_binned_crop.shape[1]+2+2] += np.sum( (1/sigma_powernoback_2 * (np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_head)[:,:,:],axis=-1)* (IRVB_head_ring_to_foil_weight.T*4*((DT_ring+ref_temperature+zeroC)**3)*dT_DT_ring)).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+2,sensitivities_binned_crop.shape[1]+2+2] += np.sum( (1/sigma_powernoback_2 * ((IRVB_head_ring_to_foil_weight.T*4*((DT_ring+ref_temperature+zeroC)**3)*dT_DT_ring).T.flatten())**2))*homogeneous_scaling*homogeneous_scaling - np.sum(foil_power_error/sigma_powernoback_2 * (IRVB_head_ring_to_foil_weight.T*4*3*((DT_ring+ref_temperature+zeroC)**2)*(dT_DT_ring**2)).T.flatten() )*homogeneous_scaling*homogeneous_scaling
			# foil fit error, dT_pinhole_plate_T_slope
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+3,:sensitivities_binned_crop.shape[1]] += np.sum(sensitivities_binned_crop.T * (1/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_pinhole_plate_T_slope)[:,:,select_pinhole_plate],axis=-1).T.flatten()),axis=-1)*homogeneous_scaling
			likelihood_power_fit_derivate[:sensitivities_binned_crop.shape[1],sensitivities_binned_crop.shape[1]+2+3] += np.sum(sensitivities_binned_crop.T * (1/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_pinhole_plate_T_slope)[:,:,select_pinhole_plate],axis=-1).T.flatten()),axis=-1)*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+3,sensitivities_binned_crop.shape[1]] += np.sum(select_foil_region_with_plasma * (1/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_pinhole_plate_T_slope)[:,:,select_pinhole_plate],axis=-1).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1],sensitivities_binned_crop.shape[1]+2+3] += np.sum(select_foil_region_with_plasma * (1/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_pinhole_plate_T_slope)[:,:,select_pinhole_plate],axis=-1).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+3,sensitivities_binned_crop.shape[1]+1] += np.sum( (1/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_pinhole_plate_T_slope)[:,:,select_pinhole_plate],axis=-1).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+1,sensitivities_binned_crop.shape[1]+2+3] += np.sum( (1/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_pinhole_plate_T_slope)[:,:,select_pinhole_plate],axis=-1).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+3,sensitivities_binned_crop.shape[1]+2] += np.sum( (1/sigma_powernoback_2 * (np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_max)[:,:,select_pinhole_plate],axis=-1)*np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_pinhole_plate_T_slope)[:,:,select_pinhole_plate],axis=-1)).T.flatten()))*homogeneous_scaling*homogeneous_scaling - np.sum(foil_power_error/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*3*(T_head_r**2)*(dT_DT_max*dT_pinhole_plate_T_slope))[:,:,select_pinhole_plate],axis=-1).T.flatten() )*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2,sensitivities_binned_crop.shape[1]+2+3] += np.sum( (1/sigma_powernoback_2 * (np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_max)[:,:,select_pinhole_plate],axis=-1)*np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_pinhole_plate_T_slope)[:,:,select_pinhole_plate],axis=-1)).T.flatten()))*homogeneous_scaling*homogeneous_scaling - np.sum(foil_power_error/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*3*(T_head_r**2)*(dT_DT_max*dT_pinhole_plate_T_slope))[:,:,select_pinhole_plate],axis=-1).T.flatten() )*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+3,sensitivities_binned_crop.shape[1]+2+1] += np.sum( (1/sigma_powernoback_2 * (np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_head)[:,:,:],axis=-1)*np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_pinhole_plate_T_slope)[:,:,select_pinhole_plate],axis=-1)).T.flatten()))*homogeneous_scaling*homogeneous_scaling - np.sum(foil_power_error/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*3*(T_head_r**2)*(dT_DT_head*dT_pinhole_plate_T_slope))[:,:,select_pinhole_plate],axis=-1).T.flatten() )*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+1,sensitivities_binned_crop.shape[1]+2+3] += np.sum( (1/sigma_powernoback_2 * (np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_DT_head)[:,:,:],axis=-1)*np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_pinhole_plate_T_slope)[:,:,select_pinhole_plate],axis=-1)).T.flatten()))*homogeneous_scaling*homogeneous_scaling - np.sum(foil_power_error/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*3*(T_head_r**2)*(dT_DT_head*dT_pinhole_plate_T_slope))[:,:,select_pinhole_plate],axis=-1).T.flatten() )*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+3,sensitivities_binned_crop.shape[1]+2+2] += np.sum( (1/sigma_powernoback_2 * (np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_pinhole_plate_T_slope)[:,:,select_pinhole_plate],axis=-1)* (IRVB_head_ring_to_foil_weight.T*4*((DT_ring+ref_temperature+zeroC)**3)*dT_DT_ring)).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+2,sensitivities_binned_crop.shape[1]+2+3] += np.sum( (1/sigma_powernoback_2 * (np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_pinhole_plate_T_slope)[:,:,select_pinhole_plate],axis=-1)* (IRVB_head_ring_to_foil_weight.T*4*((DT_ring+ref_temperature+zeroC)**3)*dT_DT_ring)).T.flatten()))*homogeneous_scaling*homogeneous_scaling
			likelihood_power_fit_derivate[sensitivities_binned_crop.shape[1]+2+3,sensitivities_binned_crop.shape[1]+2+3] += np.sum( (1/sigma_powernoback_2 * (np.sum((pinhole_plate_to_foil_weight.T*4*(T_head_r**3)*dT_pinhole_plate_T_slope)[:,:,select_pinhole_plate],axis=-1).T.flatten())**2))*homogeneous_scaling*homogeneous_scaling - np.sum(foil_power_error/sigma_powernoback_2 * np.sum((pinhole_plate_to_foil_weight.T*4*3*(T_head_r**2)*(dT_pinhole_plate_T_slope**2))[:,:,select_pinhole_plate],axis=-1).T.flatten() )*homogeneous_scaling*homogeneous_scaling
		# laplacian, radial derivative, Z derivative at the top domain boundary, non negativity penalty, emissivities
		# temporary definitions to speed up calculations
		temp1 = ((regolarisation_coeff)*not_selected_super_x_cells + (regolarisation_coeff_divertor)*selected_super_x_cells + (regolarisation_coeff_edge_laplacian)*selected_edge_cells_for_laplacian + (regolarisation_coeff_edge)*selected_edge_cells)/sigma_emissivity
		# select1 = temp1!=0
		# temp1 = temp1[select1].astype(np.float32)
		# temp1_ = grid_laplacian_masked_crop_scaled[select1][:,select1].astype(np.float32)
		if central_column_border_R_derivate_correction_present:
			temp2 = ((regolarisation_coeff_central_column_border_R_derivate)*selected_central_column_border_cells)/sigma_emissivity
		else:
			temp2 = 0
		# select2 = temp2!=0
		# temp2 = temp2[select2].astype(np.float32)
		# temp2_ = grid_R_derivate_masked_crop_scaled[select2][:,select2].astype(np.float32)
		if central_border_Z_derivate_correction_present:
			temp3 = ((regolarisation_coeff_central_border_Z_derivate)*selected_central_border_cells)/sigma_emissivity
		else:
			temp3 = 0
		# select3 = temp3!=0
		# temp3 = temp3[select3].astype(np.float32)
		# temp3_ = grid_Z_derivate_masked_crop_scaled[select3][:,select3].astype(np.float32)
		# for i in range(np.shape(sensitivities_binned_crop)[1]):
		# 	# laplacian, emissivities
		# 	likelihood_power_fit_derivate[i,:sensitivities_binned_crop.shape[1]] += 2*np.sum((grid_laplacian_masked_crop_scaled.T*grid_laplacian_masked_crop_scaled[:,i]*temp1),axis=-1)*temp1[i]
		# 	# radial derivative, emissivities
		# 	likelihood_power_fit_derivate[i,:sensitivities_binned_crop.shape[1]] += 2*np.sum((grid_R_derivate_masked_crop_scaled.T*grid_R_derivate_masked_crop_scaled[:,i]*temp2),axis=-1)*temp2[i]
		# 	# Z derivative at the top domain boundary, emissivities
		# 	likelihood_power_fit_derivate[i,:sensitivities_binned_crop.shape[1]] += 2*np.sum((grid_Z_derivate_masked_crop_scaled.T*grid_Z_derivate_masked_crop_scaled[:,i]*temp3),axis=-1)*temp3[i]
		# 	# non negativity penalty, emissivities. i add it here as i cannot operate on the diagonal of the matrix easily, as far as i know
		# 	likelihood_power_fit_derivate[i,i] += (regolarisation_coeff_non_negativity_multiplier**2)*(emissivity[i]<0)*(not selected_edge_cells[i])*2*((r_int[i]/sigma_emissivity)**2)	# I added a weight on the redious, becaus the power increase with radious and a negative voxel at high r is more important that one at low r
		# laplacian, emissivities
		# for i in range(np.sum(select1)):
		# 	likelihood_power_fit_derivate[:sensitivities_binned_crop.shape[1]][select1][i,:sensitivities_binned_crop.shape[1]][select1] += 2*np.sum((temp1_.T*temp1_[:,i]*temp1),axis=-1)*temp1[i]
		likelihood_power_fit_derivate[:sensitivities_binned_crop.shape[1]][:,:sensitivities_binned_crop.shape[1]] += 2*(temp1*grid_laplacian_masked_crop_scaled) @ ((temp1*grid_laplacian_masked_crop_scaled).T)
		# radial derivative, emissivities
		# for i in range(np.sum(select2)):
		# 	likelihood_power_fit_derivate[:sensitivities_binned_crop.shape[1]][select2][i,:sensitivities_binned_crop.shape[1]][select2] += 2*np.sum((temp2_.T*temp2_[:,i]*temp2),axis=-1)*temp2[i]
		likelihood_power_fit_derivate[:sensitivities_binned_crop.shape[1]][:,:sensitivities_binned_crop.shape[1]] += 2*(temp2*grid_R_derivate_masked_crop_scaled) @ ((temp2*grid_R_derivate_masked_crop_scaled).T)
		# Z derivative at the top domain boundary, emissivities
		# for i in range(np.sum(select3)):
		# 	likelihood_power_fit_derivate[:sensitivities_binned_crop.shape[1]][select3][i,:sensitivities_binned_crop.shape[1]][select3] += 2*np.sum((temp3_.T*temp3_[:,i]*temp3),axis=-1)*temp3[i]
		likelihood_power_fit_derivate[:sensitivities_binned_crop.shape[1]][:,:sensitivities_binned_crop.shape[1]] += 2*(temp3*grid_Z_derivate_masked_crop_scaled) @ ((temp3*grid_Z_derivate_masked_crop_scaled).T)
		# non negativity penalty, emissivities
		# for i in range(np.shape(sensitivities_binned_crop)[1]):
		# 	likelihood_power_fit_derivate[i,i] += (regolarisation_coeff_non_negativity_multiplier**2)*(emissivity[i]<0)*(not selected_edge_cells[i])*2*((r_int[i]/sigma_emissivity)**2)	# I added a weight on the redious, becaus the power increase with radious and a negative voxel at high r is more important that one at low r
		likelihood_power_fit_derivate[:sensitivities_binned_crop.shape[1]][:,:sensitivities_binned_crop.shape[1]] += 2*np.diag((regolarisation_coeff_non_negativity_multiplier**2)*(emissivity<0)*(np.logical_not(selected_edge_cells))*2*((r_int/sigma_emissivity)**2))	# I added a weight on the redious, becaus the power increase with radious and a negative voxel at high r is more important that one at low r

		return likelihood_power_fit_derivate

	return prob_and_gradient,calc_hessian

def loop_fit_over_regularisation(prob_and_gradient,regolarisation_coeff_range,guess,grid_data_masked_crop,powernoback,sigma_powernoback,sigma_emissivity,factr=1e5,pgtol=1e-6,maxiter=15000,x_optimal_all_guess=[],iprint=0,excluded_cells = [],pinhole_location = locate_pinhole(),fix_pinhole_emis_zero=False,maxls=20,m=10):
	import scipy
	import time as tm

	# added 2024/10/02 to allow logging
	likelihood_logging_all = []
	derivate_logging_all = []
	global likelihood_logging,derivate_logging

	voxels_centre = np.mean(grid_data_masked_crop,axis=1)
	dr = np.median(np.diff(np.unique(voxels_centre[:,0])))
	dz = np.median(np.diff(np.unique(voxels_centre[:,1])))
	dist_mean = (dz**2 + dr**2)/2
	if len(excluded_cells)==0:
		excluded_cells = np.ones_like(sigma_powernoback).astype(bool)
	else:
		excluded_cells = np.logical_not(excluded_cells)
	if len(guess) > (len(grid_data_masked_crop) + 2):
		bds = [[None,None]]*(len(grid_data_masked_crop)) + [[-0.01,0.01]] + [[-0.01,0.01]] + [[0,np.inf]]*3 + [[-np.inf,0]]
	else:
		bds = [[None,None]]*(len(grid_data_masked_crop)) + [[-0.01,0.01]] + [[-0.01,0.01]]

	if fix_pinhole_emis_zero:
		pinhole_location = np.array([(pinhole_location[0]**2 + pinhole_location[1]**2)**0.5,pinhole_location[2]])
		bds[np.abs((voxels_centre[:,0]-pinhole_location[0])**2 + (voxels_centre[:,1]-pinhole_location[1])**2).argmin()] = [-1,1]	# arbitrarily small bound
		guess[np.abs((voxels_centre[:,0]-pinhole_location[0])**2 + (voxels_centre[:,1]-pinhole_location[1])**2).argmin()] = 0

	x_optimal_all = []
	recompose_voxel_emissivity_all = []
	recompose_voxel_emissivity_excluded_all = []
	y_opt_all = []
	opt_info_all = []
	start = tm.time()
	for i_regolarisation_coeff,regolarisation_coeff in enumerate(regolarisation_coeff_range):
		likelihood_logging = []
		derivate_logging = []
		print('regolarisation_coeff = '+str(regolarisation_coeff))
		args = [powernoback,sigma_powernoback,sigma_emissivity,regolarisation_coeff,sigma_powernoback**2,sigma_emissivity**2]
		if len(regolarisation_coeff_range)==len(x_optimal_all_guess):
			# temp = np.mean([x_optimal_all_guess[i_regolarisation_coeff],guess],axis=0)
			temp = x_optimal_all_guess[i_regolarisation_coeff]
			x_optimal, y_opt, opt_info = scipy.optimize.fmin_l_bfgs_b(prob_and_gradient, x0=temp, args = (args), iprint=iprint, factr=factr, pgtol=pgtol,bounds=bds,maxiter=maxiter)#,m=1000, maxls=1000, pgtol=1e-10, factr=1e0)#,approx_grad = True)
		else:
			x_optimal, y_opt, opt_info = scipy.optimize.fmin_l_bfgs_b(prob_and_gradient, x0=guess, args = (args), iprint=iprint, factr=factr, pgtol=pgtol,bounds=bds,maxiter=maxiter)#,m=1000, maxls=1000, pgtol=1e-10, factr=1e0)#,approx_grad = True)
		x_optimal_all.append(x_optimal)
		y_opt_all.append(y_opt)
		opt_info_all.append(opt_info)
		if i_regolarisation_coeff==0:
			guess = cp.deepcopy(x_optimal)
		else:
			guess = x_optimal_all[-1] + np.diff(x_optimal_all[-2:],axis=0)[-1]

		recompose_voxel_emissivity = np.zeros((len(np.unique(voxels_centre[:,0])),len(np.unique(voxels_centre[:,1]))))*np.nan
		recompose_voxel_emissivity_excluded = np.zeros_like(recompose_voxel_emissivity)*np.nan
		for i_r,r in enumerate(np.unique(voxels_centre[:,0])):
			for i_z,z in enumerate(np.unique(voxels_centre[:,1])):
				dist = (voxels_centre[:,0]-r)**2 + (voxels_centre[:,1]-z)**2
				if dist.min()<dist_mean/2:
					index = np.abs(dist).argmin()
					recompose_voxel_emissivity[i_r,i_z] = x_optimal[index]*excluded_cells[index]
					recompose_voxel_emissivity_excluded[i_r,i_z] = x_optimal[index]*np.logical_not(excluded_cells[index])
		recompose_voxel_emissivity *= 4*np.pi	# this exist because the sensitivity matrix is built with 1W/str/m^3/ x nm emitters while I use 1W as reference, so I need to multiply the results by 4pi
		recompose_voxel_emissivity_all.append(recompose_voxel_emissivity)
		recompose_voxel_emissivity_excluded *= 4*np.pi	# this exist because the sensitivity matrix is built with 1W/str/m^3/ x nm emitters while I use 1W as reference, so I need to multiply the results by 4pi
		recompose_voxel_emissivity_excluded_all.append(recompose_voxel_emissivity_excluded)
		print('done in %.3gs' %(tm.time()-start))
		start = tm.time()
		likelihood_logging_all.append(likelihood_logging)
		derivate_logging_all.append(derivate_logging)
	return x_optimal_all,recompose_voxel_emissivity_all,y_opt_all,opt_info_all,voxels_centre,recompose_voxel_emissivity_excluded_all,likelihood_logging_all,derivate_logging_all

def find_optimal_regularisation(score_x,score_y,regolarisation_coeff_range,x_optimal_all,recompose_voxel_emissivity_all,y_opt_all,opt_info_all,recompose_voxel_emissivity_excluded_all,curvature_fit_regularisation_interval = 0.05,fraction_of_L_curve_for_fit = 0.08,regolarisation_coeff_upper_limit = 10**-0.2,regolarisation_coeff_lower_limit = 1e-4,forward_model_residuals=False,avoid_score_y_rise=True):
	import collections
	from scipy.signal import find_peaks, peak_prominences as get_proms
	from scipy.ndimage import median_filter

	# I need to remove the cases where points are overlapped
	points_removed = []
	counter_score_x = collections.Counter(score_x)
	counter_score_y = collections.Counter(score_y)
	test = np.logical_and( [value in np.array(list(counter_score_x.items()))[:,0][np.array(list(counter_score_x.items()))[:,1]>1] for value in score_x] , [value in np.array(list(counter_score_y.items()))[:,0][np.array(list(counter_score_y.items()))[:,1]>1] for value in score_y] )
	while np.sum(test)>0:	# removing points one on top of each other
		i__ = test.argmax()
		# print(i__)
		regolarisation_coeff_range = np.concatenate([regolarisation_coeff_range[:i__],regolarisation_coeff_range[i__+1:]])
		score_x = np.concatenate([score_x[:i__],score_x[i__+1:]])
		score_y = np.concatenate([score_y[:i__],score_y[i__+1:]])
		x_optimal_all = np.concatenate([x_optimal_all[:i__],x_optimal_all[i__+1:]])
		recompose_voxel_emissivity_all = np.concatenate([recompose_voxel_emissivity_all[:i__],recompose_voxel_emissivity_all[i__+1:]])
		recompose_voxel_emissivity_excluded_all = np.concatenate([recompose_voxel_emissivity_excluded_all[:i__],recompose_voxel_emissivity_excluded_all[i__+1:]])
		y_opt_all = np.concatenate([y_opt_all[:i__],y_opt_all[i__+1:]])
		opt_info_all = np.concatenate([opt_info_all[:i__],opt_info_all[i__+1:]])
		counter_score_x = collections.Counter(score_x)
		counter_score_y = collections.Counter(score_y)
		test = np.logical_and( [value in np.array(list(counter_score_x.items()))[:,0][np.array(list(counter_score_x.items()))[:,1]>1] for value in score_x] , [value in np.array(list(counter_score_y.items()))[:,0][np.array(list(counter_score_y.items()))[:,1]>1] for value in score_y] )
		points_removed.append(i__)
	if forward_model_residuals == False:
		test = np.diff(score_x)
		while np.sum(test<0)>0:	# removing points for which quality of the fit derease for decreasing regularisation. it shouldn't happen, but it actually can for forwards modelled data
			# i__ = test.argmin()+1
			i__ = len(test)-np.flip(test,axis=0).argmin()-1	# this way it should remove right to left
			if i__<5:
				i__ = test.argmin()
			# print(i__)
			regolarisation_coeff_range = np.concatenate([regolarisation_coeff_range[:i__],regolarisation_coeff_range[i__+1:]])
			score_x = np.concatenate([score_x[:i__],score_x[i__+1:]])
			score_y = np.concatenate([score_y[:i__],score_y[i__+1:]])
			x_optimal_all = np.concatenate([x_optimal_all[:i__],x_optimal_all[i__+1:]])
			recompose_voxel_emissivity_all = np.concatenate([recompose_voxel_emissivity_all[:i__],recompose_voxel_emissivity_all[i__+1:]])
			recompose_voxel_emissivity_excluded_all = np.concatenate([recompose_voxel_emissivity_excluded_all[:i__],recompose_voxel_emissivity_excluded_all[i__+1:]])
			y_opt_all = np.concatenate([y_opt_all[:i__],y_opt_all[i__+1:]])
			opt_info_all = np.concatenate([opt_info_all[:i__],opt_info_all[i__+1:]])
			test = np.diff(score_x)
			points_removed.append(i__)
	if avoid_score_y_rise == True:
		test = np.diff(score_y)
		while np.sum(test>0)>0:	# removing points for which the laplacian reduces for increasing regularisation (it shouldn't happen). it actually happens sometimes.
			i__ = test.argmax()+1
			# print(i__)
			regolarisation_coeff_range = np.concatenate([regolarisation_coeff_range[:i__],regolarisation_coeff_range[i__+1:]])
			score_x = np.concatenate([score_x[:i__],score_x[i__+1:]])
			score_y = np.concatenate([score_y[:i__],score_y[i__+1:]])
			x_optimal_all = np.concatenate([x_optimal_all[:i__],x_optimal_all[i__+1:]])
			recompose_voxel_emissivity_all = np.concatenate([recompose_voxel_emissivity_all[:i__],recompose_voxel_emissivity_all[i__+1:]])
			recompose_voxel_emissivity_excluded_all = np.concatenate([recompose_voxel_emissivity_excluded_all[:i__],recompose_voxel_emissivity_excluded_all[i__+1:]])
			y_opt_all = np.concatenate([y_opt_all[:i__],y_opt_all[i__+1:]])
			opt_info_all = np.concatenate([opt_info_all[:i__],opt_info_all[i__+1:]])
			test = np.diff(score_y)
			points_removed.append(i__)
	length_of_line = (np.diff(np.log10(score_y))**2 + np.diff(np.log10(score_x))**2)**0.5
	test = length_of_line<np.median(length_of_line)/1000
	while np.sum(test)>0:	# removing points that are too close each other
		i__ = test.argmax()
		# print(i__)
		regolarisation_coeff_range = np.concatenate([regolarisation_coeff_range[:i__],regolarisation_coeff_range[i__+1:]])
		score_x = np.concatenate([score_x[:i__],score_x[i__+1:]])
		score_y = np.concatenate([score_y[:i__],score_y[i__+1:]])
		x_optimal_all = np.concatenate([x_optimal_all[:i__],x_optimal_all[i__+1:]])
		recompose_voxel_emissivity_all = np.concatenate([recompose_voxel_emissivity_all[:i__],recompose_voxel_emissivity_all[i__+1:]])
		recompose_voxel_emissivity_excluded_all = np.concatenate([recompose_voxel_emissivity_excluded_all[:i__],recompose_voxel_emissivity_excluded_all[i__+1:]])
		y_opt_all = np.concatenate([y_opt_all[:i__],y_opt_all[i__+1:]])
		opt_info_all = np.concatenate([opt_info_all[:i__],opt_info_all[i__+1:]])
		length_of_line = (np.diff(np.log10(score_y))**2 + np.diff(np.log10(score_x))**2)**0.5
		test = length_of_line<np.median(length_of_line)/1000
		points_removed.append(i__)
	points_removed = np.flip(points_removed,axis=0)
	# plt.figure()
	# plt.plot(score_x,score_y)
	# plt.plot(score_x,score_y,'+')
	# plt.xlabel('||Gm-d||2')
	# plt.ylabel('||Laplacian(m)||2')
	# plt.semilogx()
	# plt.semilogy()
	# plt.grid()
	# plt.pause(0.01)

	score_y = np.log(score_y)
	score_x = np.log(score_x)

	score_y_record_rel = (score_y-score_y.min())/(score_y.max()-score_y.min())
	score_x_record_rel = (score_x-score_x.min())/(score_x.max()-score_x.min())

	# # mini smoothing
	# score_x_record_rel = median_filter(score_x_record_rel,size=[3])
	# score_y_record_rel = median_filter(score_y_record_rel,size=[3])

	length_of_line = np.sum((np.diff(score_y_record_rel)**2 + np.diff(score_x_record_rel)**2)**0.5)
	fraction_of_line_as_range = length_of_line*fraction_of_L_curve_for_fit/2*(3+6)/(np.log10(regolarisation_coeff_range).max()-np.log10(regolarisation_coeff_range).min())	# arbitrary
	length_of_line = (np.diff(score_y_record_rel)**2 + np.diff(score_x_record_rel)**2)**0.5
	# if I use a fine regularisation range I need to fit the curvature over more points. this takes care of that.
	# curvature_range was originally = 2
	curvature_range_int = max(1,int(np.ceil(np.abs(-curvature_fit_regularisation_interval*(np.max(np.log10(regolarisation_coeff_range))-np.min(np.log10(regolarisation_coeff_range)))/np.median(np.diff(np.log10(regolarisation_coeff_range)))-1)/2)))
	print('curvature_range_int = '+str(curvature_range_int))


	# plt.figure()
	# plt.plot(regolarisation_coeff_range,score_x_record_rel,label='fit error')
	# plt.plot(regolarisation_coeff_range,score_y_record_rel,label='laplacian error')
	# plt.legend()
	# plt.semilogx()
	# plt.pause(0.01)
	#
	# plt.figure()
	# plt.plot(score_x_record_rel,score_y_record_rel)
	# plt.plot(score_x_record_rel,score_y_record_rel,'+')
	# plt.xlabel('log ||Gm-d||2')
	# plt.ylabel('log ||Laplacian(m)||2')
	# plt.grid()
	# plt.pause(0.01)


	def distance_spread(coord):
		def int(trash,px,py):
			x = coord[0]
			y = coord[1]
			dist = ((x-px)**2 + (y-py)**2)**0.5
			spread = np.sum((dist-np.mean(dist))**2)
			# print(spread)
			return [spread]*5
		return int

	def distance_spread_and_gradient(coord):
		def int(arg):
			# print(arg)
			x = coord[0]
			y = coord[1]
			px = arg[0]
			py = arg[1]
			dist = ((x-px)**2 + (y-py)**2)**0.5
			spread = np.sum((dist-np.mean(dist))**2)
			# print(str([spread,dist]))
			temp = (((x-px)**2 + (y-py)**2)**-0.5)
			derivate = np.array([np.sum(2*(dist-np.mean(dist))*( -0.5*temp*2*(x-px) - np.mean(-0.5*temp*2*(x-px)) )) , np.sum(2*(dist-np.mean(dist))*( -0.5*temp*2*(y-py) - np.mean(-0.5*temp*2*(y-py)) ))])
			return spread,derivate
		return int

	# plt.figure()
	# plt.plot(score_x_record_rel,score_y_record_rel)
	curvature_range = 3
	curvature_radious = []
	curvature_range_left_all = []
	curvature_range_right_all = []
	for ii in range(len(score_y_record_rel)):
		if ii<=curvature_range:
			curvature_radious.append(np.inf)
			continue
		elif ii>=len(score_y_record_rel)-curvature_range:
			curvature_radious.append(np.inf)
			continue

		curvature_range_left = 3
		while np.sum(length_of_line[max(0,ii-curvature_range_left):ii])<fraction_of_line_as_range and ii-curvature_range_left>0:
			curvature_range_left += 1
		curvature_range_right = 3
		while np.sum(length_of_line[ii+1:ii+curvature_range_right+1])<fraction_of_line_as_range and ii+curvature_range_right+1<len(score_y_record_rel):
			curvature_range_right += 1

		curvature_range_left_all.append(curvature_range_left)
		curvature_range_right_all.append(curvature_range_right)
		# print(ii)
		# # print(curvature_range)
		print(curvature_range_left)
		print(curvature_range_right)
		# # try:
		# # 	guess = centre[0]
		# # except:
		# print(ii)
		try:
			border_left=max(0,ii-curvature_range_left)
			border_right=min(ii+curvature_range_right+1,len(score_y_record_rel))
			# bds = [[np.min(score_y_record_rel[ii-2:ii+2+1]),np.min(score_x_record_rel[ii-2:ii+2+1])],[np.inf,np.inf]]
			# centre = curve_fit(distance_spread_and_gradient([score_x_record_rel[ii-2:ii+2+1],score_y_record_rel[ii-2:ii+2+1]]),[0]*5,[0]*5,p0=guess,bounds = bds,maxfev=1e5,gtol=1e-12,verbose=1)
			if forward_model_residuals:
				guess = [np.max(score_x_record_rel[border_left:border_right])*1,np.mean(score_y_record_rel[border_left:border_right])*1]
				bds = [[np.min(score_x_record_rel[border_left:border_right]),np.inf],[-np.inf,np.inf]]
			else:
				guess = np.mean([score_x_record_rel[border_left:border_right]*1,score_y_record_rel[border_left:border_right]*1],axis=1)
				temp=np.polyfit(score_x_record_rel[border_left:border_right],score_y_record_rel[border_left:border_right],1)
				guess += fraction_of_line_as_range*1*np.array([-temp[0],np.abs(1/temp[0])])
				# bds = [[np.min(score_x_record_rel[border_left:border_right]),np.inf],[np.min(score_y_record_rel[border_left:border_right]),np.inf]]
				bds = [[-np.inf,np.inf],[np.min(score_y_record_rel[border_left:border_right]),np.inf]]
			# guess += np.std([score_x_record_rel[border_left:border_right]*1,score_y_record_rel[border_left:border_right]*1],axis=1)
			# bds = [[score_y_record_rel[ii],np.inf],[score_x_record_rel[ii],np.inf]]
			centre, y_opt, opt_info = scipy.optimize.fmin_l_bfgs_b(distance_spread_and_gradient([score_x_record_rel[border_left:border_right],score_y_record_rel[border_left:border_right]]), x0=guess, bounds = bds, iprint=0, factr=1e-2, pgtol=1e-10)#,m=1000, maxls=1000, pgtol=1e-10, factr=1e0)#,approx_grad = True)
			centre = [centre]

			dist = ((score_x_record_rel[border_left:border_right]-centre[0][0])**2 + (score_y_record_rel[border_left:border_right]-centre[0][1])**2)**0.5
			radious = np.mean(dist)
			# plt.figure()
			# plt.plot(score_x_record_rel[border_left:border_right],score_y_record_rel[border_left:border_right],'+')
			# plt.plot(score_x_record_rel[ii],score_y_record_rel[ii],'o')
			# plt.plot(centre[0][0],centre[0][1],'o')
			# plt.plot(guess[0],guess[1],'v')
			# plt.plot(np.linspace(centre[0][0]-radious,centre[0][0]+radious),centre[0][1]+(radious**2-np.linspace(-radious,+radious)**2)**0.5)
			# plt.plot(np.linspace(centre[0][0]-radious,centre[0][0]+radious),centre[0][1]-(radious**2-np.linspace(-radious,+radious)**2)**0.5)
			# plt.axhline(y=np.min(score_y_record_rel[border_left:border_right]),linestyle='--')
			# plt.axvline(x=np.min(score_x_record_rel[border_left:border_right]),linestyle='--')
			# plt.pause(0.01)
		except:
			radious = np.inf
		curvature_radious.append(radious)
	# curvature_radious = [np.max(curvature_radious)]+curvature_radious+[np.max(curvature_radious)]
	Lcurve_curvature = 1/np.array(curvature_radious)
	# plt.figure()
	# plt.plot(regolarisation_coeff_range[curvature_range:-curvature_range],Lcurve_curvature)
	# plt.semilogx()
	# plt.pause(0.01)

	peaks_all = find_peaks(Lcurve_curvature)[0]
	peaks = cp.deepcopy(peaks_all)
	proms = get_proms(Lcurve_curvature,peaks)[0]
	if False:	 # a free search doesn't really work, so I force sensible results
		proms = proms[Lcurve_curvature[peaks]>np.max(Lcurve_curvature[peaks])/4]
		peaks = peaks[Lcurve_curvature[peaks]>np.max(Lcurve_curvature[peaks])/4]	# for the case there is mainly only one peak and the rest is noise
		# peaks = np.array([y for _, y in sorted(zip(proms, peaks))])
		# proms = np.sort(proms)
		peaks = np.sort(peaks)
		peaks += curvature_range
		# best_index = peaks[max(-len(peaks),-2)]
		best_index = peaks[0]
	else:
		if len(peaks[regolarisation_coeff_range[peaks]<regolarisation_coeff_upper_limit])>0:
			peaks = peaks[regolarisation_coeff_range[peaks]<regolarisation_coeff_upper_limit]
		if len(peaks[regolarisation_coeff_range[peaks]>regolarisation_coeff_lower_limit])>0:
			peaks = peaks[regolarisation_coeff_range[peaks]>regolarisation_coeff_lower_limit]
		best_index = peaks[np.array(Lcurve_curvature[peaks]).argmax()]
		# peaks_all += curvature_range

	# recompose_voxel_emissivity = recompose_voxel_emissivity_all[Lcurve_curvature.argmax()+curvature_range]
	# regolarisation_coeff = regolarisation_coeff_range[Lcurve_curvature.argmax()+curvature_range]
	# x_optimal = x_optimal_all[Lcurve_curvature.argmax()+curvature_range]
	# y_opt = y_opt_all[Lcurve_curvature.argmax()+curvature_range]
	# opt_info = opt_info_all[Lcurve_curvature.argmax()+curvature_range]
	recompose_voxel_emissivity = recompose_voxel_emissivity_all[best_index]
	recompose_voxel_emissivity_excluded = recompose_voxel_emissivity_excluded_all[best_index]
	regolarisation_coeff = regolarisation_coeff_range[best_index]
	x_optimal = x_optimal_all[best_index]
	y_opt = y_opt_all[best_index]
	opt_info = opt_info_all[best_index]

	return score_y,score_x,score_y_record_rel,score_x_record_rel,curvature_range,Lcurve_curvature,recompose_voxel_emissivity,x_optimal,points_removed,regolarisation_coeff,regolarisation_coeff_range,y_opt,opt_info,curvature_range_left_all,curvature_range_right_all,peaks_all,best_index,recompose_voxel_emissivity_excluded

def find_optimal_regularisation_minimal(score_x,score_y,regolarisation_coeff_range,x_optimal_all,curvature_fit_regularisation_interval = 0.05,fraction_of_L_curve_for_fit = 0.08,regolarisation_coeff_upper_limit = 10**-0.2,regolarisation_coeff_lower_limit = 1e-4,forward_model_residuals=False,avoid_score_y_rise=True,mimimum_distance_between_points_vs_line_length=1e4):
	import collections
	from scipy.signal import find_peaks, peak_prominences as get_proms
	from scipy.ndimage import median_filter

	# I need to remove the cases where points are overlapped
	points_removed = []
	counter_score_x = collections.Counter(score_x)
	counter_score_y = collections.Counter(score_y)
	length_of_line = (np.diff(np.log10(score_y))**2 + np.diff(np.log10(score_x))**2)**0.5
	# test = length_of_line<np.median(length_of_line)/1000
	test = np.logical_or((length_of_line<np.median(length_of_line)/1000) , (length_of_line<np.sum(length_of_line)/max(1e5,mimimum_distance_between_points_vs_line_length)))
	while np.sum(test)>0:	# removing points that are too close each other
		i__ = test.argmax()
		# print(i__)
		regolarisation_coeff_range = np.concatenate([regolarisation_coeff_range[:i__],regolarisation_coeff_range[i__+1:]])
		score_x = np.concatenate([score_x[:i__],score_x[i__+1:]])
		score_y = np.concatenate([score_y[:i__],score_y[i__+1:]])
		x_optimal_all = np.concatenate([x_optimal_all[:i__],x_optimal_all[i__+1:]])
		length_of_line = (np.diff(np.log10(score_y))**2 + np.diff(np.log10(score_x))**2)**0.5
		# test = length_of_line<np.median(length_of_line)/1000
		test = np.logical_or((length_of_line<np.median(length_of_line)/1000) , (length_of_line<np.sum(length_of_line)/max(1e5,mimimum_distance_between_points_vs_line_length)))
		points_removed.append(i__)
	test = np.logical_and( [value in np.array(list(counter_score_x.items()))[:,0][np.array(list(counter_score_x.items()))[:,1]>1] for value in score_x] , [value in np.array(list(counter_score_y.items()))[:,0][np.array(list(counter_score_y.items()))[:,1]>1] for value in score_y] )
	while np.sum(test)>0:	# removing points one on top of each other
		i__ = test.argmax()
		# print(i__)
		regolarisation_coeff_range = np.concatenate([regolarisation_coeff_range[:i__],regolarisation_coeff_range[i__+1:]])
		score_x = np.concatenate([score_x[:i__],score_x[i__+1:]])
		score_y = np.concatenate([score_y[:i__],score_y[i__+1:]])
		x_optimal_all = np.concatenate([x_optimal_all[:i__],x_optimal_all[i__+1:]])
		counter_score_x = collections.Counter(score_x)
		counter_score_y = collections.Counter(score_y)
		test = np.logical_and( [value in np.array(list(counter_score_x.items()))[:,0][np.array(list(counter_score_x.items()))[:,1]>1] for value in score_x] , [value in np.array(list(counter_score_y.items()))[:,0][np.array(list(counter_score_y.items()))[:,1]>1] for value in score_y] )
		points_removed.append(i__)
	if forward_model_residuals == False:
		test = np.diff(score_x)
		while np.sum(test<0)>0:	# removing points for which quality of the fit derease for decreasing regularisation. it shouldn't happen, but it actually can for forwards modelled data
			# i__ = test.argmin()+1
			i__ = len(test)-np.flip(test,axis=0).argmin()-1	# this way it should remove right to left
			if i__<5:
				i__ = test.argmin()
			# print(i__)
			regolarisation_coeff_range = np.concatenate([regolarisation_coeff_range[:i__],regolarisation_coeff_range[i__+1:]])
			score_x = np.concatenate([score_x[:i__],score_x[i__+1:]])
			score_y = np.concatenate([score_y[:i__],score_y[i__+1:]])
			x_optimal_all = np.concatenate([x_optimal_all[:i__],x_optimal_all[i__+1:]])
			test = np.diff(score_x)
			points_removed.append(i__)
	if avoid_score_y_rise == True:
		test = np.diff(np.log(score_y))
		while np.sum(test>0)>0:	# removing points for which the laplacian reduces for increasing regularisation (it shouldn't happen). it actually happens sometimes.
			i__ = test.argmax()+1
			# print(i__)
			regolarisation_coeff_range = np.concatenate([regolarisation_coeff_range[:i__],regolarisation_coeff_range[i__+1:]])
			score_x = np.concatenate([score_x[:i__],score_x[i__+1:]])
			score_y = np.concatenate([score_y[:i__],score_y[i__+1:]])
			x_optimal_all = np.concatenate([x_optimal_all[:i__],x_optimal_all[i__+1:]])
			test = np.diff(np.log(score_y))
			points_removed.append(i__)
	length_of_line = (np.diff(np.log10(score_y))**2 + np.diff(np.log10(score_x))**2)**0.5
	# test = length_of_line<np.median(length_of_line)/1000
	test = np.logical_or((length_of_line<np.median(length_of_line)/1000) , (length_of_line<np.sum(length_of_line)/mimimum_distance_between_points_vs_line_length))
	while np.sum(test)>0:	# removing points that are too close each other
		i__ = test.argmax()
		# print(i__)
		regolarisation_coeff_range = np.concatenate([regolarisation_coeff_range[:i__],regolarisation_coeff_range[i__+1:]])
		score_x = np.concatenate([score_x[:i__],score_x[i__+1:]])
		score_y = np.concatenate([score_y[:i__],score_y[i__+1:]])
		x_optimal_all = np.concatenate([x_optimal_all[:i__],x_optimal_all[i__+1:]])
		length_of_line = (np.diff(np.log10(score_y))**2 + np.diff(np.log10(score_x))**2)**0.5
		# test = length_of_line<np.median(length_of_line)/1000
		test = np.logical_or((length_of_line<np.median(length_of_line)/1000) , (length_of_line<np.sum(length_of_line)/mimimum_distance_between_points_vs_line_length))
		points_removed.append(i__)
	points_removed = np.flip(points_removed,axis=0)
	# plt.figure()
	# plt.plot(score_x,score_y)
	# plt.plot(score_x,score_y,'+')
	# plt.xlabel('||Gm-d||2')
	# plt.ylabel('||Laplacian(m)||2')
	# plt.semilogx()
	# plt.semilogy()
	# plt.grid()
	# plt.pause(0.01)

	score_y = np.log(score_y)
	score_x = np.log(score_x)

	score_y_record_rel = (score_y-score_y.min())/(score_y.max()-score_y.min())
	score_x_record_rel = (score_x-score_x.min())/(score_x.max()-score_x.min())

	# # mini smoothing
	# score_x_record_rel = median_filter(score_x_record_rel,size=[3])
	# score_y_record_rel = median_filter(score_y_record_rel,size=[3])

	length_of_line = np.sum((np.diff(score_y_record_rel)**2 + np.diff(score_x_record_rel)**2)**0.5)
	fraction_of_line_as_range = length_of_line*fraction_of_L_curve_for_fit/2*(3+6)/(np.log10(regolarisation_coeff_range).max()-np.log10(regolarisation_coeff_range).min())	# arbitrary
	length_of_line = (np.diff(score_y_record_rel)**2 + np.diff(score_x_record_rel)**2)**0.5
	# if I use a fine regularisation range I need to fit the curvature over more points. this takes care of that.
	# curvature_range was originally = 2
	curvature_range_int = max(1,int(np.ceil(np.abs(-curvature_fit_regularisation_interval*(np.max(np.log10(regolarisation_coeff_range))-np.min(np.log10(regolarisation_coeff_range)))/np.median(np.diff(np.log10(regolarisation_coeff_range)))-1)/2)))
	print('curvature_range_int = '+str(curvature_range_int))


	# plt.figure()
	# plt.plot(regolarisation_coeff_range,score_x_record_rel,label='fit error')
	# plt.plot(regolarisation_coeff_range,score_y_record_rel,label='laplacian error')
	# plt.legend()
	# plt.semilogx()
	# plt.pause(0.01)
	#
	# plt.figure()
	# plt.plot(score_x_record_rel,score_y_record_rel)
	# plt.plot(score_x_record_rel,score_y_record_rel,'+')
	# plt.xlabel('log ||Gm-d||2')
	# plt.ylabel('log ||Laplacian(m)||2')
	# plt.grid()
	# plt.pause(0.01)


	def distance_spread(coord):
		def int(trash,px,py):
			x = coord[0]
			y = coord[1]
			dist = ((x-px)**2 + (y-py)**2)**0.5
			spread = np.sum((dist-np.mean(dist))**2)
			# print(spread)
			return [spread]*5
		return int

	def distance_spread_and_gradient(coord):
		def int(arg):
			# print(arg)
			x = coord[0]
			y = coord[1]
			px = arg[0]
			py = arg[1]
			dist = ((x-px)**2 + (y-py)**2)**0.5
			spread = np.sum((dist-np.mean(dist))**2)
			# print(str([spread,dist]))
			temp = (((x-px)**2 + (y-py)**2)**-0.5)
			derivate = np.array([np.sum(2*(dist-np.mean(dist))*( -0.5*temp*2*(x-px) - np.mean(-0.5*temp*2*(x-px)) )) , np.sum(2*(dist-np.mean(dist))*( -0.5*temp*2*(y-py) - np.mean(-0.5*temp*2*(y-py)) ))])
			return spread,derivate
		return int

	# plt.figure()
	# plt.plot(score_x_record_rel,score_y_record_rel)
	curvature_range = 3
	curvature_radious = []
	curvature_range_left_all = []
	curvature_range_right_all = []
	for ii in range(len(score_y_record_rel)):
		if ii<=curvature_range:
			curvature_radious.append(np.inf)
			continue
		elif ii>=len(score_y_record_rel)-curvature_range:
			curvature_radious.append(np.inf)
			continue

		curvature_range_left = 3
		while np.sum(length_of_line[max(0,ii-curvature_range_left):ii])<fraction_of_line_as_range and ii-curvature_range_left>0:
			curvature_range_left += 1
		curvature_range_right = 3
		while np.sum(length_of_line[ii+1:ii+curvature_range_right+1])<fraction_of_line_as_range and ii+curvature_range_right+1<len(score_y_record_rel):
			curvature_range_right += 1
		# curvature_range_left = 3
		# curvature_range_right = 3
		# while np.sum(length_of_line[max(0,ii-curvature_range_left):ii+curvature_range_right+1])<fraction_of_line_as_range*2:
		# 	curvature_range_left += 1
		# 	curvature_range_right += 1


		curvature_range_left_all.append(curvature_range_left)
		curvature_range_right_all.append(curvature_range_right)
		# print(ii)
		# # print(curvature_range)
		print(curvature_range_left)
		print(curvature_range_right)
		# # try:
		# # 	guess = centre[0]
		# # except:
		# print(ii)
		try:
			border_left=max(0,ii-curvature_range_left)
			border_right=min(ii+curvature_range_right+1,len(score_y_record_rel))
			# bds = [[np.min(score_y_record_rel[ii-2:ii+2+1]),np.min(score_x_record_rel[ii-2:ii+2+1])],[np.inf,np.inf]]
			# centre = curve_fit(distance_spread_and_gradient([score_x_record_rel[ii-2:ii+2+1],score_y_record_rel[ii-2:ii+2+1]]),[0]*5,[0]*5,p0=guess,bounds = bds,maxfev=1e5,gtol=1e-12,verbose=1)
			if forward_model_residuals:
				guess = [np.max(score_x_record_rel[border_left:border_right])*1,np.mean(score_y_record_rel[border_left:border_right])*1]
				bds = [[np.min(score_x_record_rel[border_left:border_right]),np.inf],[-np.inf,np.inf]]
			else:
				guess = np.mean([score_x_record_rel[border_left:border_right]*1,score_y_record_rel[border_left:border_right]*1],axis=1)
				temp=np.polyfit(score_x_record_rel[border_left:border_right],score_y_record_rel[border_left:border_right],1)
				guess += fraction_of_line_as_range*1*np.array([-temp[0],np.abs(1/temp[0])])
				# bds = [[np.min(score_x_record_rel[border_left:border_right]),np.inf],[np.min(score_y_record_rel[border_left:border_right]),np.inf]]
				bds = [[-np.inf,np.inf],[np.min(score_y_record_rel[border_left:border_right]),np.inf]]
			# guess += np.std([score_x_record_rel[border_left:border_right]*1,score_y_record_rel[border_left:border_right]*1],axis=1)
			# bds = [[score_y_record_rel[ii],np.inf],[score_x_record_rel[ii],np.inf]]
			centre, y_opt, opt_info = scipy.optimize.fmin_l_bfgs_b(distance_spread_and_gradient([score_x_record_rel[border_left:border_right],score_y_record_rel[border_left:border_right]]), x0=guess, bounds = bds, iprint=0, factr=1e-4, pgtol=1e-9)#,m=1000, maxls=1000, pgtol=1e-10, factr=1e0)#,approx_grad = True)
			centre = [centre]
			# plt.plot(score_x_record_rel[[border_left,border_right-1]].tolist()+[centre[0][0]]+score_x_record_rel[[border_left,border_right-1]].tolist(),score_y_record_rel[[border_left,border_right-1]].tolist()+[centre[0][1]]+score_y_record_rel[[border_left,border_right-1]].tolist(),'--')#,'o',fillstyle='none')

			dist = ((score_x_record_rel[border_left:border_right]-centre[0][0])**2 + (score_y_record_rel[border_left:border_right]-centre[0][1])**2)**0.5
			radious = np.mean(dist)
			# plt.figure()
			# plt.plot(score_x_record_rel[border_left:border_right],score_y_record_rel[border_left:border_right],'+')
			# plt.plot(score_x_record_rel[ii],score_y_record_rel[ii],'o')
			# plt.plot(centre[0][0],centre[0][1],'o')
			# plt.plot(guess[0],guess[1],'v')
			# plt.plot(np.linspace(centre[0][0]-radious,centre[0][0]+radious),centre[0][1]+(radious**2-np.linspace(-radious,+radious)**2)**0.5)
			# plt.plot(np.linspace(centre[0][0]-radious,centre[0][0]+radious),centre[0][1]-(radious**2-np.linspace(-radious,+radious)**2)**0.5)
			# plt.axhline(y=np.min(score_y_record_rel[border_left:border_right]),linestyle='--')
			# plt.axvline(x=np.min(score_x_record_rel[border_left:border_right]),linestyle='--')
			# plt.pause(0.01)
		except:
			radious = np.inf
		curvature_radious.append(radious)
	# curvature_radious = [np.max(curvature_radious)]+curvature_radious+[np.max(curvature_radious)]
	Lcurve_curvature = 1/np.array(curvature_radious)
	# plt.figure()
	# plt.plot(regolarisation_coeff_range[curvature_range:-curvature_range],Lcurve_curvature)
	# plt.semilogx()
	# plt.pause(0.01)

	peaks_all = find_peaks(Lcurve_curvature)[0]
	peaks = cp.deepcopy(peaks_all)
	proms = get_proms(Lcurve_curvature,peaks)[0]
	if False:	 # a free search doesn't really work, so I force sensible results
		proms = proms[Lcurve_curvature[peaks]>np.max(Lcurve_curvature[peaks])/4]
		peaks = peaks[Lcurve_curvature[peaks]>np.max(Lcurve_curvature[peaks])/4]	# for the case there is mainly only one peak and the rest is noise
		# peaks = np.array([y for _, y in sorted(zip(proms, peaks))])
		# proms = np.sort(proms)
		peaks = np.sort(peaks)
		peaks += curvature_range
		# best_index = peaks[max(-len(peaks),-2)]
		best_index = peaks[0]
	else:
		if len(peaks[regolarisation_coeff_range[peaks]<regolarisation_coeff_upper_limit])>0:
			peaks = peaks[regolarisation_coeff_range[peaks]<regolarisation_coeff_upper_limit]
		if len(peaks[regolarisation_coeff_range[peaks]>regolarisation_coeff_lower_limit])>0:
			peaks = peaks[regolarisation_coeff_range[peaks]>regolarisation_coeff_lower_limit]
		best_index = peaks[np.array(Lcurve_curvature[peaks]).argmax()]
		# peaks_all += curvature_range

	# recompose_voxel_emissivity = recompose_voxel_emissivity_all[Lcurve_curvature.argmax()+curvature_range]
	# regolarisation_coeff = regolarisation_coeff_range[Lcurve_curvature.argmax()+curvature_range]
	# x_optimal = x_optimal_all[Lcurve_curvature.argmax()+curvature_range]
	# y_opt = y_opt_all[Lcurve_curvature.argmax()+curvature_range]
	# opt_info = opt_info_all[Lcurve_curvature.argmax()+curvature_range]
	regolarisation_coeff = regolarisation_coeff_range[best_index]
	x_optimal = x_optimal_all[best_index]

	return score_y,score_x,score_y_record_rel,score_x_record_rel,curvature_range,Lcurve_curvature,x_optimal,points_removed,regolarisation_coeff,regolarisation_coeff_range,curvature_range_left_all,curvature_range_right_all,peaks_all,best_index

def cut_sensitivity_matrix_based_on_foil_anysotropy(sensitivities_binned,ROI1,ROI2,ROI_beams,laser_to_analyse,additional_output=False):
	# additional cropping of the foil to exlude regions without plasma LOS, the frame of the foil and gas puff
	foil_shape = np.shape(sensitivities_binned)[:-1]
	ROI1 = np.round((ROI1.T*foil_shape).T).astype(int)
	ROI2 = np.round((ROI2.T*foil_shape).T).astype(int)
	a,b = np.meshgrid(np.arange(foil_shape[1]),np.arange(foil_shape[0]))
	selected_ROI = np.logical_and(np.logical_and(a>=ROI1[1,0],a<ROI1[1,1]),np.logical_and(b>=sensitivities_binned.shape[0]-ROI1[0,1],b<sensitivities_binned.shape[0]-ROI1[0,0]))
	selected_ROI = np.logical_or(selected_ROI,np.logical_and(np.logical_and(a>=ROI2[1,0],a<ROI2[1,1]),np.logical_and(b>=sensitivities_binned.shape[0]-ROI2[0,1],b<sensitivities_binned.shape[0]-ROI2[0,0])))
	if check_beams_on(laser_to_analyse[-9:-4]):
		if int(laser_to_analyse[-9:-4])< 45514:	# difference between MU01 and MU02
			ROI_beams = np.round((ROI_beams.T*foil_shape).T).astype(int)
			selected_ROI = np.logical_and(selected_ROI,np.logical_not(np.logical_and(np.logical_and(a>=ROI_beams[1,0],a<ROI_beams[1,1]),np.logical_and(b>=sensitivities_binned.shape[0]-ROI_beams[0,1],b<sensitivities_binned.shape[0]-ROI_beams[0,0]))))
		else:
			ROI_beams = ROI_beams.T*foil_shape
			selected_ROI = select_region_with_plasma(sensitivities_binned,selected_ROI,outline_area_with_plasma=ROI_beams,shape_binned=foil_shape,flip_region=True)
	else:
		ROI_beams = np.array([[0,0],[0,0]])

	if True:	# setting zero to the sensitivities I want to exclude
		sensitivities_binned_crop = cp.deepcopy(sensitivities_binned)
		sensitivities_binned_crop[np.logical_not(selected_ROI),:] = 0
	else:	# cutting sensitivity out of ROI
		sensitivities_binned_crop = sensitivities_binned[sensitivities_binned.shape[0]-ROI[0,1]:sensitivities_binned.shape[0]-ROI[0,0],ROI[1,0]:ROI[1,1]]
	if additional_output:
		return sensitivities_binned_crop,selected_ROI,ROI1,ROI2,ROI_beams
	else:
		return sensitivities_binned_crop,selected_ROI

def select_region_with_plasma(sensitivities_binned_crop,selected_ROI,outline_area_with_plasma='MU01',shape_binned=[],flip_region=False):
	# this is created because just using the sensitivity as a metric for selecting the area means that is I have different ROI then this changes
	# this is likely why the inversion of shots with beams, with significantly different ROI, is always weird
	from shapely.geometry import Point
	from shapely.geometry.polygon import Polygon
	if outline_area_with_plasma=='MU01':
		shape_binned = sensitivities_binned_crop.shape[:-1]	# horizontal, vertical
		outline_area_with_plasma = np.array([[-0.05*shape_binned[0],0.45*shape_binned[1]],[0.195*shape_binned[0],0.205*shape_binned[1]],[0.275*shape_binned[0],0.167*shape_binned[1]],[0.36*shape_binned[0],0.154*shape_binned[1]],[1.1*shape_binned[0],0.14*shape_binned[1]],[1.1*shape_binned[0],1.1*shape_binned[1]],[-0.1*shape_binned[0],1.1*shape_binned[1]],[-0.05*shape_binned[0],0.45*shape_binned[1]]])
	elif outline_area_with_plasma=='MU02' or outline_area_with_plasma=='MU03':
		shape_binned = sensitivities_binned_crop.shape[:-1]	# horizontal, vertical
		outline_area_with_plasma = np.array([[-0.05*shape_binned[0],0.138*shape_binned[1]],[0.11*shape_binned[0],0.042*shape_binned[1]],[0.252*shape_binned[0],0.021*shape_binned[1]],[1.1*shape_binned[0],-0.05*shape_binned[1]],[1.1*shape_binned[0],1.1*shape_binned[1]],[-0.1*shape_binned[0],1.1*shape_binned[1]],[-0.05*shape_binned[0],0.138*shape_binned[1]]])
	else:
		pass
		# outline_area_with_plasma = outline_area_with_plasma.T*shape_binned
	select = np.zeros(shape_binned).astype(bool)
	index_flattened = np.arange(len(select.flatten()))
	polygon = Polygon(outline_area_with_plasma)
	for i_e in index_flattened:
		temp = np.unravel_index(i_e,shape_binned)
		if polygon.contains(Point((temp[0]-0.5,temp[1]-0.5))):
			select[temp[0],temp[1]] = True
	select = np.flip(select,axis=0)
	if flip_region:
		select = np.logical_not(select)
	select_foil_region_with_plasma = np.logical_and(select,selected_ROI)
	return select_foil_region_with_plasma

def calculate_tile_geometry():
	# import pyuda as uda
	from mast.geom.geomTileSurfaceUtils import get_s_coords_tables_mastu

	client_int = pyuda.Client()
	try:
		limiter=client_int.geometry("/limiter/efit",50000, no_cal=False)
		limiter_r=limiter.data.R
		limiter_z=limiter.data.Z
		s_lookup=get_s_coords_tables_mastu(limiter_r, limiter_z, ds=1e-4, debug_plot=False)
	except:
		limiter_r = [0]
		limiter_z = [0]
		s_lookup = [0]
	client_int.reset_connection()
	# reset_connection(client_int)
	# del client_int
	return limiter_r,limiter_z,s_lookup

def import_external_time(shot_number):
	# import pyuda as uda

	shot_number = int(shot_number)
	client_int = pyuda.Client()
	try:
		exernal_time = client_int.get('/XPX/CLOCK/IRVB-2', shot_number )
		transition_up = np.diff(exernal_time.data)==exernal_time.data.max()
		exernal_time = exernal_time.time.data[1:][transition_up]
	except:
		exernal_time = [0]
	client_int.reset_connection()
	# reset_connection(client_int)
	# del client_int
	return exernal_time


def calc_temp_to_power_BB_1(photon_flux_over_temperature_interpolator,temperature_minus_background_crop_binned,ref_temperature,time_binned,dx,counts_std_crop_binned,BB_proportional_crop_binned,BB_proportional_std_crop_binned,reference_background_std_crop_binned,temperature_std_crop_binned,nan_ROI_mask,grid_laplacian=[],ref_temperature_std=0,return_grid_laplacian=False):
	zeroC=273.15 #K / C

	# from scipy.sparse import csc_matrix

	dt = time_binned[2:]-time_binned[:-2]
	photon_flux_over_temperature = photon_flux_over_temperature_interpolator(temperature_minus_background_crop_binned+ref_temperature)
	# basetemp=np.nanmean(datatempcrop[0,frame-7:frame+7,1:-1,1:-1],axis=0)
	dT = (temperature_minus_background_crop_binned[2:,1:-1,1:-1]-temperature_minus_background_crop_binned[:-2,1:-1,1:-1]).astype(np.float32)
	dTdt = np.gradient(temperature_minus_background_crop_binned,time_binned,axis=0)[1:-1,1:-1,1:-1].astype(np.float32)	# K/s	# this is still a central difference but it doesn't rely on hand made code
	dTdt_std=np.divide(( ((counts_std_crop_binned[2:,1:-1,1:-1]/(photon_flux_over_temperature[2:,1:-1,1:-1]*BB_proportional_crop_binned[1:-1,1:-1]))**2 + (counts_std_crop_binned[:-2,1:-1,1:-1] /(photon_flux_over_temperature[:-2,1:-1,1:-1]*BB_proportional_crop_binned[1:-1,1:-1]))**2 + (dT*BB_proportional_std_crop_binned[1:-1,1:-1]/BB_proportional_crop_binned[1:-1,1:-1])**2)**0.5 ).T,dt).T.astype(np.float32)
	# 2024/10/01 weird effect noted: when the temperature increases photon_flux_over_temperatureincreases.
	# Given counts_std_crop_binned barely increases with counts, this means that the error DECREASES for INCREASING temperature.
	# When the error is propagated whithout accounting directly for he foil property uncertainty this leads to a very different uncertainty profile
	# if I consider the foil uncertainty I end up with roughly error proportional to signal level
	# if I don't I geto to error DECREASING (not really inversely proportional)  for increasing signal. very beneficial!

	if len(np.shape(grid_laplacian))!=2:
		horizontal_coord = np.arange(np.shape(temperature_minus_background_crop_binned)[2]).astype(np.int16)
		vertical_coord = np.arange(np.shape(temperature_minus_background_crop_binned)[1]).astype(np.int16)
		horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
		grid = np.array([[horizontal_coord.flatten()]*4,[vertical_coord.flatten()]*4]).T
		grid_laplacian = -build_laplacian(grid,diagonal_factor=0.5) / (dx**2) / 2	# the /2 comes from the fact that including the diagonals amounts to double counting, so i do a mean by summing half of it. on the diagonal I have a 1/2**0.5 in crease odistance, but I have to do the square of that distance so it becomes 1/2
	# grid_laplacian = csc_matrix(grid_laplacian)
	d2Tdxy = np.dot(temperature_minus_background_crop_binned.reshape((len(temperature_minus_background_crop_binned),np.shape(grid_laplacian)[0])),grid_laplacian).reshape(np.shape(temperature_minus_background_crop_binned))[1:-1,1:-1,1:-1]
	d2Tdxy_std = (np.dot( ((counts_std_crop_binned/(BB_proportional_crop_binned*photon_flux_over_temperature))**2+(reference_background_std_crop_binned/(BB_proportional_crop_binned*photon_flux_over_temperature_interpolator(ref_temperature)))**2 + (temperature_minus_background_crop_binned*BB_proportional_std_crop_binned/BB_proportional_crop_binned)**2 ).reshape((len(temperature_minus_background_crop_binned),np.shape(grid_laplacian)[0])),grid_laplacian**2)**0.5).reshape(np.shape(temperature_minus_background_crop_binned))[1:-1,1:-1,1:-1]

	# temp = np.ones_like(dTdt).astype(np.float32)*np.nan
	# temp[:,nan_ROI_mask[1:-1,1:-1]]=d2Tdxy[:,nan_ROI_mask[1:-1,1:-1]]
	# d2Tdxy = cp.copy(temp)
	# temp = np.ones_like(dTdt).astype(np.float32)*np.nan
	# temp[:,nan_ROI_mask[1:-1,1:-1]]=d2Tdxy_std[:,nan_ROI_mask[1:-1,1:-1]]
	# d2Tdxy_std = cp.copy(temp)
	negd2Tdxy=np.multiply(-1,d2Tdxy)
	negd2Tdxy_std=d2Tdxy_std

	T4=(temperature_minus_background_crop_binned[1:-1,1:-1,1:-1]+ref_temperature+zeroC)**4
	T4_std=T4**(3/4) *4 *temperature_std_crop_binned[1:-1,1:-1,1:-1]
	T04=(ref_temperature+zeroC)**4 *np.ones_like(temperature_minus_background_crop_binned[1:-1,1:-1,1:-1])
	T04_std=T04**(3/4) *4 *ref_temperature_std
	T4_T04 = (T4-T04).astype(np.float32)
	T4_T04_std = ((T4_std**2+T04_std**2)**0.5).astype(np.float32)
	# T4_T04 = np.ones_like(dTdt).astype(np.float32)*np.nan
	# T4_T04[:,nan_ROI_mask[1:-1,1:-1]] = (T4[:,nan_ROI_mask[1:-1,1:-1]]-T04[:,nan_ROI_mask[1:-1,1:-1]]).astype(np.float32)
	# T4_T04_std = np.ones_like(dTdt).astype(np.float32)*np.nan
	# T4_T04_std[:,nan_ROI_mask[1:-1,1:-1]] = ((T4_std[:,nan_ROI_mask[1:-1,1:-1]]**2+T04_std**2)**0.5).astype(np.float32)

	# temp = np.ones_like(dTdt).astype(np.float32)*np.nan
	# temp[:,nan_ROI_mask[1:-1,1:-1]]=dTdt[:,nan_ROI_mask[1:-1,1:-1]]
	# dTdt = cp.copy(temp)
	# temp = np.ones_like(dTdt).astype(np.float32)*np.nan
	# temp[:,nan_ROI_mask[1:-1,1:-1]]=dTdt_std[:,nan_ROI_mask[1:-1,1:-1]]
	# dTdt_std = cp.copy(temp)

	if return_grid_laplacian:
		return dTdt,dTdt_std,d2Tdxy,d2Tdxy_std,negd2Tdxy,negd2Tdxy_std,T4_T04,T4_T04_std,grid_laplacian
	else:
		return dTdt,dTdt_std,d2Tdxy,d2Tdxy_std,negd2Tdxy,negd2Tdxy_std,T4_T04,T4_T04_std


def calc_temp_to_power_BB_2(dTdt,dTdt_std,d2Tdxy,d2Tdxy_std,negd2Tdxy,negd2Tdxy_std,T4_T04,T4_T04_std,nan_ROI_mask,foilemissivityscaled,foilthicknessscaled,reciprdiffusivityscaled,conductivityscaled,foilemissivityscaled_sigma=0,foilthickness_over_diffusivityscaled_sigma=0,conductivityscaled_sigma=0,thicknessscaled_sigma=0):
	# edit 2023-12-11 this function calculates the ABSORBED power. this is different from the power from the plasma
	sigmaSB=5.6704e-08 #[W/(m2 K4)]

	try:
		len(conductivityscaled)
	except:
		conductivityscaled = conductivityscaled*np.ones_like(T4_T04[0])
	BBrad = np.ones_like(dTdt).astype(np.float32)*np.nan
	BBrad[:,nan_ROI_mask[1:-1,1:-1]] = (2*sigmaSB*((T4_T04 * foilemissivityscaled)[:,nan_ROI_mask[1:-1,1:-1]])).astype(np.float32)	# W/m2
	diffusion = np.ones_like(dTdt).astype(np.float32)*np.nan
	diffusion[:,nan_ROI_mask[1:-1,1:-1]] = ((conductivityscaled*negd2Tdxy*foilthicknessscaled)[:,nan_ROI_mask[1:-1,1:-1]]).astype(np.float32)	# W/m2
	timevariation = np.ones_like(dTdt).astype(np.float32)*np.nan
	timevariation[:,nan_ROI_mask[1:-1,1:-1]] = ((conductivityscaled*dTdt*foilthicknessscaled*reciprdiffusivityscaled)[:,nan_ROI_mask[1:-1,1:-1]]).astype(np.float32)	# W/m2
	powernoback = (diffusion + timevariation + BBrad).astype(np.float32)	# W/m2
	BBrad_std = np.ones_like(dTdt).astype(np.float32)*np.nan
	BBrad_std[:,nan_ROI_mask[1:-1,1:-1]] = (2*sigmaSB*((T4_T04_std)[:,nan_ROI_mask[1:-1,1:-1]])).astype(np.float32)
	# if len(foilemissivityscaled_sigma)==0:
	# 	foilemissivityscaled_sigma = np.zeros_like(T4_T04[0])
	# foilemissivityscaled_sigma = foilemissivityscaled_sigma[nan_ROI_mask[1:-1,1:-1]]
	BBrad_std[:,nan_ROI_mask[1:-1,1:-1]] = ( ((BBrad_std*foilemissivityscaled)[:,nan_ROI_mask[1:-1,1:-1]])**2 + ((BBrad*foilemissivityscaled_sigma)[:,nan_ROI_mask[1:-1,1:-1]])**2 ).astype(np.float32)**0.5
	diffusion_std = np.ones_like(dTdt).astype(np.float32)*np.nan
	diffusion_std[:,nan_ROI_mask[1:-1,1:-1]] = (((negd2Tdxy_std)[:,nan_ROI_mask[1:-1,1:-1]])).astype(np.float32)
	# if len(thicknessscaled_sigma)==0:
	# 	thicknessscaled_sigma = np.zeros_like(T4_T04[0])
	# thicknessscaled_sigma = thicknessscaled_sigma[nan_ROI_mask[1:-1,1:-1]]
	# if len(conductivityscaled_sigma)==0:
	# 	conductivityscaled_sigma = np.zeros_like(T4_T04[0])
	# conductivityscaled_sigma = conductivityscaled_sigma[nan_ROI_mask[1:-1,1:-1]]
	diffusion_std[:,nan_ROI_mask[1:-1,1:-1]] = ( ((diffusion_std*conductivityscaled*foilthicknessscaled)[:,nan_ROI_mask[1:-1,1:-1]])**2 + ((diffusion*conductivityscaled*thicknessscaled_sigma)[:,nan_ROI_mask[1:-1,1:-1]])**2 + ((diffusion_std*conductivityscaled_sigma*foilthicknessscaled)[:,nan_ROI_mask[1:-1,1:-1]])**2 ).astype(np.float32)**0.5
	timevariation_std = np.ones_like(dTdt).astype(np.float32)*np.nan
	timevariation_std[:,nan_ROI_mask[1:-1,1:-1]] = ((dTdt_std)[:,nan_ROI_mask[1:-1,1:-1]]).astype(np.float32)
	# if len(foilthickness_over_diffusivityscaled_sigma)==0:
	# 	foilthickness_over_diffusivityscaled_sigma = np.zeros_like(T4_T04[0])
	# foilthickness_over_diffusivityscaled_sigma = foilthickness_over_diffusivityscaled_sigma[nan_ROI_mask[1:-1,1:-1]]
	timevariation_std[:,nan_ROI_mask[1:-1,1:-1]] = ( ((timevariation_std*conductivityscaled*foilthicknessscaled*reciprdiffusivityscaled)[:,nan_ROI_mask[1:-1,1:-1]])**2 + ((timevariation*conductivityscaled_sigma*foilthicknessscaled*reciprdiffusivityscaled)[:,nan_ROI_mask[1:-1,1:-1]])**2 + ((timevariation*conductivityscaled*foilthickness_over_diffusivityscaled_sigma)[:,nan_ROI_mask[1:-1,1:-1]])**2 ).astype(np.float32)**0.5
	powernoback_std = np.ones_like(powernoback)*np.nan
	powernoback_std[:,nan_ROI_mask[1:-1,1:-1]] = ((diffusion_std[:,nan_ROI_mask[1:-1,1:-1]]**2 + timevariation_std[:,nan_ROI_mask[1:-1,1:-1]]**2 + BBrad_std[:,nan_ROI_mask[1:-1,1:-1]]**2)**0.5).astype(np.float32)
	return BBrad,diffusion,timevariation,powernoback,BBrad_std,diffusion_std,timevariation_std,powernoback_std


def calc_temp_to_power_1(dx,dt,averaged_params,counts_crop_binned,averaged_errparams,shrink_factor_t,shrink_factor_x,temperature_std_crop_binned,temperature_minus_background_crop_binned,laser_int_time,nan_ROI_mask,temperature_ref_crop_binned):
	zeroC=273.15 #K / C

	dTdt=np.divide((temperature_minus_background_crop_binned[2:,1:-1,1:-1]-temperature_minus_background_crop_binned[:-2,1:-1,1:-1]).T,dt).T.astype(np.float32)
	temp = averaged_errparams[2,2] + counts_crop_binned[2:,1:-1,1:-1]*counts_crop_binned[:-2,1:-1,1:-1]*averaged_errparams[1,1] + (counts_crop_binned[2:,1:-1,1:-1]**2)*(counts_crop_binned[:-2,1:-1,1:-1]**2)*averaged_errparams[0,0] + (estimate_counts_std(counts_crop_binned[2:,1:-1,1:-1],int_time=laser_int_time/1000)**2)*(estimate_counts_std(counts_crop_binned[:-2,1:-1,1:-1],int_time=laser_int_time/1000)**2)*(averaged_params[0]**2)/((shrink_factor_t*shrink_factor_x**2)**2) + (counts_crop_binned[2:,1:-1,1:-1]+counts_crop_binned[:-2,1:-1,1:-1])*averaged_errparams[1,2] + (counts_crop_binned[2:,1:-1,1:-1]**2 + counts_crop_binned[:-2,1:-1,1:-1]**2)*averaged_errparams[0,2] + (counts_crop_binned[2:,1:-1,1:-1]*(counts_crop_binned[:-2,1:-1,1:-1]**2)+counts_crop_binned[:-2,1:-1,1:-1]*(counts_crop_binned[2:,1:-1,1:-1]**2))*averaged_errparams[0,1]
	dTdt_std=np.divide((temperature_std_crop_binned[2:,1:-1,1:-1]**2 + temperature_std_crop_binned[:-2,1:-1,1:-1]**2 - 2*temp).T**0.5,dt).T.astype(np.float32)
	d2Tdx2=np.divide(temperature_minus_background_crop_binned[1:-1,1:-1,2:]-np.multiply(2,temperature_minus_background_crop_binned[1:-1,1:-1,1:-1])+temperature_minus_background_crop_binned[1:-1,1:-1,:-2],dx**2).astype(np.float32)
	d2Tdy2=np.divide(temperature_minus_background_crop_binned[1:-1,2:,1:-1]-np.multiply(2,temperature_minus_background_crop_binned[1:-1,1:-1,1:-1])+temperature_minus_background_crop_binned[1:-1,:-2,1:-1],dx**2).astype(np.float32)
	d2Tdxy = np.ones_like(dTdt).astype(np.float32)*np.nan
	d2Tdxy[:,nan_ROI_mask[1:-1,1:-1]]=np.add(d2Tdx2[:,nan_ROI_mask[1:-1,1:-1]],d2Tdy2[:,nan_ROI_mask[1:-1,1:-1]])
	del d2Tdx2,d2Tdy2
	d2Tdx2_std=np.divide((temperature_std_crop_binned[1:-1,1:-1,2:]**2+np.multiply(2**2,temperature_std_crop_binned[1:-1,1:-1,1:-1])**2+temperature_std_crop_binned[1:-1,1:-1,:-2]**2)**0.5,dx**2).astype(np.float32)
	d2Tdy2_std=np.divide((temperature_std_crop_binned[1:-1,2:,1:-1]**2+np.multiply(2**2,temperature_std_crop_binned[1:-1,1:-1,1:-1])**2+temperature_std_crop_binned[1:-1,:-2,1:-1]**2)**0.5,dx**2).astype(np.float32)
	d2Tdxy_std = np.ones_like(dTdt).astype(np.float32)*np.nan
	temp1 = averaged_errparams[2,2] + counts_crop_binned[1:-1,1:-1,2:]*counts_crop_binned[1:-1,1:-1,:-2]*averaged_errparams[1,1] + (counts_crop_binned[1:-1,1:-1,2:]**2)*(counts_crop_binned[1:-1,1:-1,:-2]**2)*averaged_errparams[0,0] + (estimate_counts_std(counts_crop_binned[1:-1,1:-1,2:],int_time=laser_int_time/1000)**2)*(estimate_counts_std(counts_crop_binned[1:-1,1:-1,:-2],int_time=laser_int_time/1000)**2)*(averaged_params[0]**2)/((shrink_factor_t*shrink_factor_x**2)**2) + (counts_crop_binned[1:-1,1:-1,2:]+counts_crop_binned[1:-1,1:-1,:-2])*averaged_errparams[1,2] + (counts_crop_binned[1:-1,1:-1,2:]**2+counts_crop_binned[1:-1,1:-1,:-2]**2)*averaged_errparams[0,2] + (counts_crop_binned[1:-1,1:-1,2:]*(counts_crop_binned[1:-1,1:-1,:-2]**2)+counts_crop_binned[1:-1,1:-1,:-2]*(counts_crop_binned[1:-1,1:-1,2:]**2))*averaged_errparams[0,1]
	temp2 = averaged_errparams[2,2] + counts_crop_binned[1:-1,1:-1,2:]*counts_crop_binned[1:-1,1:-1,1:-1]*averaged_errparams[1,1] + (counts_crop_binned[1:-1,1:-1,2:]**2)*(counts_crop_binned[1:-1,1:-1,1:-1]**2)*averaged_errparams[0,0] + (estimate_counts_std(counts_crop_binned[1:-1,1:-1,2:],int_time=laser_int_time/1000)**2)*(estimate_counts_std(counts_crop_binned[1:-1,1:-1,1:-1],int_time=laser_int_time/1000)**2)*(averaged_params[0]**2)/((shrink_factor_t*shrink_factor_x**2)**2) + (counts_crop_binned[1:-1,1:-1,2:]+counts_crop_binned[1:-1,1:-1,1:-1])*averaged_errparams[1,2] + (counts_crop_binned[1:-1,1:-1,2:]**2+counts_crop_binned[1:-1,1:-1,1:-1]**2)*averaged_errparams[0,2] + (counts_crop_binned[1:-1,1:-1,2:]*(counts_crop_binned[1:-1,1:-1,1:-1]**2)+counts_crop_binned[1:-1,1:-1,1:-1]*(counts_crop_binned[1:-1,1:-1,2:]**2))*averaged_errparams[0,1]
	temp3 = averaged_errparams[2,2] + counts_crop_binned[1:-1,1:-1,1:-1]*counts_crop_binned[1:-1,1:-1,:-2]*averaged_errparams[1,1] + (counts_crop_binned[1:-1,1:-1,1:-1]**2)*(counts_crop_binned[1:-1,1:-1,:-2]**2)*averaged_errparams[0,0] + (estimate_counts_std(counts_crop_binned[1:-1,1:-1,1:-1],int_time=laser_int_time/1000)**2)*(estimate_counts_std(counts_crop_binned[1:-1,1:-1,:-2],int_time=laser_int_time/1000)**2)*(averaged_params[0]**2)/((shrink_factor_t*shrink_factor_x**2)**2) + (counts_crop_binned[1:-1,1:-1,1:-1]+counts_crop_binned[1:-1,1:-1,:-2])*averaged_errparams[1,2] + (counts_crop_binned[1:-1,1:-1,1:-1]**2+counts_crop_binned[1:-1,1:-1,:-2]**2)*averaged_errparams[0,2] + (counts_crop_binned[1:-1,1:-1,1:-1]*(counts_crop_binned[1:-1,1:-1,:-2]**2)+counts_crop_binned[1:-1,1:-1,:-2]*(counts_crop_binned[1:-1,1:-1,1:-1]**2))*averaged_errparams[0,1]
	temp = 2*temp1-4*temp2-4*temp3
	temp1 = averaged_errparams[2,2] + counts_crop_binned[1:-1,2:,1:-1]*counts_crop_binned[1:-1,:-2,1:-1]*averaged_errparams[1,1] + (counts_crop_binned[1:-1,2:,1:-1]**2)*(counts_crop_binned[1:-1,:-2,1:-1]**2)*averaged_errparams[0,0] + (estimate_counts_std(counts_crop_binned[1:-1,2:,1:-1],int_time=laser_int_time/1000)**2)*(estimate_counts_std(counts_crop_binned[1:-1,:-2,1:-1],int_time=laser_int_time/1000)**2)*(averaged_params[0]**2)/((shrink_factor_t*shrink_factor_x**2)**2) + (counts_crop_binned[1:-1,2:,1:-1]+counts_crop_binned[1:-1,:-2,1:-1])*averaged_errparams[1,2] + (counts_crop_binned[1:-1,2:,1:-1]**2+counts_crop_binned[1:-1,:-2,1:-1]**2)*averaged_errparams[0,2] + (counts_crop_binned[1:-1,2:,1:-1]*(counts_crop_binned[1:-1,:-2,1:-1]**2)+counts_crop_binned[1:-1,:-2,1:-1]*(counts_crop_binned[1:-1,2:,1:-1]**2))*averaged_errparams[0,1]
	temp2 = averaged_errparams[2,2] + counts_crop_binned[1:-1,2:,1:-1]*counts_crop_binned[1:-1,1:-1,1:-1]*averaged_errparams[1,1] + (counts_crop_binned[1:-1,2:,1:-1]**2)*(counts_crop_binned[1:-1,1:-1,1:-1]**2)*averaged_errparams[0,0] + (estimate_counts_std(counts_crop_binned[1:-1,2:,1:-1],int_time=laser_int_time/1000)**2)*(estimate_counts_std(counts_crop_binned[1:-1,1:-1,1:-1],int_time=laser_int_time/1000)**2)*(averaged_params[0]**2)/((shrink_factor_t*shrink_factor_x**2)**2) + (counts_crop_binned[1:-1,2:,1:-1]+counts_crop_binned[1:-1,1:-1,1:-1])*averaged_errparams[1,2] + (counts_crop_binned[1:-1,2:,1:-1]**2+counts_crop_binned[1:-1,1:-1,1:-1]**2)*averaged_errparams[0,2] + (counts_crop_binned[1:-1,2:,1:-1]*(counts_crop_binned[1:-1,1:-1,1:-1]**2)+counts_crop_binned[1:-1,1:-1,1:-1]*(counts_crop_binned[1:-1,2:,1:-1]**2))*averaged_errparams[0,1]
	temp3 = averaged_errparams[2,2] + counts_crop_binned[1:-1,1:-1,1:-1]*counts_crop_binned[1:-1,:-2,1:-1]*averaged_errparams[1,1] + (counts_crop_binned[1:-1,1:-1,1:-1]**2)*(counts_crop_binned[1:-1,:-2,1:-1]**2)*averaged_errparams[0,0] + (estimate_counts_std(counts_crop_binned[1:-1,1:-1,1:-1],int_time=laser_int_time/1000)**2)*(estimate_counts_std(counts_crop_binned[1:-1,:-2,1:-1],int_time=laser_int_time/1000)**2)*(averaged_params[0]**2)/((shrink_factor_t*shrink_factor_x**2)**2) + (counts_crop_binned[1:-1,1:-1,1:-1]+counts_crop_binned[1:-1,:-2,1:-1])*averaged_errparams[1,2] + (counts_crop_binned[1:-1,1:-1,1:-1]**2+counts_crop_binned[1:-1,:-2,1:-1]**2)*averaged_errparams[0,2] + (counts_crop_binned[1:-1,1:-1,1:-1]*(counts_crop_binned[1:-1,:-2,1:-1]**2)+counts_crop_binned[1:-1,:-2,1:-1]*(counts_crop_binned[1:-1,1:-1,1:-1]**2))*averaged_errparams[0,1]
	temp += 2*temp1-4*temp2-4*temp3
	d2Tdxy_std[:,nan_ROI_mask[1:-1,1:-1]]=np.add(temp[:,nan_ROI_mask[1:-1,1:-1]]/(dx**4),np.add(d2Tdx2_std[:,nan_ROI_mask[1:-1,1:-1]]**2,d2Tdy2_std[:,nan_ROI_mask[1:-1,1:-1]]**2))**0.5
	del d2Tdx2_std,d2Tdy2_std
	negd2Tdxy=np.multiply(-1,d2Tdxy)
	negd2Tdxy_std=d2Tdxy_std
	T4=(temperature_minus_background_crop_binned[1:-1,1:-1,1:-1]+np.nanmean(temperature_ref_crop_binned)+zeroC)**4
	T04=(np.nanmean(temperature_ref_crop_binned)+zeroC)**4 *np.ones_like(temperature_minus_background_crop_binned[1:-1,1:-1,1:-1])
	T4_T04 = np.ones_like(dTdt).astype(np.float32)*np.nan
	T4_T04[:,nan_ROI_mask[1:-1,1:-1]] = (T4[:,nan_ROI_mask[1:-1,1:-1]]-T04[:,nan_ROI_mask[1:-1,1:-1]]).astype(np.float32)
	T4_std=T4**(3/4) *4 *temperature_std_crop_binned[1:-1,1:-1,1:-1]	# the error resulting from doing the average on the whole ROI is completely negligible
	T04_std=0
	T4_T04_std = np.ones_like(dTdt).astype(np.float32)*np.nan
	T4_T04_std[:,nan_ROI_mask[1:-1,1:-1]] = ((T4_std[:,nan_ROI_mask[1:-1,1:-1]]**2+T04_std**2)**0.5).astype(np.float32)
	return dTdt,dTdt_std,d2Tdxy,d2Tdxy_std,negd2Tdxy,negd2Tdxy_std,T4_T04,T4_T04_std


def rotate_and_crop_2D(data,foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx):
	data_rot=rotate(data,foilrotdeg,axes=(-1,-2))
	if not (height==max_ROI[0][1]+1 and width==max_ROI[1][1]+1):
		data_rot*=out_of_ROI_mask
		data_rot[np.logical_and(data_rot<np.nanmin(data[i]),data_rot>np.nanmax(data[i]))]=0
	return data_rot[foildw:foilup,foillx:foilrx]

def rotate_and_crop_3D(data,foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx):
	data_rot=rotate(data,foilrotdeg,axes=(-1,-2))
	if not (height==max_ROI[0][1]+1 and width==max_ROI[1][1]+1):
		data_rot*=out_of_ROI_mask
		# data_rot[np.logical_and(data_rot<np.nanmin(data[i]),data_rot>np.nanmax(data[i]))]=0
		data_rot[np.isnan(data_rot)]=0
	return data_rot[:,foildw:foilup,foillx:foilrx]

def rotate_and_crop_multi_digitizer(data,dimensions,foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx):
	if dimensions==2:
		out = [rotate_and_crop_2D(data_int,foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx) for data_int in data]
	elif dimensions==3:
		out = [rotate_and_crop_3D(data_int,foilrotdeg,max_ROI,height,width,out_of_ROI_mask,foildw,foilup,foillx,foilrx) for data_int in data]
	else:
		print('ERROR, dimensions must be 2 or 3, not '+str(dimensions))
	return out


def translate_emissivity_profile_with_homo_temp(original_voxels_centre,original_x_optimal,output_voxels_centre,cells_to_exclude=[]):
	output_x_optimal = np.zeros((len(output_voxels_centre)+2))
	output_x_optimal[-2:] = original_x_optimal[-2:]
	if len(cells_to_exclude)==0:
		cells_to_exclude=np.ones_like(original_x_optimal[:-2]).astype(bool)
		no_cells_to_exclude = True
	else:
		cells_to_exclude = np.logical_not(cells_to_exclude)
		no_cells_to_exclude = False

	output_recompose_voxel_emissivity = np.zeros((len(np.unique(output_voxels_centre[:,0])),len(np.unique(output_voxels_centre[:,1]))))*np.nan
	output_recompose_voxel_emissivity_excluded = np.zeros_like(output_recompose_voxel_emissivity)*np.nan
	dr = np.median(np.diff(np.unique(output_voxels_centre[:,0])))
	dz = np.median(np.diff(np.unique(output_voxels_centre[:,1])))
	dist_mean = (dz**2 + dr**2)/2
	for i_r,r in enumerate(np.unique(output_voxels_centre[:,0])):
		for i_z,z in enumerate(np.unique(output_voxels_centre[:,1])):
			dist = (output_voxels_centre[:,0]-r)**2 + (output_voxels_centre[:,1]-z)**2
			if dist.min()<dist_mean/2:
				index = np.abs(dist).argmin()
				original_dist = (original_voxels_centre[:,0]-r)**2 + (original_voxels_centre[:,1]-z)**2
				if original_dist.min()<dist_mean/2:
					original_index = np.abs(original_dist).argmin()
					output_x_optimal[index] = original_x_optimal[original_index]
					output_recompose_voxel_emissivity[i_r,i_z] = output_x_optimal[index]*cells_to_exclude[original_index]
					output_recompose_voxel_emissivity_excluded[i_r,i_z] = output_x_optimal[index]*np.logical_not(cells_to_exclude[original_index])
	output_recompose_voxel_emissivity *= 4*np.pi	# this exist because the sensitivity matrix is built with 1W/str/m^3/ x nm emitters while I use 1W as reference, so I need to multiply the results by 4pi
	output_recompose_voxel_emissivity_excluded *= 4*np.pi	# this exist because the sensitivity matrix is built with 1W/str/m^3/ x nm emitters while I use 1W as reference, so I need to multiply the results by 4pi
	if no_cells_to_exclude:
		return output_x_optimal,output_recompose_voxel_emissivity
	else:
		return output_x_optimal,output_recompose_voxel_emissivity,output_recompose_voxel_emissivity_excluded

def find_temperature_from_power_residuals(dt,grid_laplacian,input_foil_power,temperature_full_resolution_initial,ref_temperature=25,thickness=2.0531473351462095e-06,emissivity=0.9999999999999,diffusivity=1.0283685197530968e-05,Ptthermalconductivity=71.6):
	zeroC=273.15 #K / C
	sigmaSB=5.6704e-08 #[W/(m2 K4)]
	# # from 2021/09/17, Laser_data_analysis3_3.py
	# thickness = 2.0531473351462095e-06
	# emissivity = 0.9999999999999
	# diffusivity = 1.0283685197530968e-05
	T04=(ref_temperature+zeroC)**4
	temperature_full_resolution = np.zeros(np.array(np.shape(input_foil_power))+2)
	def residual(temp):
		temperature_full_resolution[1:-1,1:-1] = temp.reshape(temperature_full_resolution[1:-1,1:-1].shape)
		dTdt = (temperature_full_resolution-temperature_full_resolution_initial)/dt	# the full central difference gradient requires a global fit that takes forever, so I do this as mu
		# dTdt_derivate = np.ones_like(temperature_full_resolution)/dt
		d2Tdxy = np.dot(temperature_full_resolution.flatten(),grid_laplacian).reshape(np.shape(temperature_full_resolution))
		# negd2Tdxy=np.multiply(-1,d2Tdxy)
		# negd2Tdxy_derivate=np.ones_like(temperature_full_resolution)#np.multiply(-1,np.dot(np.ones_like(temperature_full_resolution).flatten(),grid_laplacian).reshape(np.shape(temperature_full_resolution)))
		T4=(temperature_full_resolution+ref_temperature+zeroC)**4
		T4_T04 = (T4-T04)
		T4_T04_derivate = 4*(temperature_full_resolution+ref_temperature+zeroC)**3

		BBrad = (2*sigmaSB*T4_T04 * emissivity)
		BBrad_derivate = (2*sigmaSB*T4_T04_derivate * emissivity)
		diffusion = -(Ptthermalconductivity*d2Tdxy*thickness)
		# diffusion_derivate = (Ptthermalconductivity*negd2Tdxy_derivate*thickn/ess)
		timevariation = (Ptthermalconductivity*dTdt*thickness/diffusivity)
		# timevariation_derivate = (Ptthermalconductivity*dTdt_derivate*thickness/diffusivity)
		timevariation_derivate = np.ones_like(temperature_full_resolution)*(Ptthermalconductivity*thickness/diffusivity/dt)
		powernoback = (diffusion + timevariation + BBrad)
		# powernoback_derivate = (diffusion_derivate + timevariation_derivate + BBrad_derivate)
		powernoback[1:-1,1:-1]-=input_foil_power
		powernoback[0]=0
		powernoback[-1]=0
		powernoback[:,0]=0
		powernoback[:,-1]=0
		gradient = 2*powernoback * (BBrad_derivate + timevariation_derivate) - 2*Ptthermalconductivity*thickness*np.dot(powernoback.flatten(),grid_laplacian).reshape(np.shape(temperature_full_resolution))

		return np.sum(powernoback[1:-1,1:-1]**2),gradient[1:-1,1:-1].flatten()
	return residual


#######################################################################################################################################################################################################################################

def movie_from_data_radial_profile(data,framerate,integration=1,xlabel=(),ylabel=(),barlabel=(),cmap='rainbow',form_factor_size=15,extent = [], image_extent = [],timesteps='auto',extvmin='auto',extvmax='auto',time_offset=0,prelude='',vline=None,hline=None,EFIT_path=EFIT_path_default,include_EFIT=False,efit_reconstruction=None,EFIT_output_requested = False,pulse_ID=None,overlay_x_point=False,overlay_mag_axis=False,overlay_structure=False,overlay_strike_points=False,overlay_separatrix=False,overlay_res_bolo=False,structure_alpha=0.5,foil_size=foil_size,additional_points_dict = dict([]), additional_each_frame_label_description=[], additional_each_frame_label_number=[],x_markersize=30,x_linewidth=3):
	import matplotlib.animation as animation
	import numpy as np

	if len(extent) == 0 or len(image_extent) == 0:
		print('ERROR. for coleval.movie_from_data_radial_profile you must supply an extent and image_extent of shape=1')
		# exit()

	form_factor = (image_extent[1]-image_extent[0])/(image_extent[3]-image_extent[2])
	# form_factor = (np.shape(data[0][0])[1])/(np.shape(data[0][0])[0])
	fig = plt.figure(figsize=(form_factor_size*form_factor*1.2, form_factor_size*1.2))
	ax = fig.add_subplot(111)

	data_rotated = np.array([np.flip(data[0],axis=2)])

	# I like to position my colorbars this way, but you don't have to
	# div = make_axes_locatable(ax)
	# cax = div.append_axes('right', '5%', '5%')

	# def f(x, y):
	#	 return np.exp(x) + np.sin(y)

	# x = np.linspace(0, 1, 120)
	# y = np.linspace(0, 2 * np.pi, 100).reshape(-1, 1)

	if len(image_extent)==4:
		ver = np.linspace(extent[2],extent[3],num=np.shape(data_rotated[0][0])[0]+1)
		up = np.abs(image_extent[3]-ver).argmin()-1
		down = np.abs(image_extent[2]-ver).argmin()
		hor = np.linspace(extent[0],extent[1],num=np.shape(data_rotated[0][0])[1]+1)
		left = np.abs(image_extent[0]-hor).argmin()
		right = np.abs(image_extent[1]-hor).argmin()-1
		data_rotated[0][:,:,:left] = np.nan
		data_rotated[0][:,:,right+1:] = np.nan
		data_rotated[0][:,:down] = np.nan
		data_rotated[0][:,up+1:] = np.nan

	# This is now a list of arrays rather than a list of artists
	frames = [None]*len(data_rotated[0])
	frames[0]=data_rotated[0,0]

	for i in range(len(data_rotated[0])):
		# x	   += 1
		# curVals  = f(x, y)
		frames[i]=(data_rotated[0,i])

	cv0 = frames[0]
	im = ax.imshow(cv0,cmap, origin='lower', interpolation='none', extent = extent)# [0,np.shape(data)[0]-1,0,np.shape(data)[1]-1]) # Here make an AxesImage rather than contour
	ax.set_ylim(top=image_extent[3],bottom=image_extent[2])
	ax.set_xlim(right=image_extent[1],left=image_extent[0])

	if include_EFIT:
		try:
			if efit_reconstruction == None:
				print('reading '+EFIT_path+'/epm0'+str(pulse_ID)+'.nc')
				efit_reconstruction = mclass(EFIT_path+'/epm0'+str(pulse_ID)+'.nc',pulse_ID=pulse_ID)
			else:
				print('EFIT reconstruction externally supplied')
			EFIT_dt = np.median(np.diff(efit_reconstruction.time))
		except Exception as e:
			print('reading '+EFIT_path+'/epm0'+str(pulse_ID)+'.nc failed')
			logging.exception('with error: ' + str(e))
			include_EFIT=False
			overlay_x_point=False
			overlay_mag_axis=False
			overlay_separatrix=False
			overlay_strike_points=False
			overlay_separatrix=False
			efit_reconstruction = None
		if overlay_x_point:
			all_time_x_point_location = np.array([efit_reconstruction.lower_xpoint_r,efit_reconstruction.lower_xpoint_z]).T
			plot1 = ax.plot(0,0,'+r',markersize=x_markersize, alpha=1)[0]
		if overlay_mag_axis:
			all_time_mag_axis_location = np.array([efit_reconstruction.mag_axis_r,efit_reconstruction.mag_axis_z]).T
			plot2 = ax.plot(0,0,'+r',markersize=x_markersize, alpha=1)[0]
		if overlay_separatrix or overlay_strike_points:
			all_time_sep_r,all_time_sep_z,r_fine,z_fine = efit_reconstruction_to_separatrix_on_foil(efit_reconstruction)
		if overlay_strike_points:
			all_time_strike_points_location = return_all_time_strike_points_location_radial(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine)
			plot3 = ax.plot(0,0,'xr',markersize=x_markersize, alpha=1)[0]
		if overlay_separatrix:
			all_time_separatrix = return_all_time_separatrix_radial(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine)
			plot5 = []
			for __i in range(len(all_time_separatrix[0])):
				plot5.append(ax.plot(0,0,'--b', alpha=1)[0])
	if overlay_structure:
		for i in range(len(structure_radial_profile)):
			ax.plot(structure_radial_profile[i][:,0],structure_radial_profile[i][:,1],'--k',alpha=structure_alpha)
	if overlay_res_bolo:
		for i in range(len(res_bolo_radial_profile)):
			ax.plot(res_bolo_radial_profile[i][:,0],res_bolo_radial_profile[i][:,1],'--r',alpha=1)
	if len(list(additional_points_dict.keys()))!=0:
		plot6 = []
		for __i in range(additional_points_dict['number_of_points']):
			plot6.append(ax.plot(0,0,additional_points_dict['marker'][__i], alpha=1,markersize=x_markersize,linewidth=x_linewidth)[0])

	# if len(np.shape(mask)) == 2:
	# im = ax.imshow(mask,'gray',interpolation='none',alpha=1)
	if np.sum(vline == None)==0:
		if np.shape(vline)==():
			vline = max(0,min(vline,np.shape(cv0)[1]-1))
			axvline = ax.axvline(x=vline,linestyle='--',color='k')
		else:
			for i in range(len(vline)):
				vline[i] = max(0,min(vline[i],np.shape(cv0)[1]-1))
				axvline = ax.axvline(x=vline[i],linestyle='--',color='k')
	if np.sum(hline == None)==0:
		if np.shape(hline)==():
			hline = max(0,min(hline,np.shape(cv0)[0]-1))
			axhline = ax.axhline(y=hline,linestyle='--',color='k')
		else:
			for i in range(len(hline)):
				hline[i] = max(0,min(hline[i],np.shape(cv0)[0]-1))
				axhline = ax.axhline(y=hline[i],linestyle='--',color='k')

	cb = fig.colorbar(im,fraction=0.0577, pad=0.04).set_label(barlabel)
	cb = ax.set_xlabel(xlabel)
	cb = ax.set_ylabel(ylabel)
	tx = ax.set_title('Frame 0')


	# if timesteps=='auto':
	# 	timesteps_int = time_offset+np.arange(len(data[0])+1)/framerate
	# else:
	# 	timesteps_int = cp.deepcopy(timesteps)
	def animate(i):
		arr = frames[i]
		if extvmax=='auto':
			vmax = np.nanmax(arr)
		elif extvmax=='allmax':
			vmax = np.nanmax(data_rotated)
		elif hasattr(extvmax, "__len__"):
			vmax = extvmax[i]
		else:
			vmax = extvmax

		if extvmin=='auto':
			vmin = np.nanmin(arr)
		elif extvmin=='allmin':
			vmin = np.nanmin(data_rotated)
		elif hasattr(extvmin, "__len__"):
			vmin = extvmin[i]
		else:
			vmin = extvmin

		im.set_data(arr)
		im.set_clim(vmin, vmax)
		if timesteps=='auto':
			time_int = time_offset+i/framerate
			each_frame_label = prelude + 'Frame {0}'.format(i)+', FR %.3gHz, t %.3gs, int %.3gms' %(framerate,time_int,integration)
		else:
			time_int = timesteps[i]
			each_frame_label = prelude + 'Frame {0}'.format(i)+', t %.3gs, int %.3gms' %(time_int,integration)
		if len(additional_each_frame_label_description) != 0:
			additional_each_frame_label = '\n' + additional_each_frame_label_description[i]
		else:
			additional_each_frame_label = '\n'
		if len(additional_each_frame_label_number) != 0:
			additional_each_frame_label = additional_each_frame_label + ' %.5g' %(additional_each_frame_label_number[i])
		each_frame_label = each_frame_label + additional_each_frame_label
		tx.set_text(each_frame_label)
		if include_EFIT:
			if np.min(np.abs(time_int-efit_reconstruction.time))>EFIT_dt:	# means that the reconstruction is not available for that time
				if overlay_x_point:
					plot1.set_data(([],[]))
				if overlay_mag_axis:
					plot2.set_data(([],[]))
				if overlay_strike_points:
					plot3.set_data(([],[]))
				if overlay_separatrix:
					for __i in range(len(plot5)):
						plot5[__i].set_data(([],[]))
			else:
				i_time = np.abs(time_int-efit_reconstruction.time).argmin()
				if overlay_x_point:
					if np.sum(np.isnan(all_time_x_point_location[i_time]))>=len(all_time_x_point_location[i_time]):	# means that all the points calculated are outside the foil
						plot1.set_data(([],[]))
					else:
						plot1.set_data((all_time_x_point_location[i_time][0],all_time_x_point_location[i_time][1]))
				if overlay_mag_axis:
					# if np.sum(np.isnan(all_time_mag_axis_location[i_time]))>=len(all_time_mag_axis_location[i_time]):	# means that all the points calculated are outside the foil
					# 	plot2.set_data(([],[]))
					# else:
					plot2.set_data((all_time_mag_axis_location[i_time][0],all_time_mag_axis_location[i_time][1]))
				if overlay_strike_points:
					# if np.sum(np.isnan(all_time_mag_axis_location[i_time]))>=len(all_time_mag_axis_location[i_time]):	# means that all the points calculated are outside the foil
					# 	plot3.set_data(([],[]))
					# 	for __i in range(len(plot4)):
					# 		plot4[__i].set_data(([],[]))
					# else:
					plot3.set_data((all_time_strike_points_location[i_time][0],all_time_strike_points_location[i_time][1]))
				if overlay_separatrix:
					# if np.sum(np.isnan(all_time_mag_axis_location[i_time]))>=len(all_time_mag_axis_location[i_time]):	# means that all the points calculated are outside the foil
					for __i in range(len(plot5)):
						plot5[__i].set_data((all_time_separatrix[i_time][__i][0],all_time_separatrix[i_time][__i][1]))
		if len(list(additional_points_dict.keys()))!=0:
			i_time2 = np.abs(time_int-additional_points_dict['time']).argmin()
			# if np.sum(np.isnan(all_time_mag_axis_location[i_time]))>=len(all_time_mag_axis_location[i_time]):	# means that all the points calculated are outside the foil
			for __i in range(additional_points_dict['number_of_points']):
				plot6[__i].set_data((additional_points_dict[str(__i)][i_time2][0],additional_points_dict[str(__i)][i_time2][1]))
			# 	masked = np.ma.masked_where(mask == 0, mask)
			# 	ax.imshow(masked, 'gray', interpolation='none', alpha=0.2,origin='lower',extent = [0,np.shape(data)[0]-1,0,np.shape(data)[1]-1])

			# In this version you don't have to do anything to the colorbar,
			# it updates itself when the mappable it watches (im) changes
	# else:
	# 	def animate(i):
	# 		arr = frames[i]
	# 		if extvmax=='auto':
	# 			vmax = np.max(arr)
	# 		elif extvmax=='allmax':
	# 			vmax = np.max(data)
	# 		else:
	# 			vmax = extvmax
	#
	# 		if extvmin=='auto':
	# 			vmin = np.min(arr)
	# 		elif extvmin=='allmin':
	# 			vmin = np.min(data)
	# 		else:
	# 			vmin = extvmin
	# 		im.set_data(arr)
	# 		im.set_clim(vmin, vmax)
	# 		tx.set_text(prelude + 'Frame {0}'.format(i)+', t %.3gs, int %.3gms' %(timesteps[i],integration))
	# 		# if len(np.shape(mask)) == 2:
	# 		# 	# 	im.imshow(mask,'binary',interpolation='none',alpha=0.3)
	# 		# 	masked = np.ma.masked_where(mask == 0, mask)
	# 		# 	ax.imshow(masked, 'gray', interpolation='none', alpha=0.2,origin='lower',extent = [0,np.shape(data)[0]-1,0,np.shape(data)[1]-1])

	ani = animation.FuncAnimation(fig, animate, frames=len(data[0]))

	if EFIT_output_requested == False:
		return ani
	else:
		return ani,efit_reconstruction

def image_from_data_radial_profile(data,xlabel=(),ylabel=(),barlabel=(),cmap='rainbow',form_factor_size=15,extent = [], image_extent = [],ref_time=None,extvmin='auto',extvmax='auto',prelude='',vline=None,hline=None,EFIT_path=EFIT_path_default,include_EFIT=False,efit_reconstruction=None,EFIT_output_requested = False,pulse_ID=None,overlay_x_point=False,overlay_mag_axis=False,overlay_structure=False,overlay_strike_points=False,overlay_separatrix=False,overlay_res_bolo=False,structure_alpha=0.5,foil_size=foil_size,additional_points_dict = dict([])):
	import numpy as np
	from matplotlib import cm	# to print nan as white

	if len(extent) == 0 or len(image_extent) == 0:
		print('ERROR. for coleval.image_from_data_radial_profile you must supply an extent and image_extent of shape=1')
		# exit()

	form_factor = (image_extent[1]-image_extent[0])/(image_extent[3]-image_extent[2])
	fig = plt.figure(figsize=(form_factor_size*form_factor, form_factor_size*1.2))
	ax = fig.add_subplot(111)

	data_rotated = np.array([np.flip(data[0],axis=2)])

	# I like to position my colorbars this way, but you don't have to
	# div = make_axes_locatable(ax)
	# cax = div.append_axes('right', '5%', '5%')

	# def f(x, y):
	#	 return np.exp(x) + np.sin(y)

	# x = np.linspace(0, 1, 120)
	# y = np.linspace(0, 2 * np.pi, 100).reshape(-1, 1)

	if len(image_extent)==4:
		ver = np.linspace(extent[2],extent[3],num=np.shape(data_rotated[0][0])[0]+1)
		up = np.abs(image_extent[3]-ver).argmin()-1
		down = np.abs(image_extent[2]-ver).argmin()
		hor = np.linspace(extent[0],extent[1],num=np.shape(data_rotated[0][0])[1]+1)
		left = np.abs(image_extent[0]-hor).argmin()
		right = np.abs(image_extent[1]-hor).argmin()-1
		data_rotated[0][:,:,:left] = np.nan
		data_rotated[0][:,:,right+1:] = np.nan
		data_rotated[0][:,:down] = np.nan
		data_rotated[0][:,up+1:] = np.nan

	# This is now a list of arrays rather than a list of artists
	frames = [None]*len(data_rotated[0])
	frames[0]=data_rotated[0,0]

	for i in range(len(data_rotated[0])):
		# x	   += 1
		# curVals  = f(x, y)
		frames[i]=(data_rotated[0,i])

	cv0 = frames[0]
	masked_array = np.ma.array (cv0, mask=np.isnan(cv0))
	# exec('cmap=cm.' + cmap)
	# cmap=cm.rainbow
	cmap=eval('cm.'+cmap)
	cmap.set_bad('white',1.)
	im = ax.imshow(masked_array,cmap=cmap, origin='lower', interpolation='none', extent = extent)# [0,np.shape(data)[0]-1,0,np.shape(data)[1]-1]) # Here make an AxesImage rather than contour
	ax.set_ylim(top=image_extent[3],bottom=image_extent[2])
	ax.set_xlim(right=image_extent[1],left=image_extent[0])

	if ref_time==None:
		include_EFIT = False

	if include_EFIT:
		try:
			if efit_reconstruction == None:
				print('reading '+EFIT_path+'/epm0'+str(pulse_ID)+'.nc')
				efit_reconstruction = mclass(EFIT_path+'/epm0'+str(pulse_ID)+'.nc',pulse_ID=pulse_ID)
			else:
				print('EFIT reconstruction externally supplied')
			EFIT_dt = np.median(np.diff(efit_reconstruction.time))
		except Exception as e:
			print('reading '+EFIT_path+'/epm0'+str(pulse_ID)+'.nc failed')
			logging.exception('with error: ' + str(e))
			include_EFIT=False
			overlay_x_point=False
			overlay_mag_axis=False
			overlay_separatrix=False
			overlay_strike_points=False
			overlay_separatrix=False
			efit_reconstruction = None
		if overlay_x_point:
			all_time_x_point_location = np.array([efit_reconstruction.lower_xpoint_r,efit_reconstruction.lower_xpoint_z]).T
			plot1 = ax.plot(0,0,'+r', alpha=1,markersize=20)[0]
		if overlay_mag_axis:
			all_time_mag_axis_location = np.array([efit_reconstruction.mag_axis_r,efit_reconstruction.mag_axis_z]).T
			plot2 = ax.plot(0,0,'+r', alpha=1,markersize=20)[0]
		if overlay_separatrix or overlay_strike_points:
			all_time_sep_r,all_time_sep_z,r_fine,z_fine = efit_reconstruction_to_separatrix_on_foil(efit_reconstruction)
		if overlay_strike_points:
			all_time_strike_points_location = return_all_time_strike_points_location_radial(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine)
			plot3 = ax.plot(0,0,'xr',markersize=20, alpha=1)[0]
		if overlay_separatrix:
			all_time_separatrix = return_all_time_separatrix_radial(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine)
			plot5 = []
			for __i in range(len(all_time_separatrix[0])):
				plot5.append(ax.plot(0,0,'--b', alpha=1)[0])
	if overlay_structure:
		for i in range(len(structure_radial_profile)):
			ax.plot(structure_radial_profile[i][:,0],structure_radial_profile[i][:,1],'--k',alpha=structure_alpha)
	if overlay_res_bolo:
		for i in range(len(res_bolo_radial_profile)):
			ax.plot(res_bolo_radial_profile[i][:,0],res_bolo_radial_profile[i][:,1],'--r',alpha=1)
	if len(list(additional_points_dict.keys()))!=0:
		plot6 = []
		for __i in range(additional_points_dict['number_of_points']):
			plot6.append(ax.plot(0,0,additional_points_dict['marker'][__i], alpha=1,markersize=20)[0])

	# if len(np.shape(mask)) == 2:
	# im = ax.imshow(mask,'gray',interpolation='none',alpha=1)
	if np.sum(vline == None)==0:
		if np.shape(vline)==():
			vline = max(0,min(vline,np.shape(cv0)[1]-1))
			axvline = ax.axvline(x=vline,linestyle='--',color='k')
		else:
			for i in range(len(vline)):
				vline[i] = max(0,min(vline[i],np.shape(cv0)[1]-1))
				axvline = ax.axvline(x=vline[i],linestyle='--',color='k')
	if np.sum(hline == None)==0:
		if np.shape(hline)==():
			hline = max(0,min(hline,np.shape(cv0)[0]-1))
			axhline = ax.axhline(y=hline,linestyle='--',color='k')
		else:
			for i in range(len(hline)):
				hline[i] = max(0,min(hline[i],np.shape(cv0)[0]-1))
				axhline = ax.axhline(y=hline[i],linestyle='--',color='k')

	cb = fig.colorbar(im,fraction=0.0577, pad=0.04).set_label(barlabel)
	cb = ax.set_xlabel(xlabel)
	cb = ax.set_ylabel(ylabel)
	tx = ax.set_title('Frame 0')


	arr = frames[0]
	if extvmax=='auto':
		vmax = np.nanmax(arr)
	elif extvmax=='allmax':
		vmax = np.nanmax(data_rotated)
	else:
		vmax = extvmax

	if extvmin=='auto':
		vmin = np.nanmin(arr)
	elif extvmin=='allmin':
		vmin = np.nanmin(data_rotated)
	else:
		vmin = extvmin
	masked_array = np.ma.array (arr, mask=np.isnan(arr))
	im.set_data(masked_array)
	im.set_clim(vmin, vmax)
	tx.set_text(prelude)
	if include_EFIT:
		if np.min(np.abs(ref_time-efit_reconstruction.time))>EFIT_dt:	# means that the reconstruction is not available for that time
			if overlay_x_point:
				plot1.set_data(([],[]))
			if overlay_mag_axis:
				plot2.set_data(([],[]))
			if overlay_strike_points:
				plot3.set_data(([],[]))
			if overlay_separatrix:
				for __i in range(len(plot5)):
					plot5[__i].set_data(([],[]))
		else:
			i_time = np.abs(ref_time-efit_reconstruction.time).argmin()
			if overlay_x_point:
				# if np.sum(np.isnan(all_time_x_point_location[i_time]))>=len(all_time_x_point_location[i_time]):	# means that all the points calculated are outside the foil
				# 	plot1.set_data(([],[]))
				# else:
				plot1.set_data((all_time_x_point_location[i_time][0],all_time_x_point_location[i_time][1]))
			if overlay_mag_axis:
				# if np.sum(np.isnan(all_time_mag_axis_location[i_time]))>=len(all_time_mag_axis_location[i_time]):	# means that all the points calculated are outside the foil
				# 	plot2.set_data(([],[]))
				# else:
				plot2.set_data((all_time_mag_axis_location[i_time][0],all_time_mag_axis_location[i_time][1]))
			if overlay_strike_points:
				# if np.sum(np.isnan(all_time_mag_axis_location[i_time]))>=len(all_time_mag_axis_location[i_time]):	# means that all the points calculated are outside the foil
				# 	plot3.set_data(([],[]))
				# 	for __i in range(len(plot4)):
				# 		plot4[__i].set_data(([],[]))
				# else:
				plot3.set_data((all_time_strike_points_location[i_time][0],all_time_strike_points_location[i_time][1]))
			if overlay_separatrix:
				# if np.sum(np.isnan(all_time_mag_axis_location[i_time]))>=len(all_time_mag_axis_location[i_time]):	# means that all the points calculated are outside the foil
				for __i in range(len(plot5)):
					plot5[__i].set_data((all_time_separatrix[i_time][__i][0],all_time_separatrix[i_time][__i][1]))
			if len(list(additional_points_dict.keys()))!=0:
				i_time2 = np.abs(ref_time-additional_points_dict['time']).argmin()
				# if np.sum(np.isnan(all_time_mag_axis_location[i_time]))>=len(all_time_mag_axis_location[i_time]):	# means that all the points calculated are outside the foil
				for __i in range(additional_points_dict['number_of_points']):
					plot6[__i].set_data((additional_points_dict[str(__i)][i_time2][0],additional_points_dict[str(__i)][i_time2][1]))

	if EFIT_output_requested == False:
		return fig
	else:
		return fig,efit_reconstruction

##############################################################################################################################################################################################################

def estimate_counts_std(counts,int_time=2,framerate=383):
	# if framerate<994+3:
	if np.abs(int_time-2)<0.001:
		# counts_std_fit = np.array([6.96431538e-20, -3.26767625e-15,  5.84235577e-11, -4.97550821e-07, 2.18103951e-03])	# found 2021-10-06 by looking at 383Hz 1ms and 2ms data. it is likely not correct for 0.5ms integration time
		counts_std_fit = np.array([-5.32486228e-31,  3.53268894e-26, -9.85692764e-22,  1.50423830e-17,-1.36726859e-13,  7.54763409e-10, -2.46270434e-06,  4.48742195e-03])	# found 2021-10-06 with generate_count_std_VS_count_coeff.py
		print('noise estimation 2ms int time')
	# else:
	elif np.abs(int_time-1)<0.001:
		# counts_std_fit = np.array([-5.32486228e-31,  3.53268894e-26, -9.85692764e-22,  1.50423830e-17,-1.36726859e-13,  7.54763409e-10, -2.46270434e-06,  4.48742195e-03])/2	# only copied from the 2ms case and dividewd by 2
		counts_std_fit = np.array([8.93967567e-29, -4.40146876e-24,  9.21437103e-20, -1.06261647e-15, 7.28477508e-12, -2.96490865e-08,  6.61137425e-05, -6.11776345e-02])	# found 2022-12-07 with generate_count_std_VS_count_coeff.py
		print('noise estimation 1ms int time')
	elif np.abs(int_time-0.5)<0.001:
		counts_std_fit = np.array([1.50373493e-27, -4.19101786e-23,  4.93707276e-19, -3.17906562e-15,  1.20397117e-11, -2.66143902e-08,  3.10847589e-05, -1.27980432e-02])	# found 2022-12-07 with generate_count_std_VS_count_coeff.py
		print('noise estimation 0.5ms int time')
	else:
		print(str(framerate)+'Hz framerate does not have std coefficients assotiated with it, this should be checked')
		a=b	# a and b are not defined so this will cause an error to occour
	counts_temp_std = np.polyval(counts_std_fit,counts).astype(np.float32)*counts
	return counts_temp_std

##############################################################################################################################################################################################################

def get_tend(shot_id):
	# import pyuda as uda

	client_int = pyuda.Client()
	try:
		from pycpf import pycpf
		tend = pycpf.query(['tend'], filters=['exp_number = '+shot_id])['tend'][0]
	except:
		try:
			signal_name = '/AMC/Plasma_current'
			temp = client_int.get(signal_name,shot_id)
			Plasma_current_time = temp.time.data
			Plasma_current = temp.data
			tend = Plasma_current_time[np.logical_and(Plasma_current_time>0.1,Plasma_current<20).argmax()]	# the 20 (kA) treshold comes from Jimmy Measures
		except:
			tend = 1	# arbitrary time
	client_int.reset_connection()
	# reset_connection(client_int)
	# del client_int
	return tend

def check_beams_on(shot_id,timefirst=0.2,time_last=False,path='/home/ffederic/work/irvb/MAST-U/'+'shot_list2.ods'):
	# timefirst=0.2 is just an arbitrary time to avoid to consider shots where the beams failed early on
	beams_on_flag = False

	try:
		# shot_list = get_data(path)
		shot_list = load_shot_list(path)
		temp1 = (np.array(shot_list['Sheet1'][0])=='shot number').argmax()
		shot_found = False
		for i in range(1,len(shot_list['Sheet1'])):
			if shot_list['Sheet1'][i][temp1] == int(shot_id):
				shot_found = True
				if shot_list['Sheet1'][i][(np.array(shot_list['Sheet1'][0])=='SW beam').argmax()] in ['x','X','Y','N']:
					if shot_list['Sheet1'][i][(np.array(shot_list['Sheet1'][0])=='SW beam').argmax()] in ['x','X','Y']:
						beams_on_flag = True
				if shot_list['Sheet1'][i][(np.array(shot_list['Sheet1'][0])=='SS beam').argmax()] in ['x','X','Y','N']:
					if shot_list['Sheet1'][i][(np.array(shot_list['Sheet1'][0])=='SS beam').argmax()] in ['x','X','Y']:
						beams_on_flag = True
				break
		if not shot_found:
			sba=gnu	# shot not found in the shot list file, I want this to generate an error
	except:
		# import pyuda as uda
		client_int = pyuda.Client()
		try:
			signal_name = '/XNB/SS/BEAMPOWER'
			data = client_int.get(signal_name,shot_id,timefirst=timefirst,time_last=time_last)
			if np.nanmax(np.abs(data.data))>0:
				beams_on_flag = True
		except:
			beams_on_flag = True	# in case I cannot know, better assume there are beams
			pass
		try:
			signal_name = '/XNB/SW/BEAMPOWER'
			data = client_int.get(signal_name,shot_id,timefirst=timefirst,time_last=time_last)
			if np.nanmax(np.abs(data.data))>0:
				beams_on_flag = True
		except:
			beams_on_flag = True	# in case I cannot know, better assume there are beams
			pass
		client_int.reset_connection()
		# reset_connection(client_int)
		# del client_int
	return beams_on_flag

################################################################################################################################################################################################################

# sensitivity matrix manipulation

def build_laplacian(grid,diagonal_factor=0.5,cells_to_exclude=[]):
	# Try making grid laplacian matrix for spatial regularisation
	num_cells = len(grid)
	grid_laplacian = np.zeros((num_cells, num_cells),dtype=np.float32)
	unique_x = np.unique(np.mean(grid,axis=1)[:,0])
	unique_y = np.unique(np.mean(grid,axis=1)[:,1])
	grid_x = np.mean(grid,axis=1)[:,0]
	grid_y = np.mean(grid,axis=1)[:,1]
	if diagonal_factor==0:
		account_diagonals = False
	else:
		account_diagonals = True
	# 	# change to take into account that the distance is different on the diagonals.
	# 	# https://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm
	# 	# here they mention too a laplacian operator with a different coefficient on the diagonals
	# 	diagonal_factor = 0.5
	# else:
	# 	diagonal_factor = 1
	there_are_cells_to_exclude = len(cells_to_exclude)>0
	if there_are_cells_to_exclude:
		cells_to_exclude = np.array(cells_to_exclude).flatten()

	for ith_cell in range(num_cells):
		if there_are_cells_to_exclude:
			if cells_to_exclude[ith_cell]==True:
				continue

		# get the 2D mesh coordinates of this cell
		ix = np.abs(unique_x - np.mean(grid,axis=1)[ith_cell,0]).argmin()	# radious
		iy = np.abs(unique_y - np.mean(grid,axis=1)[ith_cell,1]).argmin()	# Z

		neighbours = 0

		if ix>0:
			try:
				select = np.logical_and(grid_x==unique_x[ix-1],grid_y==unique_y[iy])  # neighbour 1 left
				if np.sum(select)>0:
					n1 = select.argmax()
					if there_are_cells_to_exclude:
						if cells_to_exclude[n1]!=True:
							grid_laplacian[ith_cell, n1] = -1
							neighbours += 1
					else:
						grid_laplacian[ith_cell, n1] = -1
						neighbours += 1
			except KeyError:
				pass

		try:
			select = np.logical_and(grid_x==unique_x[ix],grid_y==unique_y[iy+1])  # neighbour 3 top
			if np.sum(select)>0:
				n1 = select.argmax()
				if there_are_cells_to_exclude:
					if cells_to_exclude[n1]!=True:
						grid_laplacian[ith_cell, n1] = -1
						neighbours += 1
				else:
					grid_laplacian[ith_cell, n1] = -1
					neighbours += 1
		except:
			pass

		try:
			select = np.logical_and(grid_x==unique_x[ix+1],grid_y==unique_y[iy])  # neighbour 5 right
			if np.sum(select)>0:
				n1 = select.argmax()
				if there_are_cells_to_exclude:
					if cells_to_exclude[n1]!=True:
						grid_laplacian[ith_cell, n1] = -1
						neighbours += 1
				else:
					grid_laplacian[ith_cell, n1] = -1
					neighbours += 1
		except:
			pass

		if iy>0:
			try:
				select = np.logical_and(grid_x==unique_x[ix],grid_y==unique_y[iy-1])  # neighbour 7 down
				if np.sum(select)>0:
					n1 = select.argmax()
					if there_are_cells_to_exclude:
						if cells_to_exclude[n1]!=True:
							grid_laplacian[ith_cell, n1] = -1
							neighbours += 1
					else:
						grid_laplacian[ith_cell, n1] = -1
						neighbours += 1
			except:
				pass

		if account_diagonals:
			if ix>0:
				try:
					select = np.logical_and(grid_x==unique_x[ix-1],grid_y==unique_y[iy+1])  # neighbour 2 top left
					if np.sum(select)>0:
						n1 = select.argmax()
						if there_are_cells_to_exclude:
							if cells_to_exclude[n1]!=True:
								grid_laplacian[ith_cell, n1] = -diagonal_factor
								neighbours += diagonal_factor
						else:
							grid_laplacian[ith_cell, n1] = -diagonal_factor
							neighbours += diagonal_factor
				except:
					pass

			try:
				select = np.logical_and(grid_x==unique_x[ix+1],grid_y==unique_y[iy+1])  # neighbour 4 top right
				if np.sum(select)>0:
					n1 = select.argmax()
					if there_are_cells_to_exclude:
						if cells_to_exclude[n1]!=True:
							grid_laplacian[ith_cell, n1] = -diagonal_factor
							neighbours += diagonal_factor
					else:
						grid_laplacian[ith_cell, n1] = -diagonal_factor
						neighbours += diagonal_factor
			except:
				pass

			if iy>0:
				try:
					select = np.logical_and(grid_x==unique_x[ix+1],grid_y==unique_y[iy-1])  # neighbour 6 down right
					if np.sum(select)>0:
						n1 = select.argmax()
						if there_are_cells_to_exclude:
							if cells_to_exclude[n1]!=True:
								grid_laplacian[ith_cell, n1] = -diagonal_factor
								neighbours += diagonal_factor
						else:
							grid_laplacian[ith_cell, n1] = -diagonal_factor
							neighbours += diagonal_factor
				except:
					pass

			if ix>0 and iy>0:
				try:
					select = np.logical_and(grid_x==unique_x[ix-1],grid_y==unique_y[iy-1])  # neighbour 8 down left
					if np.sum(select)>0:
						n1 = select.argmax()
						if there_are_cells_to_exclude:
							if cells_to_exclude[n1]!=True:
								grid_laplacian[ith_cell, n1] = -diagonal_factor
								neighbours += diagonal_factor
						else:
							grid_laplacian[ith_cell, n1] = -diagonal_factor
							neighbours += diagonal_factor
				except:
					pass

		grid_laplacian[ith_cell, ith_cell] = neighbours
	return grid_laplacian

def build_Z_derivate(grid,cells_to_exclude=[]):
	# Try making grid Z direction derivate matrix for spatial regularisation
	num_cells = len(grid)
	grid_laplacian = np.zeros((num_cells, num_cells),dtype=np.float32)
	unique_x = np.unique(np.mean(grid,axis=1)[:,0])
	unique_y = np.unique(np.mean(grid,axis=1)[:,1])
	grid_x = np.mean(grid,axis=1)[:,0]
	grid_y = np.mean(grid,axis=1)[:,1]

	there_are_cells_to_exclude = len(cells_to_exclude)>0
	if there_are_cells_to_exclude:
		cells_to_exclude = np.array(cells_to_exclude).flatten()

	for ith_cell in range(num_cells):
		if there_are_cells_to_exclude:
			if cells_to_exclude[ith_cell]==True:
				continue

		# get the 2D mesh coordinates of this cell
		ix = np.abs(unique_x - np.mean(grid,axis=1)[ith_cell,0]).argmin()	# radious
		iy = np.abs(unique_y - np.mean(grid,axis=1)[ith_cell,1]).argmin()	# Z

		neighbours = 0

		try:
			select = np.logical_and(grid_x==unique_x[ix],grid_y==unique_y[iy+1])  # neighbour 3 top
			if np.sum(select)>0:
				n1 = select.argmax()
				if there_are_cells_to_exclude:
					if cells_to_exclude[n1]!=True:
						grid_laplacian[ith_cell, n1] = 1
						neighbours += 1
				else:
					grid_laplacian[ith_cell, n1] = 1
					neighbours += 1
		except:
			pass

		if iy>0:
			try:
				select = np.logical_and(grid_x==unique_x[ix],grid_y==unique_y[iy-1])  # neighbour 7 down
				if np.sum(select)>0:
					n1 = select.argmax()
					if there_are_cells_to_exclude:
						if cells_to_exclude[n1]!=True:
							grid_laplacian[ith_cell, n1] = -1
							neighbours += 1
					else:
						grid_laplacian[ith_cell, n1] = -1
						neighbours += 1
			except:
				pass

		if neighbours==2:
			grid_laplacian[ith_cell, ith_cell] = 0
			grid_laplacian[ith_cell] /=2
		elif neighbours==1:
			grid_laplacian[ith_cell, ith_cell] = 1
		else:
			grid_laplacian[ith_cell, ith_cell] = 0

	return grid_laplacian

def build_R_derivate(grid,cells_to_exclude=[]):
	# Try making grid R direction derivate matrix for spatial regularisation
	num_cells = len(grid)
	grid_laplacian = np.zeros((num_cells, num_cells),dtype=np.float32)
	unique_x = np.unique(np.mean(grid,axis=1)[:,0])
	unique_y = np.unique(np.mean(grid,axis=1)[:,1])
	grid_x = np.mean(grid,axis=1)[:,0]
	grid_y = np.mean(grid,axis=1)[:,1]

	there_are_cells_to_exclude = len(cells_to_exclude)>0
	if there_are_cells_to_exclude:
		cells_to_exclude = np.array(cells_to_exclude).flatten()

	for ith_cell in range(num_cells):
		if there_are_cells_to_exclude:
			if cells_to_exclude[ith_cell]==True:
				continue

		# get the 2D mesh coordinates of this cell
		ix = np.abs(unique_x - np.mean(grid,axis=1)[ith_cell,0]).argmin()	# radious
		iy = np.abs(unique_y - np.mean(grid,axis=1)[ith_cell,1]).argmin()	# Z

		neighbours = 0

		if ix>0:
			try:
				select = np.logical_and(grid_x==unique_x[ix-1],grid_y==unique_y[iy])  # neighbour 1 left
				if np.sum(select)>0:
					n1 = select.argmax()
					if there_are_cells_to_exclude:
						if cells_to_exclude[n1]!=True:
							grid_laplacian[ith_cell, n1] = -1
							neighbours += 1
					else:
						grid_laplacian[ith_cell, n1] = -1
						neighbours += 1
			except KeyError:
				pass

		try:
			select = np.logical_and(grid_x==unique_x[ix+1],grid_y==unique_y[iy])  # neighbour 5 right
			if np.sum(select)>0:
				n1 = select.argmax()
				if there_are_cells_to_exclude:
					if cells_to_exclude[n1]!=True:
						grid_laplacian[ith_cell, n1] = 1
						neighbours += 1
				else:
					grid_laplacian[ith_cell, n1] = 1
					neighbours += 1
		except:
			pass

		if neighbours==2:
			grid_laplacian[ith_cell, ith_cell] = 0
			grid_laplacian[ith_cell] /=2
		elif neighbours==1:
			grid_laplacian[ith_cell, ith_cell] = 1
		else:
			grid_laplacian[ith_cell, ith_cell] = 0
	return grid_laplacian

def reduce_voxels(sensitivities_reshaped,grid_laplacian,grid_data,std_treshold = 1e-4, sum_treshold = 0.000, core_radious_treshold = 1.9, divertor_radious_treshold = 1.9,chop_top_corner = False, extra_chop_top_corner = False , chop_corner_close_to_baffle = False, chop_corner_outside_SXD_divertor = True, restrict_polygon = FULL_MASTU_CORE_GRID_POLYGON,cells_to_exclude=[]):
	from shapely.geometry import Point
	from shapely.geometry.polygon import Polygon
	# masking the voxels whose emission does not reach the foil
	# select = np.sum(sensitivities_reshaped,axis=(0,1))>0.05
	select = np.logical_and(np.std(sensitivities_reshaped,axis=(0,1))>std_treshold*(np.std(sensitivities_reshaped,axis=(0,1)).max()),np.sum(sensitivities_reshaped,axis=(0,1))>sum_treshold)
	grid_data_masked = grid_data[select]
	sensitivities_reshaped_masked = sensitivities_reshaped[:,:,select]

	# this is not enough because the voxels close to the pinhole have a too large influence on it and the inversion is weird
	# select = np.median(sensitivities_reshaped_masked,axis=(0,1))<5*np.mean(np.median(sensitivities_reshaped_masked,axis=(0,1)))
	select = np.logical_or(np.mean(grid_data_masked,axis=1)[:,0]<core_radious_treshold,np.mean(grid_data_masked,axis=1)[:,1]<-1.3)
	if chop_corner_outside_SXD_divertor:
		x1 = [1.33,-2.115]	# r,z
		x2 = [1.735,-1.717]
		interp = interp1d([x1[0],x2[0]],[x1[1],x2[1]],fill_value="extrapolate",bounds_error=False)
		select = np.logical_and(select,np.mean(grid_data_masked,axis=1)[:,1]>interp(np.mean(grid_data_masked,axis=1)[:,0]))
	if chop_top_corner:
		x1 = [1.1,0.6]	# r,z
		x2 = [1.6,0.0]
		interp = interp1d([x1[0],x2[0]],[x1[1],x2[1]],fill_value="extrapolate",bounds_error=False)
		select = np.logical_and(select,np.mean(grid_data_masked,axis=1)[:,1]<interp(np.mean(grid_data_masked,axis=1)[:,0]))
	if extra_chop_top_corner:
		x1 = [1.1,0.3]	# r,z
		x2 = [1.4,-0.05]
		interp = interp1d([x1[0],x2[0]],[x1[1],x2[1]],fill_value="extrapolate",bounds_error=False)
		select = np.logical_and(select,np.mean(grid_data_masked,axis=1)[:,1]<interp(np.mean(grid_data_masked,axis=1)[:,0]))
	if chop_corner_close_to_baffle:
		x1 = [1.33,-0.9]	# r,z
		x2 = [1.5,-1.03]	# r,z
		x3 = [1.2,-1.07]	# r,z
		interp1 = interp1d([x1[0],x2[0]],[x1[1],x2[1]],fill_value="extrapolate",bounds_error=False)
		interp2 = interp1d([x1[0],x3[0]],[x1[1],x3[1]],fill_value="extrapolate",bounds_error=False)
		select2 = np.logical_or(np.mean(grid_data_masked,axis=1)[:,1]>interp1(np.mean(grid_data_masked,axis=1)[:,0]),np.mean(grid_data_masked,axis=1)[:,1]>interp2(np.mean(grid_data_masked,axis=1)[:,0]))
		select2 = np.logical_or(select2,np.mean(grid_data_masked,axis=1)[:,1]<-1.2)
		# x4 = [1.4,-0.55]	# r,z
		# interp3 = interp1d([x4[0],x3[0]],[x4[1],x3[1]],fill_value="extrapolate",bounds_error=False)
		# select2 = np.logical_or(np.mean(grid_data_masked,axis=1)[:,1]<-1.1,np.mean(grid_data_masked,axis=1)[:,1]>interp3(np.mean(grid_data_masked,axis=1)[:,0]))
		select = np.logical_and(select,select2)
	if len(restrict_polygon)>0:
		polygon = Polygon(restrict_polygon)
		for i_e in range(len(grid_data_masked)):
			if np.sum([polygon.contains(Point((grid_data_masked[i_e][i__e,0],grid_data_masked[i_e][i__e,1]))) for i__e in range(4)])==0:
				select[i_e] = False
	if len(cells_to_exclude)==len(select):
		select = np.logical_and(select,np.logical_not(cells_to_exclude))
	grid_data_masked = grid_data_masked[select]
	sensitivities_reshaped_masked = sensitivities_reshaped_masked[:,:,select]
	select = np.logical_or(np.mean(grid_data_masked,axis=1)[:,0]<divertor_radious_treshold,np.mean(grid_data_masked,axis=1)[:,1]>-1.3)
	grid_data_masked = grid_data_masked[select]
	sensitivities_reshaped_masked = sensitivities_reshaped_masked[:,:,select]
	grid_laplacian_masked = build_laplacian(grid_data_masked)
	grid_Z_derivate_masked = build_Z_derivate(grid_data_masked)
	grid_R_derivate_masked = build_R_derivate(grid_data_masked)


	return sensitivities_reshaped_masked,grid_laplacian_masked,grid_data_masked,grid_Z_derivate_masked,grid_R_derivate_masked


def retrive_shot_scenario(shot,path='/home/ffederic/work/irvb/MAST-U/'+'shot_list2.ods'):

	scenario = ''
	try:
		# shot_list = get_data(path)
		shot_list = load_shot_list(path)
		temp1 = (np.array(shot_list['Sheet1'][0])=='shot number').argmax()
		for i in range(1,len(shot_list['Sheet1'])):
			if shot_list['Sheet1'][i][temp1] == int(shot):
				scenario = shot_list['Sheet1'][i][(np.array(shot_list['Sheet1'][0])=='Scenario').argmax()]
				break
	except:
		pass
	# try:
	# 	day = str(date.date())
	# except:
	# 	day = str(date)
	return scenario

def retrive_shot_date_and_time(shot,path='/home/ffederic/work/irvb/MAST-U/'+'shot_list2.ods'):
	from datetime import datetime
	from pyexcel_ods import save_data,get_data

	# shot_list = get_data(path)
	shot_list = load_shot_list(path)
	temp1 = (np.array(shot_list['Sheet1'][0])=='shot number').argmax()
	all_shots = np.array([shot_list['Sheet1'][i][3] for i in range(len(shot_list['Sheet1'])) if (len(shot_list['Sheet1'][i])>0 and shot_list['Sheet1'][i][3]!='')][1:])
	try:
		date=''
		if int(shot) in all_shots:
			date = shot_list['Sheet1'][np.argmax(all_shots.astype(int)==int(shot))+1][(np.array(shot_list['Sheet1'][0])=='date').argmax()]
		# for i in range(1,len(shot_list['Sheet1'])):
		# 	if shot_list['Sheet1'][i][temp1] == int(shot):
		# 		date = shot_list['Sheet1'][i][(np.array(shot_list['Sheet1'][0])=='date').argmax()]
		# 		break
		if date=='':
			sba=gnu	# date not found in the shot list file, I want this to generate an error
	except:
		try:
			# import pyuda
			client_int=pyuda.Client()
			date_time = client_int.get_shot_date_time(shot)
			date = date_time[0]+' '+date_time[1][:8]
			# shot_list = get_data(path)
			# temp1 = (np.array(shot_list['Sheet1'][0])=='shot number').argmax()
			if int(shot) in all_shots:
				shot_list['Sheet1'][np.argmax(all_shots.astype(int)==int(shot))+1][(np.array(shot_list['Sheet1'][0])=='date').argmax()] = date
				save_data(path,shot_list)
			# for i in range(1,len(shot_list['Sheet1'])):
			# 	if shot_list['Sheet1'][i][temp1] == int(shot):
			# 		shot_list['Sheet1'][i][(np.array(shot_list['Sheet1'][0])=='date').argmax()] = date
			# 		save_data(path,shot_list)
			# 		break
		except:
			pass
		try:
			client_int.reset_connection()
			# reset_connection(client_int)
		except:
			pass
		# try:
		# 	del client_int
		# except:
		# 	pass

	# try:
	# 	day = str(date.date())
	# except:
	# 	day = str(date)
	date = date.replace("'",'')
	return date[:10],date[11:]

def retrive_aborted(shot,path='/home/ffederic/work/irvb/MAST-U/'+'shot_list2.ods'):
	from pyexcel_ods import save_data,get_data

	# shot_list = get_data(path)
	shot_list = load_shot_list(path)
	temp1 = (np.array(shot_list['Sheet1'][0])=='shot number').argmax()
	all_shots = np.array([shot_list['Sheet1'][i][3] for i in range(len(shot_list['Sheet1'])) if (len(shot_list['Sheet1'][i])>0 and shot_list['Sheet1'][i][3]!='')][1:])
	try:
		Abort=''
		if int(shot) in all_shots:
			Abort = shot_list['Sheet1'][np.argmax(all_shots.astype(int)==int(shot))+1][(np.array(shot_list['Sheet1'][0])=='Abort').argmax()]

		if Abort=='':
			sba=gnu	# abort state not found in the shot list file
		elif Abort in ['yes', 'effectively yes']:
			Abort=True
	except:
		Abort=False

	return Abort

def retrive_vessel_average_temp_archve(shot):
	import csv
	from datetime import datetime
	# import pyuda

	# client_int=pyuda.Client()
	try:
		# date = client_int.get_shot_date_time(shot)[0]+', '+client_int.get_shot_date_time(shot)[1][:8]
		# date_format = datetime.strptime(date,"%Y-%m-%d, %H:%M:%S")
		date,time = retrive_shot_date_and_time(shot)
		date_format = datetime.strptime(date+' '+time,"%Y-%m-%d %H:%M:%S")
		date_unix = datetime.timestamp(date_format)

		if int(shot) > 45517:	# restricted to MU02
			filename = '/home/ffederic/work/irvb/MAST-U/MU02_VesselTemp_Raw_2023.csv'
			csvfile=open(filename,'r')
			reader = csv.reader(csvfile)
			read = np.array(list(reader))

			temperature = np.interp(date_unix,read[1:].astype(int)[:,0],read[1:].astype(int)[:,1])
		elif int(shot) < 45517:
			filename = '/home/ffederic/work/irvb/MAST-U/MU01_VesselTemp_Raw_2021.csv'
			csvfile=open(filename,'r')
			reader = csv.reader(csvfile)
			read = np.array(list(reader))

			# index = np.abs(read[1:].astype(int)[:,0]-date_unix).argmin()
			# temperature = int(read[1:][index,1])
			temperature = np.interp(date_unix,read[1:].astype(int)[:,0],read[1:].astype(int)[:,1])
		else:
			print('a method to get vessel temperature for >MU02 has still to be devised, for now it is given a standart 25deg')
			temperature = 25
	except Exception as e:
		print('reading of vessel temperature failed')
		logging.exception('with error: ' + str(e))
		temperature = np.nan
	# reset_connection(client_int)
	# del client_int

	return temperature

def load_shot_list(path):
	from pyexcel_ods import get_data
	import time as tm

	path_read = False
	counter = 0
	while (not path_read) and counter<30:	# 30 times x 20 seconds =10 minutes
		counter += 1
		try:
			shot_list = get_data(path)
			path_read = True
			print(path+ ' read at attempt '+str(counter))
		except:
			tm.sleep(20)
			pass
	return shot_list


def retrive_shot_foil_mask_type(shot,path='/home/ffederic/work/irvb/MAST-U/'+'shot_list2.ods'):
	from datetime import datetime
	from pyexcel_ods import save_data,get_data

	# shot_list = get_data(path)
	shot_list = load_shot_list(path)
	temp1 = (np.array(shot_list['Sheet1'][0])=='shot number').argmax()
	all_shots = np.array([shot_list['Sheet1'][i][3] for i in range(len(shot_list['Sheet1'])) if (len(shot_list['Sheet1'][i])>0 and shot_list['Sheet1'][i][3]!='')][1:])
	try:
		mask_type=''
		if int(shot) in all_shots:
			mask_type = shot_list['Sheet1'][np.argmax(all_shots.astype(int)==int(shot))+1][(np.array(shot_list['Sheet1'][0])=='IRVB mask type').argmax()]
		if mask_type=='':
			mask_type = 'standard'
			shot_list['Sheet1'][np.argmax(all_shots.astype(int)==int(shot))+1][(np.array(shot_list['Sheet1'][0])=='IRVB mask type').argmax()] = mask_type
			save_data(path,shot_list)
	except:
		print('reading of mask type of '+str(shot)+' failed, standard used')
		mask_type = 'standard'

	# try:
	# 	day = str(date.date())
	# except:
	# 	day = str(date)
	return mask_type


# from Kevin 11/05/2022
def uda_transfer(shotnr,tag,savedir=os.getcwd()+'/dum.nc',extra_path = ''):
	import pyuda

	if savedir[-6:] == 'dum.nc':
		if tag=='elp':
			savedir = savedir[:-6] +tag+'0'+ str(shotnr) + '.nc'
		else:
			savedir = savedir[:-6] +tag+ str(shotnr) + '.nc'
	# get efit

	if not isinstance(shotnr,str):
		shotnr = str(shotnr)

	client_int = pyuda.Client()
	try:
		client_int.get_file('$MAST_DATA'+extra_path+'/'+shotnr+'/LATEST/'+tag+'0'+shotnr+'.nc',savedir)
	except:
		savedir = 'failed'
	client_int.reset_connection()
	# reset_connection(client_int)
	# del client_int

	return savedir


def L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,point,efit_data,i_efit_time):
	import mastu_exhaust_analysis.fluxsurface_tracer as ft
	r_coord_smooth_origial = np.flip(np.array([y for _, y in sorted(zip(z_coord_smooth_origial, r_coord_smooth_origial))]),axis=0)
	z_coord_smooth_origial = np.flip(np.sort(z_coord_smooth_origial),axis=0)
	dist = ((r_coord_smooth_origial-point[0])**2 + (z_coord_smooth_origial-point[1])**2)**0.5
	points_temp_r = np.array([y for _, y in sorted(zip(dist, r_coord_smooth_origial))])[:14]
	points_temp_z = np.array([y for _, y in sorted(zip(dist, z_coord_smooth_origial))])[:14]
	points_temp_r = np.array([y for _, y in sorted(zip(points_temp_z, points_temp_r))])
	points_temp_z = np.sort(points_temp_z)
	points_temp_r = np.interp(np.linspace(points_temp_z.min(),points_temp_z.max(),num=200),points_temp_z,points_temp_r)
	points_temp_z = np.linspace(points_temp_z.min(),points_temp_z.max(),num=200)
	dist = ((points_temp_r-point[0])**2 + (points_temp_z-point[1])**2)**0.5
	point = [points_temp_r[dist.argmin()],points_temp_z[dist.argmin()]]	# this means I pick the closer point on the separatrix to the peak
	# trace_flux_surface = ft.trace_flux_surface(efit_data, i_efit_time,point[0],point[1])
	# r_coord_smooth = np.array(trace_flux_surface.r)
	# z_coord_smooth = np.array(trace_flux_surface.z)
	dist = (r_coord_smooth_origial-point[0])**2 + (z_coord_smooth_origial-point[1])**2
	r_coord_smooth_origial = r_coord_smooth_origial[dist.argmin():]
	z_coord_smooth_origial = z_coord_smooth_origial[dist.argmin():]
	L_poloidal_point = np.nansum((np.diff(r_coord_smooth_origial)**2 + np.diff(z_coord_smooth_origial)**2)**0.5)
	L_poloidal_point += np.sign(point[1] - z_coord_smooth_origial[0])*(((point[0] - r_coord_smooth_origial[0])**2 + (point[1] - z_coord_smooth_origial[0])**2)**0.5)
	return L_poloidal_point


def baricentre_outer_separatrix_radiation(inverted_data,inverted_data_sigma,inversion_R,inversion_Z,time_full_binned_crop,efit_reconstruction,covariance_out,grid_data_masked_crop,leg_resolution = 0.1,sideways_leg_resolution=0.1,x_point_region_radious=0.1,sideways_leg_resolution_for_interp=0.02,starting_distance_find_separatrix=0.02,midpoint_range=0.2):
	from shapely.geometry import Point
	from shapely.geometry.polygon import Polygon
	from scipy.ndimage import generic_filter
	import mastu_exhaust_analysis.fluxsurface_tracer as ft
	from mastu_exhaust_analysis.read_efit import read_epm
	from scipy.interpolate import interp1d
	from scipy.interpolate import RegularGridInterpolator

	all_time_sep_r,all_time_sep_z,r_fine,z_fine = efit_reconstruction_to_separatrix_on_foil(efit_reconstruction)	# the separatrix are ordered as left_of_up_xpoints,right_of_up_xpoints,left_of_low_xpoints,right_of_low_xpoints
	all_time_strike_points_location = return_all_time_strike_points_location_radial(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine)
	fdir = uda_transfer(efit_reconstruction.shotnumber,'epm')
	efit_data = read_epm(fdir,calc_bfield=True)
	os.remove(fdir)
	greenwald_density = efit_data['ngw']
	greenwald_density = np.interp(time_full_binned_crop,efit_reconstruction.time,greenwald_density,left=np.nan,right=np.nan)
	dr_sep_in = efit_data['dr_sep_in']
	dr_sep_in = np.interp(time_full_binned_crop,efit_reconstruction.time,dr_sep_in,left=np.nan,right=np.nan)
	dr_sep_out = efit_data['dr_sep_out']
	dr_sep_out = np.interp(time_full_binned_crop,efit_reconstruction.time,dr_sep_out,left=np.nan,right=np.nan)

	dr = np.median(np.diff(inversion_R))
	dz = np.median(np.diff(inversion_Z))
	a,b = np.meshgrid(inversion_Z,inversion_R)
	a_flat = a.flatten()
	b_flat = b.flatten()
	grid_resolution = np.median(np.concatenate([np.diff(inversion_Z),np.diff(inversion_R)]))
	# leg_resolution = 0.1	# m
	# structure_radial_profile = return_structure_radial_profile()
	z_,r_ = np.meshgrid(inversion_Z,inversion_R)
	MU01_sxd_region_delimiter = return_MU01_sxd_region_delimiter()
	DMS_LOS19_V2_higher_delimiter = return_DMS_LOS19_V2_higher_delimiter()
	MWI_delimiter = return_MWI_delimiter()

	voxels_centre = np.mean(grid_data_masked_crop,axis=1)

	data_length = 0
	local_mean_emis_all = []
	local_power_all = []
	local_L_poloidal_all = []
	leg_length_all = []
	leg_length_interval_all = []
	half_peak_L_pol_all = []
	half_peak_divertor_L_pol_all = []
	emissivity_baricentre_all = []
	emissivity_peak_all = []
	L_poloidal_baricentre_all = []
	L_poloidal_peak_all = []
	L_poloidal_peak_only_leg_all = []
	L_poloidal_baricentre_only_leg_all = []
	L_poloidal_x_point_all = []
	L_poloidal_midplane_all = []
	leg_reliable_power_all = []
	leg_reliable_power_sigma_all = []
	DMS_equivalent_power_all = []
	DMS_equivalent_power_sigma_all = []
	MWI_equivalent_power_all = []
	MWI_equivalent_power_sigma_all = []
	x_point_power_all = []
	x_point_power_sigma_all = []
	sxd_tot_rad_power_all = []
	sxd_tot_rad_power_sigma_all = []
	for i_t in range(len(time_full_binned_crop)):
		print('starting t=%.4gms' %(time_full_binned_crop[i_t]*1e3))
		try:
			i_efit_time = np.abs(efit_reconstruction.time-time_full_binned_crop[i_t]).argmin()
			try:
				distance_find_separatrix = cp.deepcopy(starting_distance_find_separatrix)
				r_coord_smooth_int = np.array([0,0])
				while np.sum(r_coord_smooth_int<efit_data['lower_xpoint_r'][i_efit_time])>0 and distance_find_separatrix<0.2:
					trace_flux_surface = ft.trace_flux_surface(efit_data, i_efit_time,efit_data['lower_xpoint_r'][i_efit_time]+distance_find_separatrix,efit_data['lower_xpoint_z'][i_efit_time])
					r_coord_smooth = np.array(trace_flux_surface.r)
					z_coord_smooth = np.array(trace_flux_surface.z)
					r_coord_smooth_int = r_coord_smooth[z_coord_smooth<=efit_reconstruction.mag_axis_z[i_efit_time]]	# I add this because it could get confusing for signficant dr_sep
					# print(distance_find_separatrix)
					distance_find_separatrix += 0.001
				if False:
					plt.plot(trace_flux_surface.r,trace_flux_surface.z,'+')
				r_coord_smooth = scipy.signal.savgol_filter(np.array(trace_flux_surface.r),7,2)
				z_coord_smooth = scipy.signal.savgol_filter(np.array(trace_flux_surface.z),7,2)
			except:
				print('skipped to my own method to find the separatrix')
				if len(all_time_sep_r[i_efit_time][1])==0 and len(all_time_sep_r[i_efit_time][3])>0:
					i_closer_separatrix_to_x_point = 3
				elif len(all_time_sep_r[i_efit_time][3])==0 and len(all_time_sep_r[i_efit_time][1])>0:
					i_closer_separatrix_to_x_point = 1
				elif len(all_time_sep_r[i_efit_time][3])==0 and len(all_time_sep_r[i_efit_time][1])==0:
					local_mean_emis_all.append([])
					local_power_all.append([])
					local_L_poloidal_all.append([])
					leg_length_all.append(0)
					leg_length_interval_all.append([])
					half_peak_L_pol_all.append([0,0,0])
					half_peak_divertor_L_pol_all.append([0,0,0])
					emissivity_baricentre_all.append([np.nan,np.nan])
					emissivity_peak_all.append([np.nan,np.nan])
					L_poloidal_baricentre_all.append(0)
					L_poloidal_peak_all.append(0)
					L_poloidal_peak_only_leg_all.append(0)
					L_poloidal_baricentre_only_leg_all.append(0)
					L_poloidal_x_point_all.append(np.inf)
					L_poloidal_midplane_all.append(np.inf)
					leg_reliable_power_all.append(0)
					leg_reliable_power_sigma_all.append(0)
					DMS_equivalent_power_all.append(0)
					DMS_equivalent_power_sigma_all.append(0)
					MWI_equivalent_power_all.append(0)
					MWI_equivalent_power_sigma_all.append(0)
					x_point_power_all.append(0)
					x_point_power_sigma_all.append(0)
					sxd_tot_rad_power_all.append(0)
					sxd_tot_rad_power_sigma_all.append(0)
					print('skipped t=%.4gms' %(time_full_binned_crop[i_t]*1e3))
					continue
				elif np.abs((r_fine[all_time_sep_r[i_efit_time][1]]- efit_reconstruction.lower_xpoint_r[i_efit_time])**2 + (z_fine[all_time_sep_z[i_efit_time][1]]- efit_reconstruction.lower_xpoint_z[i_efit_time])**2 ).min() < np.abs((r_fine[all_time_sep_r[i_efit_time][3]]- efit_reconstruction.lower_xpoint_r[i_efit_time])**2 + (z_fine[all_time_sep_z[i_efit_time][3]]- efit_reconstruction.lower_xpoint_z[i_efit_time])**2 ).min():
					i_closer_separatrix_to_x_point = 1
				else:
					i_closer_separatrix_to_x_point = 3
				i_where_x_point_is = np.abs((r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]]- efit_reconstruction.lower_xpoint_r[i_efit_time])**2 + (z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]]- efit_reconstruction.lower_xpoint_z[i_efit_time])**2 ).argmin()

				temp = np.array([((all_time_strike_points_location[i_efit_time][0][i] - r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][:i_where_x_point_is])**2 + (all_time_strike_points_location[i_efit_time][1][i] - z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][:i_where_x_point_is])**2).min() for i in range(len(all_time_strike_points_location[i_efit_time][1]))])
				temp[np.isnan(temp)] = np.inf
				i_which_strike_point_is = temp.argmin()
				i_where_strike_point_is = ((all_time_strike_points_location[i_efit_time][0][i_which_strike_point_is] - r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][:i_where_x_point_is])**2 + (all_time_strike_points_location[i_efit_time][1][i_which_strike_point_is] - z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][:i_where_x_point_is])**2).argmin()
				# plt.figure()
				# plt.plot(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is],z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is])
				# plt.pause(0.001)
				# r_coord_smooth = generic_filter(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is],np.mean,size=11)
				# z_coord_smooth = generic_filter(z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is],np.mean,size=11)
				r_coord_smooth = scipy.signal.savgol_filter(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:],max(11,int(len(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:])/50//2*2+1)),2)
				z_coord_smooth = scipy.signal.savgol_filter(z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:],max(11,int(len(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:])/50//2*2+1)),2)
			r_coord_smooth = r_coord_smooth[z_coord_smooth<efit_reconstruction.mag_axis_z[i_efit_time]]
			z_coord_smooth = z_coord_smooth[z_coord_smooth<efit_reconstruction.mag_axis_z[i_efit_time]]
			z_coord_smooth = z_coord_smooth[r_coord_smooth>efit_reconstruction.lower_xpoint_r[i_efit_time]]
			r_coord_smooth = r_coord_smooth[r_coord_smooth>efit_reconstruction.lower_xpoint_r[i_efit_time]]
			r_coord_smooth_origial = cp.deepcopy(r_coord_smooth)
			z_coord_smooth_origial = cp.deepcopy(z_coord_smooth)
			leg_length = np.sum((np.diff(r_coord_smooth)**2 + np.diff(z_coord_smooth)**2)**0.5)
			# I arbitrarily decide to cut the leg in 10cm pieces
			leg_length_interval = [0]
			target_length = 0 + leg_resolution
			i_ref_points = [0]
			ref_points = [[r_coord_smooth[0],z_coord_smooth[0]]]
			while target_length < leg_length + leg_resolution:
				# print(target_length)
				temp = np.abs(np.cumsum((np.diff(r_coord_smooth)**2 + np.diff(z_coord_smooth)**2)**0.5) - target_length).argmin()
				leg_length_interval.append(np.cumsum((np.diff(r_coord_smooth)**2 + np.diff(z_coord_smooth)**2)**0.5)[temp] - np.sum(leg_length_interval))
				i_ref_points.append(temp+1)
				ref_points.append([r_coord_smooth[temp+1],z_coord_smooth[temp+1]])
				target_length += leg_resolution
			ref_points = np.array(ref_points)
			leg_length_interval = leg_length_interval[1:]
			# I want to eliminate doubles
			i_ref_points = np.concatenate([[i_ref_points[0]],np.array(i_ref_points[1:])[np.abs(np.diff(i_ref_points))>0]])
			ref_points = np.concatenate([[ref_points[0]],ref_points[1:][np.abs(np.diff(ref_points[:,0]))+np.abs(np.diff(ref_points[:,1]))>0]])
			leg_length_interval = np.array(leg_length_interval)[np.array(leg_length_interval)>0].tolist()

			ref_points_1 = expand_line_sideways(r_coord_smooth,z_coord_smooth,i_ref_points,-sideways_leg_resolution)
			ref_points_2 = expand_line_sideways(r_coord_smooth,z_coord_smooth,i_ref_points,+sideways_leg_resolution)
			ref_points_for_interp_vlx = expand_line_sideways(r_coord_smooth,z_coord_smooth,np.arange(len(r_coord_smooth)),-sideways_leg_resolution_for_interp)
			ref_points_for_interp_lx = expand_line_sideways(r_coord_smooth,z_coord_smooth,np.arange(len(r_coord_smooth)),-sideways_leg_resolution_for_interp/2)
			ref_points_for_interp_rx = expand_line_sideways(r_coord_smooth,z_coord_smooth,np.arange(len(r_coord_smooth)),+sideways_leg_resolution_for_interp/2)
			ref_points_for_interp_vrx = expand_line_sideways(r_coord_smooth,z_coord_smooth,np.arange(len(r_coord_smooth)),+sideways_leg_resolution_for_interp)

			# ref_points_1 = []
			# ref_points_2 = []
			# ref_points_for_interp_lx = []
			# ref_points_for_interp_rx = []
			# try:
			# 	m = -1/((z_coord_smooth[i_ref_points[0]]-z_coord_smooth[i_ref_points[0]+1])/(r_coord_smooth[i_ref_points[0]]-r_coord_smooth[i_ref_points[0]+1]))
			# 	ref_points_1.append([r_coord_smooth[i_ref_points[0]] - sideways_leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[0]] - m*sideways_leg_resolution/((1+m**2)**0.5)])
			# 	ref_points_2.append([r_coord_smooth[i_ref_points[0]] + sideways_leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[0]] + m*sideways_leg_resolution/((1+m**2)**0.5)])
			# 	ref_points_for_interp_lx.append([r_coord_smooth[i_ref_points[0]] - sideways_leg_resolution_for_interp/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[0]] - m*sideways_leg_resolution_for_interp/((1+m**2)**0.5)])
			# 	ref_points_for_interp_rx.append([r_coord_smooth[i_ref_points[0]] + sideways_leg_resolution_for_interp/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[0]] + m*sideways_leg_resolution_for_interp/((1+m**2)**0.5)])
			# except:
			# 	pass
			# for i_ref_point in range(1,len(i_ref_points)):
			# 	try:
			# 		m = -1/((z_coord_smooth[i_ref_points[i_ref_point]-1]-z_coord_smooth[i_ref_points[i_ref_point]+1])/(r_coord_smooth[i_ref_points[i_ref_point]-1]-r_coord_smooth[i_ref_points[i_ref_point]+1]))
			# 		ref_points_1.append([r_coord_smooth[i_ref_points[i_ref_point]] - sideways_leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[i_ref_point]] - m*sideways_leg_resolution/((1+m**2)**0.5)])
			# 		ref_points_2.append([r_coord_smooth[i_ref_points[i_ref_point]] + sideways_leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[i_ref_point]] + m*sideways_leg_resolution/((1+m**2)**0.5)])
			# 		ref_points_for_interp_lx.append([r_coord_smooth[i_ref_points[i_ref_point]] - sideways_leg_resolution_for_interp/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[i_ref_point]] - m*sideways_leg_resolution_for_interp/((1+m**2)**0.5)])
			# 		ref_points_for_interp_rx.append([r_coord_smooth[i_ref_points[i_ref_point]] + sideways_leg_resolution_for_interp/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[i_ref_point]] + m*sideways_leg_resolution_for_interp/((1+m**2)**0.5)])
			# 	except:
			# 		pass
			# try:
			# 	m = -1/((z_coord_smooth[i_ref_points[-1]-1]-z_coord_smooth[i_ref_points[-1]])/(r_coord_smooth[i_ref_points[-1]-1]-r_coord_smooth[i_ref_points[-1]]))
			# 	ref_points_1.append([r_coord_smooth[i_ref_points[-1]] - sideways_leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[-1]] - m*sideways_leg_resolution/((1+m**2)**0.5)])
			# 	ref_points_2.append([r_coord_smooth[i_ref_points[-1]] + sideways_leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[-1]] + m*sideways_leg_resolution/((1+m**2)**0.5)])
			# 	ref_points_for_interp_lx.append([r_coord_smooth[i_ref_points[-1]] - sideways_leg_resolution_for_interp/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[-1]] - m*sideways_leg_resolution_for_interp/((1+m**2)**0.5)])
			# 	ref_points_for_interp_rx.append([r_coord_smooth[i_ref_points[-1]] + sideways_leg_resolution_for_interp/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[-1]] + m*sideways_leg_resolution_for_interp/((1+m**2)**0.5)])
			# except:
			# 	pass
			# ref_points_1 = np.array(ref_points_1)
			# ref_points_2 = np.array(ref_points_2)
			# ref_points_for_interp_lx = np.array(ref_points_for_interp_lx)
			# ref_points_for_interp_rx = np.array(ref_points_for_interp_rx)

			ref_points_1_original = cp.deepcopy(ref_points_1)
			ref_points_2_original = cp.deepcopy(ref_points_2)
			# I want to be sure to include all the radiation close to the target so I prolong the references of 2 steps
			ref_points_1 = np.array(ref_points_1.tolist() + np.array([interp1d([0,1],ref_points_1[:,0][-2:],bounds_error=False, fill_value='extrapolate')([2,3]),interp1d([0,1],ref_points_1[:,1][-2:],bounds_error=False, fill_value='extrapolate')([2,3])]).T.tolist())	# r,z
			ref_points_1 = np.array(np.array([interp1d([0,1],ref_points_1[:,0][:2],bounds_error=False, fill_value='extrapolate')([-2,-1]),interp1d([0,1],ref_points_1[:,1][:2],bounds_error=False, fill_value='extrapolate')([-2,-1])]).T.tolist() + ref_points_1.tolist())	# r,z
			ref_points_2 = np.array(ref_points_2.tolist() + np.array([interp1d([0,1],ref_points_2[:,0][-2:],bounds_error=False, fill_value='extrapolate')([2,3]),interp1d([0,1],ref_points_2[:,1][-2:],bounds_error=False, fill_value='extrapolate')([2,3])]).T.tolist())	# r,z
			ref_points_2 = np.array(np.array([interp1d([0,1],ref_points_2[:,0][:2],bounds_error=False, fill_value='extrapolate')([-2,-1]),interp1d([0,1],ref_points_2[:,1][:2],bounds_error=False, fill_value='extrapolate')([-2,-1])]).T.tolist() + ref_points_2.tolist())	# r,z
			ref_points_for_interp_vlx = np.array(ref_points_for_interp_vlx.tolist() + np.array([interp1d([0,1],ref_points_for_interp_vlx[:,0][-2:],bounds_error=False, fill_value='extrapolate')([2,3]),interp1d([0,1],ref_points_for_interp_vlx[:,1][-2:],bounds_error=False, fill_value='extrapolate')([2,3])]).T.tolist())	# r,z
			ref_points_for_interp_vlx = np.array(np.array([interp1d([0,1],ref_points_for_interp_vlx[:,0][:2],bounds_error=False, fill_value='extrapolate')([-2,-1]),interp1d([0,1],ref_points_for_interp_vlx[:,1][:2],bounds_error=False, fill_value='extrapolate')([-2,-1])]).T.tolist() + ref_points_for_interp_vlx.tolist())	# r,z
			ref_points_for_interp_lx = np.array(ref_points_for_interp_lx.tolist() + np.array([interp1d([0,1],ref_points_for_interp_lx[:,0][-2:],bounds_error=False, fill_value='extrapolate')([2,3]),interp1d([0,1],ref_points_for_interp_lx[:,1][-2:],bounds_error=False, fill_value='extrapolate')([2,3])]).T.tolist())	# r,z
			ref_points_for_interp_lx = np.array(np.array([interp1d([0,1],ref_points_for_interp_lx[:,0][:2],bounds_error=False, fill_value='extrapolate')([-2,-1]),interp1d([0,1],ref_points_for_interp_lx[:,1][:2],bounds_error=False, fill_value='extrapolate')([-2,-1])]).T.tolist() + ref_points_for_interp_lx.tolist())	# r,z
			ref_points_for_interp_rx = np.array(ref_points_for_interp_rx.tolist() + np.array([interp1d([0,1],ref_points_for_interp_rx[:,0][-2:],bounds_error=False, fill_value='extrapolate')([2,3]),interp1d([0,1],ref_points_for_interp_rx[:,1][-2:],bounds_error=False, fill_value='extrapolate')([2,3])]).T.tolist())	# r,z
			ref_points_for_interp_rx = np.array(np.array([interp1d([0,1],ref_points_for_interp_rx[:,0][:2],bounds_error=False, fill_value='extrapolate')([-2,-1]),interp1d([0,1],ref_points_for_interp_rx[:,1][:2],bounds_error=False, fill_value='extrapolate')([-2,-1])]).T.tolist() + ref_points_for_interp_rx.tolist())	# r,z
			ref_points_for_interp_vrx = np.array(ref_points_for_interp_vrx.tolist() + np.array([interp1d([0,1],ref_points_for_interp_vrx[:,0][-2:],bounds_error=False, fill_value='extrapolate')([2,3]),interp1d([0,1],ref_points_for_interp_vrx[:,1][-2:],bounds_error=False, fill_value='extrapolate')([2,3])]).T.tolist())	# r,z
			ref_points_for_interp_vrx = np.array(np.array([interp1d([0,1],ref_points_for_interp_vrx[:,0][:2],bounds_error=False, fill_value='extrapolate')([-2,-1]),interp1d([0,1],ref_points_for_interp_vrx[:,1][:2],bounds_error=False, fill_value='extrapolate')([-2,-1])]).T.tolist() + ref_points_for_interp_vrx.tolist())	# r,z
			coord_smooth_extended = np.array([r_coord_smooth,z_coord_smooth]).T
			coord_smooth_extended = np.array(coord_smooth_extended.tolist() + np.array([interp1d([0,1],coord_smooth_extended[:,0][-2:],bounds_error=False, fill_value='extrapolate')([2,3]),interp1d([0,1],coord_smooth_extended[:,1][-2:],bounds_error=False, fill_value='extrapolate')([2,3])]).T.tolist())	# r,z
			coord_smooth_extended = np.array(np.array([interp1d([0,1],coord_smooth_extended[:,0][:2],bounds_error=False, fill_value='extrapolate')([-2,-1]),interp1d([0,1],coord_smooth_extended[:,1][:2],bounds_error=False, fill_value='extrapolate')([-2,-1])]).T.tolist() + coord_smooth_extended.tolist())	# r,z

			# in the outer separatrix the inner side is always good and the outer bad, the reverse in the inner separatrix
			# ref_points_1[:,0] = scipy.signal.savgol_filter(ref_points_1[:,0],5,2)
			# ref_points_1[:,1] = scipy.signal.savgol_filter(ref_points_1[:,1],5,2)
			ref_points_2[:,0] = scipy.signal.savgol_filter(ref_points_2[:,0],7,2)
			ref_points_2[:,1] = scipy.signal.savgol_filter(ref_points_2[:,1],7,2)
			# I select only points that progressively go to the target
			temp = np.logical_and(np.diff(ref_points_1[:,1])<0 , np.diff(ref_points_2[:,1])<0)
			ref_points_1 = np.concatenate([[ref_points_1[0]] , ref_points_1[1:][temp]])
			ref_points_2 = np.concatenate([[ref_points_2[0]] , ref_points_2[1:][temp]])

			# polygon_structure = Polygon(structure_radial_profile[0])
			polygon = Polygon((np.flip(ref_points_1,axis=0).tolist() + ref_points_2.tolist()))
			# selected = np.zeros_like(inverted_data[0]).astype(bool)
			# for i_r in range(len(inversion_R)):
			# 	for i_z in range(len(inversion_Z)):
			# 		# if inversion_Z[i_z]<efit_reconstruction.mag_axis_z[i_efit_time]:
			# 		# 	# I want to exclude brightness coming from the inner separatrix movement
			# 		# 	if inversion_Z[i_z]<efit_reconstruction.lower_xpoint_z[i_efit_time] and inversion_R[i_r]>efit_reconstruction.lower_xpoint_r[i_efit_time]:
			# 		point = Point((inversion_R[i_r],inversion_Z[i_z]))
			# 		if polygon.contains(point):
			# 			selected[i_r,i_z] = True
			selected = select_cells_inside_polygon(polygon,[inversion_R,inversion_Z],center_line=coord_smooth_extended)
			# selected[inversion_Z<efit_reconstruction.mag_axis_z[i_efit_time]] = False
			selected_sigma = select_cells_inside_polygon(polygon,[voxels_centre])
			# selected_sigma[voxels_centre[:,1]<efit_reconstruction.mag_axis_z[i_efit_time]] = False

			temp = cp.deepcopy(inverted_data[i_t])
			temp1 = cp.deepcopy(inverted_data[i_t])
			temp1[np.isnan(temp1)] = 0
			inverted_data_interpolator = RegularGridInterpolator((inversion_R, inversion_Z), temp1,bounds_error=False,fill_value=0)

			# temp_sigma = inverted_data_sigma[i_t]
			# temp[temp<0] = 0
			emissivity_baricentre = [(np.nansum(selected*temp*(b**2))/np.nansum(selected*temp))**0.5,np.nansum(selected*temp*a)/np.nansum(selected*temp)]
			L_poloidal_baricentre = L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,emissivity_baricentre,efit_data,i_efit_time)

			emissivity_peak = np.unravel_index(np.nanargmax(selected*inverted_data[i_t]),np.shape(selected))
			emissivity_peak = [inversion_R[emissivity_peak[0]],inversion_Z[emissivity_peak[1]]]
			L_poloidal_peak = L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,emissivity_peak,efit_data,i_efit_time)
			L_poloidal_x_point = L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,[efit_reconstruction.lower_xpoint_r[i_efit_time],efit_reconstruction.lower_xpoint_z[i_efit_time]],efit_data,i_efit_time)

			# selecting the outer leg
			selected[inversion_R<efit_reconstruction.lower_xpoint_r[i_efit_time]] = False
			selected[:,inversion_Z>efit_reconstruction.lower_xpoint_z[i_efit_time]] = False
			# excluding the x-point
			selected[((np.meshgrid(inversion_Z,inversion_R)[0]-efit_reconstruction.lower_xpoint_z[i_efit_time])**2 + (np.meshgrid(inversion_Z,inversion_R)[1]-efit_reconstruction.lower_xpoint_r[i_efit_time])**2)**0.5<x_point_region_radious] = False
			selected_sigma[voxels_centre[:,0]<efit_reconstruction.lower_xpoint_r[i_efit_time]] = False
			selected_sigma[voxels_centre[:,1]>efit_reconstruction.lower_xpoint_z[i_efit_time]] = False
			selected_sigma[((voxels_centre[:,1]-efit_reconstruction.lower_xpoint_z[i_efit_time])**2 + (voxels_centre[:,0]-efit_reconstruction.lower_xpoint_r[i_efit_time])**2)**0.5<x_point_region_radious] = False

			emissivity_peak_only_leg = np.unravel_index(np.nanargmax(selected*inverted_data[i_t]),np.shape(selected))
			emissivity_peak_only_leg = [inversion_R[emissivity_peak_only_leg[0]],inversion_Z[emissivity_peak_only_leg[1]]]
			L_poloidal_peak_only_leg = L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,emissivity_peak_only_leg,efit_data,i_efit_time)
			emissivity_baricentre_only_leg = [(np.nansum(selected*temp*(b**2))/np.nansum(selected*temp))**0.5,np.nansum(selected*temp*a)/np.nansum(selected*temp)]
			L_poloidal_baricentre_only_leg = L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,emissivity_baricentre_only_leg,efit_data,i_efit_time)

			L_poloidal_midplane = L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,np.array([r_coord_smooth_origial[np.abs(r_coord_smooth_origial-0).argmin()],z_coord_smooth_origial[np.abs(r_coord_smooth_origial-0).argmin()]]),efit_data,i_efit_time)

			# selecting the part of the leg that is away from the region of power unreliability
			temp1 = np.ones_like(selected).astype(bool)
			temp1[z_<MU01_sxd_region_delimiter(r_)] = 0	# delimiter as used in the thesis
			selected = np.logical_and(selected,temp1)
			temp1 = np.ones_like(selected_sigma).astype(bool)
			temp1[voxels_centre[:,1]<MU01_sxd_region_delimiter(voxels_centre[:,0])] = False	# delimiter as used in the thesis
			selected_sigma = np.logical_and(selected_sigma,temp1)

			leg_reliable_power = np.nansum((temp.T*inversion_R).T*selected*2*np.pi*(grid_resolution**2))
			temp_select = selected_sigma*2*np.pi*(grid_resolution**2)*(voxels_centre[:,0])
			leg_reliable_power_sigma = np.nansum((covariance_out[i_t][:len(grid_data_masked_crop),:len(grid_data_masked_crop)]*temp_select).T*temp_select)**0.5
			# leg_reliable_power_sigma = np.nansum(((temp_sigma.T*inversion_R).T*selected*2*np.pi*(grid_resolution**2))**2)**0.5

			# selecting only the x-point
			selected = np.zeros_like(inverted_data[0]).astype(bool)
			selected[((np.meshgrid(inversion_Z,inversion_R)[0]-efit_reconstruction.lower_xpoint_z[i_efit_time])**2 + (np.meshgrid(inversion_Z,inversion_R)[1]-efit_reconstruction.lower_xpoint_r[i_efit_time])**2)**0.5<=x_point_region_radious] = True
			x_point_power = np.nansum((temp.T*inversion_R).T*selected*2*np.pi*(grid_resolution**2))
			selected_sigma = np.zeros_like(voxels_centre[:,0]).astype(bool)
			selected_sigma[((voxels_centre[:,1]-efit_reconstruction.lower_xpoint_z[i_efit_time])**2 + (voxels_centre[:,0]-efit_reconstruction.lower_xpoint_r[i_efit_time])**2)**0.5<=x_point_region_radious] = True
			temp_select = selected_sigma*2*np.pi*(grid_resolution**2)*(voxels_centre[:,0])
			x_point_power_sigma = np.nansum((covariance_out[i_t][:len(grid_data_masked_crop),:len(grid_data_masked_crop)]*temp_select).T*temp_select)**0.5

			# selecting the part of the leg that is inside the region of power unreliability
			selected = np.zeros_like(inverted_data[0]).astype(bool)
			selected[z_<MU01_sxd_region_delimiter(r_)] = True	# delimiter as used in the thesis
			sxd_tot_rad_power = np.nansum((temp.T*inversion_R).T*selected*2*np.pi*(grid_resolution**2))
			selected_sigma = np.zeros_like(voxels_centre[:,0]).astype(bool)
			selected_sigma[voxels_centre[:,1]<MU01_sxd_region_delimiter(voxels_centre[:,0])] = True	# delimiter as used in the thesis
			temp_select = selected_sigma*2*np.pi*(grid_resolution**2)*(voxels_centre[:,0])
			sxd_tot_rad_power_sigma = np.nansum((covariance_out[i_t][:len(grid_data_masked_crop),:len(grid_data_masked_crop)]*temp_select).T*temp_select)**0.5

			# selecting the volume of the SXD that coincides with the DMS
			selected = np.zeros_like(inverted_data[0]).astype(bool)
			selected[z_<DMS_LOS19_V2_higher_delimiter(r_)] = True
			DMS_equivalent_power = np.nansum((temp.T*inversion_R).T*selected*2*np.pi*(grid_resolution**2))
			selected_sigma = np.zeros_like(voxels_centre[:,0]).astype(bool)
			selected_sigma[voxels_centre[:,1]<DMS_LOS19_V2_higher_delimiter(voxels_centre[:,0])] = True
			temp_select = selected_sigma*2*np.pi*(grid_resolution**2)*(voxels_centre[:,0])
			DMS_equivalent_power_sigma = np.nansum((covariance_out[i_t][:len(grid_data_masked_crop),:len(grid_data_masked_crop)]*temp_select).T*temp_select)**0.5

			# selecting the volume of the SXD that coincides with the DMS
			selected = np.zeros_like(inverted_data[0]).astype(bool)
			selected[z_<MWI_delimiter(r_)] = True
			MWI_equivalent_power = np.nansum((temp.T*inversion_R).T*selected*2*np.pi*(grid_resolution**2))
			selected_sigma = np.zeros_like(voxels_centre[:,0]).astype(bool)
			selected_sigma[voxels_centre[:,1]<MWI_delimiter(voxels_centre[:,0])] = True
			temp_select = selected_sigma*2*np.pi*(grid_resolution**2)*(voxels_centre[:,0])
			MWI_equivalent_power_sigma = np.nansum((covariance_out[i_t][:len(grid_data_masked_crop),:len(grid_data_masked_crop)]*temp_select).T*temp_select)**0.5

			ref_points_1 = cp.deepcopy(ref_points_1_original)
			ref_points_2 = cp.deepcopy(ref_points_2_original)
			# in the outer separatrix the inner side is always good and the outer bad, the reverse in the inner separatrix
			# ref_points_1[:,0] = scipy.signal.savgol_filter(ref_points_1[:,0],5,2)
			# ref_points_1[:,1] = scipy.signal.savgol_filter(ref_points_1[:,1],5,2)
			ref_points_2[:,0] = scipy.signal.savgol_filter(ref_points_2[:,0],7,2)
			ref_points_2[:,1] = scipy.signal.savgol_filter(ref_points_2[:,1],7,2)
			# I want a higher resolution than 10cm. simple approach: interpolate between each ref_points to get a 5cm resolution...!
			ref_points_1 = np.array([np.interp(np.arange(0,len(ref_points_1)-0.5,0.5),np.arange(0,len(ref_points_1)-0.5,1),ref_points_1[:,0]),np.interp(np.arange(0,len(ref_points_1)-0.5,0.5),np.arange(0,len(ref_points_1)-0.5,1),ref_points_1[:,1])]).T
			ref_points_2 = np.array([np.interp(np.arange(0,len(ref_points_2)-0.5,0.5),np.arange(0,len(ref_points_2)-0.5,1),ref_points_2[:,0]),np.interp(np.arange(0,len(ref_points_2)-0.5,0.5),np.arange(0,len(ref_points_2)-0.5,1),ref_points_2[:,1])]).T
			local_mean_emis = []
			local_power = []
			local_L_poloidal = []
			emissivity_flat = inverted_data[i_t].flatten()
			for i_ref_point in range(1,len(ref_points_1)):
				# print(i_ref_point)
				polygon = Polygon([ref_points_1[i_ref_point-1], ref_points_1[i_ref_point], ref_points_2[i_ref_point], ref_points_2[i_ref_point-1]])
				# select = []
				# for i_e in range(len(emissivity_flat)):
				# 	point = Point((b_flat[i_e],a_flat[i_e]))
				# 	select.append(polygon.contains(point))
				select = select_cells_inside_polygon(polygon,[inversion_R,inversion_Z],center_line=coord_smooth_extended).flatten()

				local_mean_emis.append(np.nanmean(emissivity_flat[select]))
				local_power.append(2*np.pi*np.nansum(emissivity_flat[select]*b_flat[select]*(grid_resolution**2)))
				local_L_poloidal.append(L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,[np.mean(b_flat[select]),np.mean(a_flat[select])],efit_data,i_efit_time))

			# section to find the half peak
			# this is midplane to target
			ref1 = inverted_data_interpolator((ref_points_for_interp_vlx[:,0],ref_points_for_interp_vlx[:,1]))
			ref2 = inverted_data_interpolator((ref_points_for_interp_lx[:,0],ref_points_for_interp_lx[:,1]))
			ref3 = inverted_data_interpolator((coord_smooth_extended[:,0],coord_smooth_extended[:,1]))
			ref4 = inverted_data_interpolator((ref_points_for_interp_rx[:,0],ref_points_for_interp_rx[:,1]))
			ref5 = inverted_data_interpolator((ref_points_for_interp_vrx[:,0],ref_points_for_interp_vrx[:,1]))
			# midpoint_ref1 = (np.arange(len(ref1))[ref1>np.nanmax(ref1)*0.5])[-1]
			# midpoint_ref2 = (np.arange(len(ref2))[ref1>np.nanmax(ref2)*0.5])[-1]
			# midpoint_ref3 = (np.arange(len(ref3))[ref2>np.nanmax(ref3)*0.5])[-1]
			# midpoint_ref4 = (np.arange(len(ref4))[ref3>np.nanmax(ref4)*0.5])[-1]
			# midpoint_ref5 = (np.arange(len(ref5))[ref3>np.nanmax(ref5)*0.5])[-1]
			[midpoint_ref1,midpoint_ref2,midpoint_ref3,midpoint_ref4,midpoint_ref5] = [ [(np.arange(len(ref))[ref>np.nanmax(ref)*temp])[-1] for temp in [0.5-midpoint_range,0.5,0.5+midpoint_range]]  for ref in [ref1,ref2,ref3,ref4,ref5] ]
			ref1=ref_points_for_interp_vlx[midpoint_ref1]
			ref2=ref_points_for_interp_lx[midpoint_ref2]
			ref3=coord_smooth_extended[midpoint_ref3]
			ref4=ref_points_for_interp_rx[midpoint_ref4]
			ref5=ref_points_for_interp_vrx[midpoint_ref5]
			# half_peak = np.array([np.median([ref1[0],ref2[0],ref3[0],ref4[0],ref5[0]]),np.median([ref1[1],ref2[1],ref3[1],ref4[1],ref5[1]])])
			half_peak = [np.array([np.median([ref1[i][0],ref2[i][0],ref3[i][0],ref4[i][0],ref5[i][0]]),np.median([ref1[i][1],ref2[i][1],ref3[i][1],ref4[i][1],ref5[i][1]])]) for i in range(len(ref1))]
			# half_peak_L_pol = L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,half_peak,efit_data,i_efit_time)
			half_peak_L_pol = [ L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,pos,efit_data,i_efit_time) for pos in half_peak ]

			# this is x-point to target
			ref1 = inverted_data_interpolator((ref_points_for_interp_vlx[:,0][ref_points_for_interp_vlx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]],ref_points_for_interp_vlx[:,1][ref_points_for_interp_vlx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]]))
			ref2 = inverted_data_interpolator((ref_points_for_interp_lx[:,0][ref_points_for_interp_lx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]],ref_points_for_interp_lx[:,1][ref_points_for_interp_lx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]]))
			ref3 = inverted_data_interpolator((coord_smooth_extended[:,0][coord_smooth_extended[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]],coord_smooth_extended[:,1][coord_smooth_extended[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]]))
			ref4 = inverted_data_interpolator((ref_points_for_interp_rx[:,0][ref_points_for_interp_rx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]],ref_points_for_interp_rx[:,1][ref_points_for_interp_rx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]]))
			ref5 = inverted_data_interpolator((ref_points_for_interp_vrx[:,0][ref_points_for_interp_vrx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]],ref_points_for_interp_vrx[:,1][ref_points_for_interp_vrx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]]))
			# midpoint_ref1 = (np.arange(len(ref1))[ref1>np.nanmax(ref1)/2])[-1]
			# midpoint_ref2 = (np.arange(len(ref2))[ref2>np.nanmax(ref2)/2])[-1]
			# midpoint_ref3 = (np.arange(len(ref3))[ref3>np.nanmax(ref3)/2])[-1]
			# midpoint_ref4 = (np.arange(len(ref4))[ref4>np.nanmax(ref4)/2])[-1]
			# midpoint_ref5 = (np.arange(len(ref5))[ref5>np.nanmax(ref5)/2])[-1]
			[midpoint_ref1,midpoint_ref2,midpoint_ref3,midpoint_ref4,midpoint_ref5] = [ [(np.arange(len(ref))[ref>np.nanmax(ref)*temp])[-1] for temp in [0.5-midpoint_range,0.5,0.5+midpoint_range]]  for ref in [ref1,ref2,ref3,ref4,ref5] ]
			ref1=ref_points_for_interp_vlx[ref_points_for_interp_vlx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]][midpoint_ref1]
			ref2=ref_points_for_interp_lx[ref_points_for_interp_lx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]][midpoint_ref2]
			ref3=coord_smooth_extended[coord_smooth_extended[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]][midpoint_ref3]
			ref4=ref_points_for_interp_rx[ref_points_for_interp_rx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]][midpoint_ref4]
			ref5=ref_points_for_interp_vrx[ref_points_for_interp_vrx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]][midpoint_ref5]
			# half_peak_divertor = np.array([np.median([ref1[0],ref2[0],ref3[0],ref4[0],ref5[0]]),np.median([ref1[1],ref2[1],ref3[1],ref4[1],ref5[1]])])
			half_peak_divertor = [np.array([np.median([ref1[i][0],ref2[i][0],ref3[i][0],ref4[i][0],ref5[i][0]]),np.median([ref1[i][1],ref2[i][1],ref3[i][1],ref4[i][1],ref5[i][1]])]) for i in range(len(ref1))]
			# half_peak_divertor_L_pol = L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,half_peak_divertor,efit_data,i_efit_time)
			half_peak_divertor_L_pol = [ L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,pos,efit_data,i_efit_time) for pos in half_peak_divertor ]

			# local_mean_emis = np.array(local_mean_emis)
			# local_power = np.array(local_power)
			# local_mean_emis = local_mean_emis[np.logical_not(np.isnan(local_mean_emis))].tolist()
			# local_power = local_power[np.logical_not(np.isnan(local_power))].tolist()
			emissivity_baricentre_all.append(emissivity_baricentre)
			L_poloidal_baricentre_all.append(L_poloidal_baricentre)
			emissivity_peak_all.append(emissivity_peak)
			L_poloidal_peak_all.append(L_poloidal_peak)
			L_poloidal_peak_only_leg_all.append(L_poloidal_peak_only_leg)
			L_poloidal_baricentre_only_leg_all.append(L_poloidal_baricentre_only_leg)
			L_poloidal_x_point_all.append(L_poloidal_x_point)
			L_poloidal_midplane_all.append(L_poloidal_midplane)
			leg_reliable_power_all.append(leg_reliable_power)
			leg_reliable_power_sigma_all.append(leg_reliable_power_sigma)
			DMS_equivalent_power_all.append(DMS_equivalent_power)
			DMS_equivalent_power_sigma_all.append(DMS_equivalent_power_sigma)
			MWI_equivalent_power_all.append(MWI_equivalent_power)
			MWI_equivalent_power_sigma_all.append(MWI_equivalent_power_sigma)
			x_point_power_all.append(x_point_power)
			x_point_power_sigma_all.append(x_point_power_sigma)
			sxd_tot_rad_power_all.append(sxd_tot_rad_power)
			sxd_tot_rad_power_sigma_all.append(sxd_tot_rad_power_sigma)
			local_mean_emis_all.append(local_mean_emis)
			local_power_all.append(local_power)
			local_L_poloidal_all.append(local_L_poloidal)
			data_length = max(data_length,len(local_power))
			leg_length_all.append(leg_length)
			leg_length_interval_all.append(leg_length_interval)
			half_peak_L_pol_all.append(half_peak_L_pol)
			half_peak_divertor_L_pol_all.append(half_peak_divertor_L_pol)
		except Exception as e:
			print('failed')
			# logging.exception('with error: ' + str(e))
			local_mean_emis_all.append([])
			local_power_all.append([])
			local_L_poloidal_all.append([])
			leg_length_interval_all.append([])
			half_peak_L_pol_all.append([0,0,0])
			half_peak_divertor_L_pol_all.append([0,0,0])
			leg_length_all.append(0)
			emissivity_baricentre_all.append([np.nan,np.nan])
			emissivity_peak_all.append([np.nan,np.nan])
			L_poloidal_baricentre_all.append(0)
			L_poloidal_peak_all.append(0)
			L_poloidal_peak_only_leg_all.append(0)
			L_poloidal_baricentre_only_leg_all.append(0)
			L_poloidal_x_point_all.append(np.inf)
			L_poloidal_midplane_all.append(np.inf)
			leg_reliable_power_all.append(0)
			leg_reliable_power_sigma_all.append(0)
			DMS_equivalent_power_all.append(0)
			DMS_equivalent_power_sigma_all.append(0)
			MWI_equivalent_power_all.append(0)
			MWI_equivalent_power_sigma_all.append(0)
			x_point_power_all.append(0)
			x_point_power_sigma_all.append(0)
			sxd_tot_rad_power_all.append(0)
			sxd_tot_rad_power_sigma_all.append(0)
	emissivity_baricentre_all = np.array(emissivity_baricentre_all)
	emissivity_peak_all = np.array(emissivity_peak_all)
	L_poloidal_baricentre_all = np.array(L_poloidal_baricentre_all)
	L_poloidal_peak_all = np.array(L_poloidal_peak_all)
	L_poloidal_peak_only_leg_all = np.array(L_poloidal_peak_only_leg_all)
	L_poloidal_baricentre_only_leg_all = np.array(L_poloidal_baricentre_only_leg_all)
	L_poloidal_x_point_all = np.array(L_poloidal_x_point_all)
	L_poloidal_midplane_all = np.array(L_poloidal_midplane_all)
	leg_reliable_power_all = np.array(leg_reliable_power_all)
	leg_reliable_power_sigma_all = np.array(leg_reliable_power_sigma_all)
	DMS_equivalent_power_all = np.array(DMS_equivalent_power_all)
	DMS_equivalent_power_sigma_all = np.array(DMS_equivalent_power_sigma_all)
	MWI_equivalent_power_all = np.array(MWI_equivalent_power_all)
	MWI_equivalent_power_sigma_all = np.array(MWI_equivalent_power_sigma_all)
	x_point_power_all = np.array(x_point_power_all)
	x_point_power_sigma_all = np.array(x_point_power_sigma_all)
	sxd_tot_rad_power_all = np.array(sxd_tot_rad_power_all)
	sxd_tot_rad_power_sigma_all = np.array(sxd_tot_rad_power_sigma_all)

	for i_t in range(len(time_full_binned_crop)):
		if len(local_mean_emis_all[i_t])<data_length:
			local_mean_emis_all[i_t].extend([0]*(data_length-len(local_mean_emis_all[i_t])))
			local_power_all[i_t].extend([0]*(data_length-len(local_power_all[i_t])))
			local_L_poloidal_all[i_t].extend([0]*(data_length-len(local_L_poloidal_all[i_t])))
			leg_length_interval_all[i_t].extend([0]*(data_length-len(leg_length_interval_all[i_t])))

	return local_mean_emis_all,local_power_all,local_L_poloidal_all,leg_length_interval_all,leg_length_all,data_length,leg_resolution,emissivity_baricentre_all,emissivity_peak_all,L_poloidal_baricentre_all,L_poloidal_peak_all,L_poloidal_peak_only_leg_all,L_poloidal_baricentre_only_leg_all,greenwald_density,dr_sep_in,dr_sep_out,L_poloidal_x_point_all,L_poloidal_midplane_all,leg_reliable_power_all,leg_reliable_power_sigma_all,DMS_equivalent_power_all,DMS_equivalent_power_sigma_all,MWI_equivalent_power_all,MWI_equivalent_power_sigma_all,x_point_power_all,x_point_power_sigma_all,sxd_tot_rad_power_all,sxd_tot_rad_power_sigma_all,half_peak_L_pol_all,half_peak_divertor_L_pol_all,sideways_leg_resolution


def expand_line_sideways(r_coord_smooth,z_coord_smooth,i_ref_points,sideways_leg_resolution):
	ref_points_1 = []
	try:
		m = -1/((z_coord_smooth[i_ref_points[0]]-z_coord_smooth[i_ref_points[0]+1])/(r_coord_smooth[i_ref_points[0]]-r_coord_smooth[i_ref_points[0]+1]))
		ref_points_1.append([r_coord_smooth[i_ref_points[0]] + sideways_leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[0]] + m*sideways_leg_resolution/((1+m**2)**0.5)])
	except:
		pass
	for i_ref_point in range(1,len(i_ref_points)):
		try:
			m = -1/((z_coord_smooth[i_ref_points[i_ref_point]-1]-z_coord_smooth[i_ref_points[i_ref_point]+1])/(r_coord_smooth[i_ref_points[i_ref_point]-1]-r_coord_smooth[i_ref_points[i_ref_point]+1]))
			ref_points_1.append([r_coord_smooth[i_ref_points[i_ref_point]] + sideways_leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[i_ref_point]] + m*sideways_leg_resolution/((1+m**2)**0.5)])
		except:
			pass
	try:
		m = -1/((z_coord_smooth[i_ref_points[-1]-1]-z_coord_smooth[i_ref_points[-1]])/(r_coord_smooth[i_ref_points[-1]-1]-r_coord_smooth[i_ref_points[-1]]))
		ref_points_1.append([r_coord_smooth[i_ref_points[-1]] + sideways_leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[-1]] + m*sideways_leg_resolution/((1+m**2)**0.5)])
	except:
		pass
	ref_points_1 = np.array(ref_points_1)
	return ref_points_1

def baricentre_inner_separatrix_radiation(inverted_data,inverted_data_sigma,inversion_R,inversion_Z,time_full_binned_crop,efit_reconstruction,covariance_out,grid_data_masked_crop,leg_resolution = 0.1,sideways_leg_resolution=0.1,x_point_region_radious=0.1,sideways_leg_resolution_for_interp=0.02,starting_distance_find_separatrix=0.02,midpoint_range=0.2):
	from shapely.geometry import Point
	from shapely.geometry.polygon import Polygon
	from scipy.ndimage import generic_filter
	import mastu_exhaust_analysis.fluxsurface_tracer as ft
	from mastu_exhaust_analysis.read_efit import read_epm
	from scipy.interpolate import interp1d
	from scipy.interpolate import RegularGridInterpolator

	all_time_sep_r,all_time_sep_z,r_fine,z_fine = efit_reconstruction_to_separatrix_on_foil(efit_reconstruction)	# the separatrix are ordered as left_of_up_xpoints,right_of_up_xpoints,left_of_low_xpoints,right_of_low_xpoints
	all_time_strike_points_location = return_all_time_strike_points_location_radial(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine)
	fdir = uda_transfer(efit_reconstruction.shotnumber,'epm')
	efit_data = read_epm(fdir,calc_bfield=True)
	os.remove(fdir)
	greenwald_density = efit_data['ngw']
	greenwald_density = np.interp(time_full_binned_crop,efit_reconstruction.time,greenwald_density,left=np.nan,right=np.nan)
	dr_sep_in = efit_data['dr_sep_in']
	dr_sep_in = np.interp(time_full_binned_crop,efit_reconstruction.time,dr_sep_in,left=np.nan,right=np.nan)
	dr_sep_out = efit_data['dr_sep_out']
	dr_sep_out = np.interp(time_full_binned_crop,efit_reconstruction.time,dr_sep_out,left=np.nan,right=np.nan)

	dr = np.median(np.diff(inversion_R))
	dz = np.median(np.diff(inversion_Z))
	a,b = np.meshgrid(inversion_Z,inversion_R)
	a_flat = a.flatten()
	b_flat = b.flatten()
	grid_resolution = np.median(np.concatenate([np.diff(inversion_Z),np.diff(inversion_R)]))
	# leg_resolution = 0.1	# m
	# structure_radial_profile = return_structure_radial_profile()
	z_,r_ = np.meshgrid(inversion_Z,inversion_R)
	MU01_sxd_region_delimiter = return_MU01_sxd_region_delimiter()

	voxels_centre = np.mean(grid_data_masked_crop,axis=1)

	data_length = 0
	local_mean_emis_all = []
	local_power_all = []
	local_L_poloidal_all = []
	leg_length_all = []
	leg_length_interval_all = []
	half_peak_L_pol_all = []
	half_peak_divertor_L_pol_all = []
	emissivity_baricentre_all = []
	emissivity_peak_all = []
	L_poloidal_baricentre_all = []
	L_poloidal_peak_all = []
	L_poloidal_peak_only_leg_all = []
	L_poloidal_baricentre_only_leg_all = []
	L_poloidal_x_point_all = []
	L_poloidal_midplane_all = []
	leg_reliable_power_all = []
	leg_reliable_power_sigma_all = []
	for i_t in range(len(time_full_binned_crop)):
		print('starting t=%.4gms' %(time_full_binned_crop[i_t]*1e3))
		try:
			i_efit_time = np.abs(efit_reconstruction.time-time_full_binned_crop[i_t]).argmin()
			try:
				distance_find_separatrix = cp.deepcopy(starting_distance_find_separatrix)
				r_coord_smooth_int = np.array([2,2])
				while np.sum(r_coord_smooth_int>efit_data['lower_xpoint_r'][i_efit_time])>0 and distance_find_separatrix<0.2:
					trace_flux_surface = ft.trace_flux_surface(efit_data, i_efit_time,efit_data['lower_xpoint_r'][i_efit_time]-distance_find_separatrix,efit_data['lower_xpoint_z'][i_efit_time])
					r_coord_smooth = np.array(trace_flux_surface.r)
					z_coord_smooth = np.array(trace_flux_surface.z)
					r_coord_smooth_int = r_coord_smooth[z_coord_smooth<=efit_reconstruction.mag_axis_z[i_efit_time]]	# I add this because it could get confusing for signficant dr_sep
					# print(distance_find_separatrix)
					distance_find_separatrix += 0.001
				if False:
					plt.plot(trace_flux_surface.r,trace_flux_surface.z,'+')
				r_coord_smooth = scipy.signal.savgol_filter(np.array(trace_flux_surface.r),7,2)
				z_coord_smooth = scipy.signal.savgol_filter(np.array(trace_flux_surface.z),7,2)
			except:
				print('skipped to my own method to find the separatrix')
				i_efit_time = np.abs(efit_reconstruction.time-time_full_binned_crop[i_t]).argmin()
				if len(all_time_sep_r[i_efit_time][1])==0 and len(all_time_sep_r[i_efit_time][3])>0:
					i_closer_separatrix_to_x_point = 3
				elif len(all_time_sep_r[i_efit_time][3])==0 and len(all_time_sep_r[i_efit_time][1])>0:
					i_closer_separatrix_to_x_point = 1
				elif len(all_time_sep_r[i_efit_time][3])==0 and len(all_time_sep_r[i_efit_time][1])==0:
					local_mean_emis_all.append([])
					local_power_all.append([])
					local_L_poloidal_all.append([])
					leg_length_all.append(0)
					leg_length_interval_all.append([])
					half_peak_L_pol_all.append([0,0,0])
					half_peak_divertor_L_pol_all.append([0,0,0])
					emissivity_baricentre_all.append([np.nan,np.nan])
					emissivity_peak_all.append([np.nan,np.nan])
					L_poloidal_baricentre_all.append(0)
					L_poloidal_peak_all.append(0)
					L_poloidal_peak_only_leg_all.append(0)
					L_poloidal_baricentre_only_leg_all.append(0)
					L_poloidal_x_point_all.append(np.inf)
					L_poloidal_midplane_all.append(np.inf)
					leg_reliable_power_all.append(0)
					leg_reliable_power_sigma_all.append(0)
					print('skipped t=%.4gms' %(time_full_binned_crop[i_t]*1e3))
					continue
				elif np.abs((r_fine[all_time_sep_r[i_efit_time][0]]- efit_reconstruction.lower_xpoint_r[i_efit_time])**2 + (z_fine[all_time_sep_z[i_efit_time][0]]- efit_reconstruction.lower_xpoint_z[i_efit_time])**2 ).min() < np.abs((r_fine[all_time_sep_r[i_efit_time][2]]- efit_reconstruction.lower_xpoint_r[i_efit_time])**2 + (z_fine[all_time_sep_z[i_efit_time][2]]- efit_reconstruction.lower_xpoint_z[i_efit_time])**2 ).min():
					i_closer_separatrix_to_x_point = 0
				else:
					i_closer_separatrix_to_x_point = 2
				i_where_x_point_is = np.abs((r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]]- efit_reconstruction.lower_xpoint_r[i_efit_time])**2 + (z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]]- efit_reconstruction.lower_xpoint_z[i_efit_time])**2 ).argmin()

				temp = np.array([((all_time_strike_points_location[i_efit_time][0][i] - r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][:i_where_x_point_is])**2 + (all_time_strike_points_location[i_efit_time][1][i] - z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][:i_where_x_point_is])**2).min() for i in range(len(all_time_strike_points_location[i_efit_time][1]))])
				temp[np.isnan(temp)] = np.inf
				i_which_strike_point_is = temp.argmin()
				i_where_strike_point_is = ((all_time_strike_points_location[i_efit_time][0][i_which_strike_point_is] - r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][:i_where_x_point_is])**2 + (all_time_strike_points_location[i_efit_time][1][i_which_strike_point_is] - z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][:i_where_x_point_is])**2).argmin()
				# plt.figure()
				# plt.plot(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is],z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is])
				# plt.pause(0.001)
				# r_coord_smooth = generic_filter(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is],np.mean,size=11)
				# z_coord_smooth = generic_filter(z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:i_where_x_point_is],np.mean,size=11)
				r_coord_smooth = scipy.signal.savgol_filter(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:],max(11,int(len(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:])/50//2*2+1)),2)
				z_coord_smooth = scipy.signal.savgol_filter(z_fine[all_time_sep_z[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:],max(11,int(len(r_fine[all_time_sep_r[i_efit_time][i_closer_separatrix_to_x_point]][i_where_strike_point_is:])/50//2*2+1)),2)
			r_coord_smooth = r_coord_smooth[z_coord_smooth<efit_reconstruction.mag_axis_z[i_efit_time]]
			z_coord_smooth = z_coord_smooth[z_coord_smooth<efit_reconstruction.mag_axis_z[i_efit_time]]
			z_coord_smooth = z_coord_smooth[r_coord_smooth<efit_reconstruction.lower_xpoint_r[i_efit_time]]
			r_coord_smooth = r_coord_smooth[r_coord_smooth<efit_reconstruction.lower_xpoint_r[i_efit_time]]
			r_coord_smooth_origial = cp.deepcopy(r_coord_smooth)
			z_coord_smooth_origial = cp.deepcopy(z_coord_smooth)
			leg_length = np.sum((np.diff(r_coord_smooth)**2 + np.diff(z_coord_smooth)**2)**0.5)
			# I arbitrarily decide to cut the leg in 10cm pieces
			leg_length_interval = [0]
			target_length = 0 + leg_resolution
			i_ref_points = [0]
			ref_points = [[r_coord_smooth[0],z_coord_smooth[0]]]
			while target_length < leg_length + leg_resolution:
				# print(target_length)
				temp = np.abs(np.cumsum((np.diff(r_coord_smooth)**2 + np.diff(z_coord_smooth)**2)**0.5) - target_length).argmin()
				leg_length_interval.append(np.cumsum((np.diff(r_coord_smooth)**2 + np.diff(z_coord_smooth)**2)**0.5)[temp] - np.sum(leg_length_interval))
				i_ref_points.append(temp+1)
				ref_points.append([r_coord_smooth[temp+1],z_coord_smooth[temp+1]])
				target_length += leg_resolution
			ref_points = np.array(ref_points)
			leg_length_interval = leg_length_interval[1:]
			# I want to eliminate doubles
			i_ref_points = np.concatenate([[i_ref_points[0]],np.array(i_ref_points[1:])[np.abs(np.diff(i_ref_points))>0]])
			ref_points = np.concatenate([[ref_points[0]],ref_points[1:][np.abs(np.diff(ref_points[:,0]))+np.abs(np.diff(ref_points[:,1]))>0]])
			leg_length_interval = np.array(leg_length_interval)[np.array(leg_length_interval)>0].tolist()

			ref_points_1 = expand_line_sideways(r_coord_smooth,z_coord_smooth,i_ref_points,-sideways_leg_resolution)
			ref_points_2 = expand_line_sideways(r_coord_smooth,z_coord_smooth,i_ref_points,+sideways_leg_resolution)
			ref_points_for_interp_vlx = expand_line_sideways(r_coord_smooth,z_coord_smooth,np.arange(len(r_coord_smooth)),-sideways_leg_resolution_for_interp)
			ref_points_for_interp_lx = expand_line_sideways(r_coord_smooth,z_coord_smooth,np.arange(len(r_coord_smooth)),-sideways_leg_resolution_for_interp/2)
			ref_points_for_interp_rx = expand_line_sideways(r_coord_smooth,z_coord_smooth,np.arange(len(r_coord_smooth)),+sideways_leg_resolution_for_interp/2)
			ref_points_for_interp_vrx = expand_line_sideways(r_coord_smooth,z_coord_smooth,np.arange(len(r_coord_smooth)),+sideways_leg_resolution_for_interp)

			# ref_points_1 = []
			# ref_points_2 = []
			# try:
			# 	m = -1/((z_coord_smooth[i_ref_points[0]]-z_coord_smooth[i_ref_points[0]+1])/(r_coord_smooth[i_ref_points[0]]-r_coord_smooth[i_ref_points[0]+1]))
			# 	ref_points_1.append([r_coord_smooth[i_ref_points[0]] - sideways_leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[0]] - m*sideways_leg_resolution/((1+m**2)**0.5)])
			# 	ref_points_2.append([r_coord_smooth[i_ref_points[0]] + sideways_leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[0]] + m*sideways_leg_resolution/((1+m**2)**0.5)])
			# except:
			# 	pass
			# for i_ref_point in range(1,len(i_ref_points)):
			# 	try:
			# 		m = -1/((z_coord_smooth[i_ref_points[i_ref_point]-1]-z_coord_smooth[i_ref_points[i_ref_point]+1])/(r_coord_smooth[i_ref_points[i_ref_point]-1]-r_coord_smooth[i_ref_points[i_ref_point]+1]))
			# 		ref_points_1.append([r_coord_smooth[i_ref_points[i_ref_point]] - sideways_leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[i_ref_point]] - m*sideways_leg_resolution/((1+m**2)**0.5)])
			# 		ref_points_2.append([r_coord_smooth[i_ref_points[i_ref_point]] + sideways_leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[i_ref_point]] + m*sideways_leg_resolution/((1+m**2)**0.5)])
			# 	except:
			# 		pass
			# try:
			# 	m = -1/((z_coord_smooth[i_ref_points[-1]-1]-z_coord_smooth[i_ref_points[-1]])/(r_coord_smooth[i_ref_points[-1]-1]-r_coord_smooth[i_ref_points[-1]]))
			# 	ref_points_1.append([r_coord_smooth[i_ref_points[-1]] - sideways_leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[-1]] - m*sideways_leg_resolution/((1+m**2)**0.5)])
			# 	ref_points_2.append([r_coord_smooth[i_ref_points[-1]] + sideways_leg_resolution/((1+m**2)**0.5) , z_coord_smooth[i_ref_points[-1]] + m*sideways_leg_resolution/((1+m**2)**0.5)])
			# except:
			# 	pass
			# ref_points_1 = np.array(ref_points_1)
			# ref_points_2 = np.array(ref_points_2)

			ref_points_1_original = cp.deepcopy(ref_points_1)
			ref_points_2_original = cp.deepcopy(ref_points_2)
			# I want to be sure to include all the radiation close to the target so I prolonung the references of 2 steps
			ref_points_1 = np.array(ref_points_1.tolist() + np.array([interp1d([0,1],ref_points_1[:,0][-2:],bounds_error=False, fill_value='extrapolate')([2,3]),interp1d([0,1],ref_points_1[:,1][-2:],bounds_error=False, fill_value='extrapolate')([2,3])]).T.tolist())	# r,z
			ref_points_1 = np.array(np.array([interp1d([0,1],ref_points_1[:,0][:2],bounds_error=False, fill_value='extrapolate')([-2,-1]),interp1d([0,1],ref_points_1[:,1][:2],bounds_error=False, fill_value='extrapolate')([-2,-1])]).T.tolist() + ref_points_1.tolist())	# r,z
			ref_points_2 = np.array(ref_points_2.tolist() + np.array([interp1d([0,1],ref_points_2[:,0][-2:],bounds_error=False, fill_value='extrapolate')([2,3]),interp1d([0,1],ref_points_2[:,1][-2:],bounds_error=False, fill_value='extrapolate')([2,3])]).T.tolist())	# r,z
			ref_points_2 = np.array(np.array([interp1d([0,1],ref_points_2[:,0][:2],bounds_error=False, fill_value='extrapolate')([-2,-1]),interp1d([0,1],ref_points_2[:,1][:2],bounds_error=False, fill_value='extrapolate')([-2,-1])]).T.tolist() + ref_points_2.tolist())	# r,z
			ref_points_for_interp_vlx = np.array(ref_points_for_interp_vlx.tolist() + np.array([interp1d([0,1],ref_points_for_interp_vlx[:,0][-2:],bounds_error=False, fill_value='extrapolate')([2,3]),interp1d([0,1],ref_points_for_interp_vlx[:,1][-2:],bounds_error=False, fill_value='extrapolate')([2,3])]).T.tolist())	# r,z
			ref_points_for_interp_vlx = np.array(np.array([interp1d([0,1],ref_points_for_interp_vlx[:,0][:2],bounds_error=False, fill_value='extrapolate')([-2,-1]),interp1d([0,1],ref_points_for_interp_vlx[:,1][:2],bounds_error=False, fill_value='extrapolate')([-2,-1])]).T.tolist() + ref_points_for_interp_vlx.tolist())	# r,z
			ref_points_for_interp_lx = np.array(ref_points_for_interp_lx.tolist() + np.array([interp1d([0,1],ref_points_for_interp_lx[:,0][-2:],bounds_error=False, fill_value='extrapolate')([2,3]),interp1d([0,1],ref_points_for_interp_lx[:,1][-2:],bounds_error=False, fill_value='extrapolate')([2,3])]).T.tolist())	# r,z
			ref_points_for_interp_lx = np.array(np.array([interp1d([0,1],ref_points_for_interp_lx[:,0][:2],bounds_error=False, fill_value='extrapolate')([-2,-1]),interp1d([0,1],ref_points_for_interp_lx[:,1][:2],bounds_error=False, fill_value='extrapolate')([-2,-1])]).T.tolist() + ref_points_for_interp_lx.tolist())	# r,z
			ref_points_for_interp_rx = np.array(ref_points_for_interp_rx.tolist() + np.array([interp1d([0,1],ref_points_for_interp_rx[:,0][-2:],bounds_error=False, fill_value='extrapolate')([2,3]),interp1d([0,1],ref_points_for_interp_rx[:,1][-2:],bounds_error=False, fill_value='extrapolate')([2,3])]).T.tolist())	# r,z
			ref_points_for_interp_rx = np.array(np.array([interp1d([0,1],ref_points_for_interp_rx[:,0][:2],bounds_error=False, fill_value='extrapolate')([-2,-1]),interp1d([0,1],ref_points_for_interp_rx[:,1][:2],bounds_error=False, fill_value='extrapolate')([-2,-1])]).T.tolist() + ref_points_for_interp_rx.tolist())	# r,z
			ref_points_for_interp_vrx = np.array(ref_points_for_interp_vrx.tolist() + np.array([interp1d([0,1],ref_points_for_interp_vrx[:,0][-2:],bounds_error=False, fill_value='extrapolate')([2,3]),interp1d([0,1],ref_points_for_interp_vrx[:,1][-2:],bounds_error=False, fill_value='extrapolate')([2,3])]).T.tolist())	# r,z
			ref_points_for_interp_vrx = np.array(np.array([interp1d([0,1],ref_points_for_interp_vrx[:,0][:2],bounds_error=False, fill_value='extrapolate')([-2,-1]),interp1d([0,1],ref_points_for_interp_vrx[:,1][:2],bounds_error=False, fill_value='extrapolate')([-2,-1])]).T.tolist() + ref_points_for_interp_vrx.tolist())	# r,z
			coord_smooth_extended = np.array([r_coord_smooth,z_coord_smooth]).T
			coord_smooth_extended = np.array(coord_smooth_extended.tolist() + np.array([interp1d([0,1],coord_smooth_extended[:,0][-2:],bounds_error=False, fill_value='extrapolate')([2,3]),interp1d([0,1],coord_smooth_extended[:,1][-2:],bounds_error=False, fill_value='extrapolate')([2,3])]).T.tolist())	# r,z
			coord_smooth_extended = np.array(np.array([interp1d([0,1],coord_smooth_extended[:,0][:2],bounds_error=False, fill_value='extrapolate')([-2,-1]),interp1d([0,1],coord_smooth_extended[:,1][:2],bounds_error=False, fill_value='extrapolate')([-2,-1])]).T.tolist() + coord_smooth_extended.tolist())	# r,z

			# in the outer separatrix the inner side is always good and the outer bad, the reverse in the inner separatrix
			ref_points_1[:,0] = scipy.signal.savgol_filter(ref_points_1[:,0],7,2)
			ref_points_1[:,1] = scipy.signal.savgol_filter(ref_points_1[:,1],7,2)
			# ref_points_2[:,0] = scipy.signal.savgol_filter(ref_points_2[:,0],7,2)
			# ref_points_2[:,1] = scipy.signal.savgol_filter(ref_points_2[:,1],7,2)
			# I select only points that progressively go to the target
			temp = np.logical_and(np.diff(ref_points_1[:,1])>0 , np.diff(ref_points_2[:,1])>0)
			ref_points_1 = np.concatenate([[ref_points_1[0]] , ref_points_1[1:][temp]])
			ref_points_2 = np.concatenate([[ref_points_2[0]] , ref_points_2[1:][temp]])

			# polygon_structure = Polygon(structure_radial_profile[0])
			polygon = Polygon(np.flip(ref_points_1,axis=0).tolist() + ref_points_2.tolist())
			# selected = np.zeros_like(inverted_data[0]).astype(bool)
			# for i_r in range(len(inversion_R)):
			# 	for i_z in range(len(inversion_Z)):
			# 		if inversion_Z[i_z]<efit_reconstruction.mag_axis_z[i_efit_time]:
			# 			point = Point((inversion_R[i_r],inversion_Z[i_z]))
			# 			if polygon.contains(point):
			# 				selected[i_r,i_z] = True
			selected = select_cells_inside_polygon(polygon,[inversion_R,inversion_Z],center_line=coord_smooth_extended)
			# selected[:,inversion_Z>efit_reconstruction.mag_axis_z[i_efit_time]] = False
			selected_sigma = select_cells_inside_polygon(polygon,[voxels_centre])
			# selected_sigma[voxels_centre[:,1]<efit_reconstruction.mag_axis_z[i_efit_time]] = False
			temp = cp.deepcopy(inverted_data[i_t])
			temp1 = cp.deepcopy(inverted_data[i_t])
			temp1[np.isnan(temp1)] = 0
			inverted_data_interpolator = RegularGridInterpolator((inversion_R, inversion_Z), temp1,bounds_error=False,fill_value=0)

			# temp[temp<0] = 0
			# emissivity_baricentre = [(np.nansum(selected*temp*(b**2)/inverted_data_sigma[i_t])/np.nansum(selected*temp/inverted_data_sigma[i_t]))**0.5,np.nansum(selected*temp*a/inverted_data_sigma[i_t])/np.nansum(selected*temp/inverted_data_sigma[i_t])]
			emissivity_baricentre = [(np.nansum(selected*temp*(b**2))/np.nansum(selected*temp))**0.5,np.nansum(selected*temp*a)/np.nansum(selected*temp)]
			L_poloidal_baricentre = L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,emissivity_baricentre,efit_data,i_efit_time)

			emissivity_peak = np.unravel_index(np.nanargmax(selected*inverted_data[i_t]),np.shape(selected))
			emissivity_peak = [inversion_R[emissivity_peak[0]],inversion_Z[emissivity_peak[1]]]
			L_poloidal_peak = L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,emissivity_peak,efit_data,i_efit_time)
			L_poloidal_x_point = L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,[efit_reconstruction.lower_xpoint_r[i_efit_time],efit_reconstruction.lower_xpoint_z[i_efit_time]],efit_data,i_efit_time)

			# selecting only the inner leg
			selected[inversion_R>efit_reconstruction.lower_xpoint_r[i_efit_time]] = False
			selected[:,inversion_Z>efit_reconstruction.lower_xpoint_z[i_efit_time]] = False
			# excluding the x-point
			selected[((np.meshgrid(inversion_Z,inversion_R)[0]-efit_reconstruction.lower_xpoint_z[i_efit_time])**2 + (np.meshgrid(inversion_Z,inversion_R)[1]-efit_reconstruction.lower_xpoint_r[i_efit_time])**2)**0.5<x_point_region_radious] = False
			selected_sigma[voxels_centre[:,0]>efit_reconstruction.lower_xpoint_r[i_efit_time]] = False
			selected_sigma[voxels_centre[:,1]>efit_reconstruction.lower_xpoint_z[i_efit_time]] = False
			selected_sigma[((voxels_centre[:,1]-efit_reconstruction.lower_xpoint_z[i_efit_time])**2 + (voxels_centre[:,0]-efit_reconstruction.lower_xpoint_r[i_efit_time])**2)**0.5<x_point_region_radious] = False
			emissivity_peak_only_leg = np.unravel_index(np.nanargmax(selected*inverted_data[i_t]),np.shape(selected))
			emissivity_peak_only_leg = [inversion_R[emissivity_peak_only_leg[0]],inversion_Z[emissivity_peak_only_leg[1]]]
			L_poloidal_peak_only_leg = L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,emissivity_peak_only_leg,efit_data,i_efit_time)
			emissivity_baricentre_only_leg = [(np.nansum(selected*temp*(b**2))/np.nansum(selected*temp))**0.5,np.nansum(selected*temp*a)/np.nansum(selected*temp)]
			L_poloidal_baricentre_only_leg = L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,emissivity_baricentre_only_leg,efit_data,i_efit_time)

			L_poloidal_midplane = L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,np.array([r_coord_smooth_origial[np.abs(r_coord_smooth_origial-0).argmin()],z_coord_smooth_origial[np.abs(r_coord_smooth_origial-0).argmin()]]),efit_data,i_efit_time)

			# selecting the part of the leg that is away from the region of power unreliability
			# temp1 = np.ones_like(selected).astype(bool)
			# # temp1[inversion_R<0.800] = False	# this might be reworked one the phantom scans are done
			# # temp1[:,inversion_Z>-1.550] = False	# this might be reworked one the phantom scans are done
			# temp1[z_<MU01_sxd_region_delimiter(r_)] = 0	# delimiter as used in the thesis
			# selected = np.logical_and(selected,np.logical_not(temp1))
			leg_reliable_power = np.nansum((temp.T*inversion_R).T*selected*2*np.pi*(grid_resolution**2))

			# temp1 = np.ones_like(selected_sigma).astype(bool)
			# # temp1[voxels_centre[:,0]<0.800] = False	# this might be reworked one the phantom scans are done
			# # temp1[voxels_centre[:,1]>-1.550] = False	# this might be reworked one the phantom scans are done
			# temp1[voxels_centre[:,1]<MU01_sxd_region_delimiter(voxels_centre[:,0])] = False	# delimiter as used in the thesis
			# selected_sigma = np.logical_and(selected_sigma,np.logical_not(temp1))
			temp_select = selected_sigma*2*np.pi*(grid_resolution**2)*(voxels_centre[:,0])
			leg_reliable_power_sigma = np.nansum((covariance_out[i_t][:len(grid_data_masked_crop),:len(grid_data_masked_crop)]*temp_select).T*temp_select)**0.5
			# leg_reliable_power_sigma = np.nansum(((temp_sigma.T*inversion_R).T*selected*2*np.pi*(grid_resolution**2))**2)**0.5

			ref_points_1 = cp.deepcopy(ref_points_1_original)
			ref_points_2 = cp.deepcopy(ref_points_2_original)
			# in the outer separatrix the inner side is always good and the outer bad, the reverse in the inner separatrix
			ref_points_1[:,0] = scipy.signal.savgol_filter(ref_points_1[:,0],5,2)
			ref_points_1[:,1] = scipy.signal.savgol_filter(ref_points_1[:,1],5,2)
			# ref_points_2[:,0] = scipy.signal.savgol_filter(ref_points_2[:,0],7,2)
			# ref_points_2[:,1] = scipy.signal.savgol_filter(ref_points_2[:,1],7,2)
			# I want a higher resolution than 10cm. simple approach: interpolate between each ref_points to get a 5cm resolution...!
			ref_points_1 = np.array([np.interp(np.arange(0,len(ref_points_1)-0.5,0.5),np.arange(0,len(ref_points_1)-0.5,1),ref_points_1[:,0]),np.interp(np.arange(0,len(ref_points_1)-0.5,0.5),np.arange(0,len(ref_points_1)-0.5,1),ref_points_1[:,1])]).T
			ref_points_2 = np.array([np.interp(np.arange(0,len(ref_points_2)-0.5,0.5),np.arange(0,len(ref_points_2)-0.5,1),ref_points_2[:,0]),np.interp(np.arange(0,len(ref_points_2)-0.5,0.5),np.arange(0,len(ref_points_2)-0.5,1),ref_points_2[:,1])]).T
			local_mean_emis = []
			local_power = []
			local_L_poloidal = []
			emissivity_flat = cp.deepcopy(inverted_data[i_t].flatten())
			for i_ref_point in range(1,len(ref_points_1)):
				# print(i_ref_point)
				polygon = Polygon([ref_points_1[i_ref_point-1], ref_points_1[i_ref_point], ref_points_2[i_ref_point], ref_points_2[i_ref_point-1]])
				# select = []
				# for i_e in range(len(emissivity_flat)):
				# 	point = Point((b_flat[i_e],a_flat[i_e]))
				# 	select.append(polygon.contains(point))
				select = select_cells_inside_polygon(polygon,[inversion_R,inversion_Z],center_line=coord_smooth_extended).flatten()
				local_mean_emis.append(np.nanmean(emissivity_flat[select]))
				local_power.append(2*np.pi*np.nansum(emissivity_flat[select]*b_flat[select]*(grid_resolution**2)))
				local_L_poloidal.append(L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,[np.mean(b_flat[select]),np.mean(a_flat[select])],efit_data,i_efit_time))

			# section to find the half peak
			ref1 = inverted_data_interpolator((ref_points_for_interp_vlx[:,0],ref_points_for_interp_vlx[:,1]))
			ref2 = inverted_data_interpolator((ref_points_for_interp_lx[:,0],ref_points_for_interp_lx[:,1]))
			ref3 = inverted_data_interpolator((coord_smooth_extended[:,0],coord_smooth_extended[:,1]))
			ref4 = inverted_data_interpolator((ref_points_for_interp_rx[:,0],ref_points_for_interp_rx[:,1]))
			ref5 = inverted_data_interpolator((ref_points_for_interp_vrx[:,0],ref_points_for_interp_vrx[:,1]))
			if False:	# just to make plots
				plt.figure()
				x_axis = np.array([ L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,pos,efit_data,i_efit_time) for pos in coord_smooth_extended ])/L_poloidal_x_point
				plt.plot(x_axis,ref3)
				plt.plot(L_poloidal_peak/L_poloidal_x_point,'o')
				midpoint_ref3 = (np.arange(len(ref3))[ref3>np.nanmax(ref3)*0.5])[0]
				ref3=coord_smooth_extended[midpoint_ref3]
				plt.plot(L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,ref3,efit_data,i_efit_time)/L_poloidal_x_point,'o')
				pass
			# midpoint_ref1 = (np.arange(len(ref1))[ref1>np.nanmax(ref1)*0.5])[0]
			# midpoint_ref2 = (np.arange(len(ref2))[ref2>np.nanmax(ref2)*0.5])[0]
			# midpoint_ref3 = (np.arange(len(ref3))[ref3>np.nanmax(ref3)*0.5])[0]
			# midpoint_ref4 = (np.arange(len(ref4))[ref4>np.nanmax(ref4)*0.5])[0]
			# midpoint_ref5 = (np.arange(len(ref5))[ref5>np.nanmax(ref5)*0.5])[0]
			[midpoint_ref1,midpoint_ref2,midpoint_ref3,midpoint_ref4,midpoint_ref5] = [ [(np.arange(len(ref))[ref>np.nanmax(ref)*temp])[0] for temp in [0.5-midpoint_range,0.5,0.5+midpoint_range]]  for ref in [ref1,ref2,ref3,ref4,ref5] ]
			ref1=ref_points_for_interp_vlx[midpoint_ref1]
			ref2=ref_points_for_interp_lx[midpoint_ref2]
			ref3=coord_smooth_extended[midpoint_ref3]
			ref4=ref_points_for_interp_rx[midpoint_ref4]
			ref5=ref_points_for_interp_vrx[midpoint_ref5]
			# half_peak = np.array([np.median([ref1[0],ref2[0],ref3[0],ref4[0],ref5[0]]),np.median([ref1[1],ref2[1],ref3[1],ref4[1],ref5[1]])])
			half_peak = [np.array([np.median([ref1[i][0],ref2[i][0],ref3[i][0],ref4[i][0],ref5[i][0]]),np.median([ref1[i][1],ref2[i][1],ref3[i][1],ref4[i][1],ref5[i][1]])]) for i in range(len(ref1))]
			# half_peak_L_pol = L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,half_peak,efit_data,i_efit_time)
			half_peak_L_pol = [ L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,pos,efit_data,i_efit_time) for pos in half_peak ]


			ref1 = inverted_data_interpolator((ref_points_for_interp_vlx[:,0][ref_points_for_interp_vlx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]],ref_points_for_interp_vlx[:,1][ref_points_for_interp_vlx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]]))
			ref2 = inverted_data_interpolator((ref_points_for_interp_lx[:,0][ref_points_for_interp_lx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]],ref_points_for_interp_lx[:,1][ref_points_for_interp_lx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]]))
			ref3 = inverted_data_interpolator((coord_smooth_extended[:,0][coord_smooth_extended[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]],coord_smooth_extended[:,1][coord_smooth_extended[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]]))
			ref4 = inverted_data_interpolator((ref_points_for_interp_rx[:,0][ref_points_for_interp_rx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]],ref_points_for_interp_rx[:,1][ref_points_for_interp_rx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]]))
			ref5 = inverted_data_interpolator((ref_points_for_interp_vrx[:,0][ref_points_for_interp_vrx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]],ref_points_for_interp_vrx[:,1][ref_points_for_interp_vrx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]]))
			# midpoint_ref1 = (np.arange(len(ref1))[ref1>np.nanmax(ref1)/2])[0]
			# midpoint_ref2 = (np.arange(len(ref2))[ref2>np.nanmax(ref2)/2])[0]
			# midpoint_ref3 = (np.arange(len(ref3))[ref3>np.nanmax(ref3)/2])[0]
			# midpoint_ref4 = (np.arange(len(ref4))[ref4>np.nanmax(ref4)/2])[0]
			# midpoint_ref5 = (np.arange(len(ref5))[ref5>np.nanmax(ref5)/2])[0]
			[midpoint_ref1,midpoint_ref2,midpoint_ref3,midpoint_ref4,midpoint_ref5] = [ [(np.arange(len(ref))[ref>np.nanmax(ref)*temp])[0] for temp in [0.5-midpoint_range,0.5,0.5+midpoint_range]]  for ref in [ref1,ref2,ref3,ref4,ref5] ]
			ref1=ref_points_for_interp_vlx[ref_points_for_interp_vlx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]][midpoint_ref1]
			ref2=ref_points_for_interp_lx[ref_points_for_interp_lx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]][midpoint_ref2]
			ref3=coord_smooth_extended[coord_smooth_extended[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]][midpoint_ref3]
			ref4=ref_points_for_interp_rx[ref_points_for_interp_rx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]][midpoint_ref4]
			ref5=ref_points_for_interp_vrx[ref_points_for_interp_vrx[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]][midpoint_ref5]
			# half_peak_divertor = np.array([np.median([ref1[0],ref2[0],ref3[0],ref4[0],ref5[0]]),np.median([ref1[1],ref2[1],ref3[1],ref4[1],ref5[1]])])
			half_peak_divertor = [np.array([np.median([ref1[i][0],ref2[i][0],ref3[i][0],ref4[i][0],ref5[i][0]]),np.median([ref1[i][1],ref2[i][1],ref3[i][1],ref4[i][1],ref5[i][1]])]) for i in range(len(ref1))]
			# half_peak_divertor_L_pol = L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,half_peak_divertor,efit_data,i_efit_time)
			half_peak_divertor_L_pol = [ L_poloidal_from_point_to_target(r_coord_smooth_origial,z_coord_smooth_origial,pos,efit_data,i_efit_time) for pos in half_peak_divertor ]

			# local_mean_emis = np.array(local_mean_emis)
			# local_power = np.array(local_power)
			# local_mean_emis = local_mean_emis[np.logical_not(np.isnan(local_mean_emis))].tolist()
			# local_power = local_power[np.logical_not(np.isnan(local_power))].tolist()
			emissivity_baricentre_all.append(emissivity_baricentre)
			L_poloidal_baricentre_all.append(L_poloidal_baricentre)
			emissivity_peak_all.append(emissivity_peak)
			L_poloidal_peak_all.append(L_poloidal_peak)
			L_poloidal_peak_only_leg_all.append(L_poloidal_peak_only_leg)
			L_poloidal_baricentre_only_leg_all.append(L_poloidal_baricentre_only_leg)
			L_poloidal_x_point_all.append(L_poloidal_x_point)
			L_poloidal_midplane_all.append(L_poloidal_midplane)
			leg_reliable_power_all.append(leg_reliable_power)
			leg_reliable_power_sigma_all.append(leg_reliable_power_sigma)
			local_mean_emis_all.append(local_mean_emis)
			local_power_all.append(local_power)
			local_L_poloidal_all.append(local_L_poloidal)
			data_length = max(data_length,len(local_power))
			leg_length_all.append(leg_length)
			leg_length_interval_all.append(leg_length_interval)
			half_peak_L_pol_all.append(half_peak_L_pol)
			half_peak_divertor_L_pol_all.append(half_peak_divertor_L_pol)
		except Exception as e:
			print('failed')
			# logging.exception('with error: ' + str(e))
			local_mean_emis_all.append([])
			local_power_all.append([])
			local_L_poloidal_all.append([])
			leg_length_interval_all.append([])
			leg_length_all.append(0)
			emissivity_baricentre_all.append([np.nan,np.nan])
			emissivity_peak_all.append([np.nan,np.nan])
			L_poloidal_baricentre_all.append(0)
			L_poloidal_peak_all.append(0)
			L_poloidal_peak_only_leg_all.append(0)
			L_poloidal_baricentre_only_leg_all.append(0)
			L_poloidal_x_point_all.append(np.inf)
			L_poloidal_midplane_all.append(np.inf)
			leg_reliable_power_all.append(0)
			leg_reliable_power_sigma_all.append(0)
			half_peak_L_pol_all.append([0,0,0])
			half_peak_divertor_L_pol_all.append([0,0,0])
	emissivity_baricentre_all = np.array(emissivity_baricentre_all)
	emissivity_peak_all = np.array(emissivity_peak_all)
	L_poloidal_baricentre_all = np.array(L_poloidal_baricentre_all)
	L_poloidal_peak_all = np.array(L_poloidal_peak_all)
	L_poloidal_peak_only_leg_all = np.array(L_poloidal_peak_only_leg_all)
	L_poloidal_baricentre_only_leg_all = np.array(L_poloidal_baricentre_only_leg_all)
	L_poloidal_x_point_all = np.array(L_poloidal_x_point_all)
	L_poloidal_midplane_all = np.array(L_poloidal_midplane_all)
	leg_reliable_power_all = np.array(leg_reliable_power_all)
	leg_reliable_power_sigma_all = np.array(leg_reliable_power_sigma_all)

	for i_t in range(len(time_full_binned_crop)):
		if len(local_mean_emis_all[i_t])<data_length:
			local_mean_emis_all[i_t].extend([0]*(data_length-len(local_mean_emis_all[i_t])))
			local_power_all[i_t].extend([0]*(data_length-len(local_power_all[i_t])))
			local_L_poloidal_all[i_t].extend([0]*(data_length-len(local_L_poloidal_all[i_t])))
			leg_length_interval_all[i_t].extend([0]*(data_length-len(leg_length_interval_all[i_t])))

	return local_mean_emis_all,local_power_all,local_L_poloidal_all,leg_length_interval_all,leg_length_all,data_length,leg_resolution,emissivity_baricentre_all,emissivity_peak_all,L_poloidal_baricentre_all,L_poloidal_peak_all,L_poloidal_peak_only_leg_all,L_poloidal_baricentre_only_leg_all,L_poloidal_midplane_all,leg_reliable_power_all,leg_reliable_power_sigma_all,greenwald_density,dr_sep_in,dr_sep_out,L_poloidal_x_point_all,half_peak_L_pol_all,half_peak_divertor_L_pol_all,sideways_leg_resolution


def inside_vs_outside_separatrix_radiation(inverted_data,inverted_data_sigma,inversion_R,inversion_Z,time_full_binned_crop,efit_reconstruction,covariance_out,grid_data_masked_crop,x_point_region_radious=0.1):
	#here I want to calculate precisely the radiation coming from inside the separatrix compared to outside
	from shapely.geometry import Point
	from shapely.geometry.polygon import Polygon
	from scipy.ndimage import generic_filter
	import mastu_exhaust_analysis.fluxsurface_tracer as ft
	from mastu_exhaust_analysis.read_efit import read_epm
	from scipy.interpolate import interp1d,interp2d

	all_time_sep_r,all_time_sep_z,r_fine,z_fine = efit_reconstruction_to_separatrix_on_foil(efit_reconstruction)	# the separatrix are ordered as left_of_up_xpoints,right_of_up_xpoints,left_of_low_xpoints,right_of_low_xpoints
	all_time_strike_points_location = return_all_time_strike_points_location_radial(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine)
	fdir = uda_transfer(efit_reconstruction.shotnumber,'epm')
	efit_data = read_epm(fdir,calc_bfield=True)
	os.remove(fdir)

	dr = np.median(np.diff(inversion_R))
	dz = np.median(np.diff(inversion_Z))
	a,b = np.meshgrid(inversion_Z,inversion_R)
	a_flat = a.flatten()
	b_flat = b.flatten()
	grid_resolution = np.median(np.concatenate([np.diff(inversion_Z),np.diff(inversion_R)]))
	# leg_resolution = 0.1	# m
	# structure_radial_profile = return_structure_radial_profile()
	z_,r_ = np.meshgrid(inversion_Z,inversion_R)

	voxels_centre = np.mean(grid_data_masked_crop,axis=1)

	real_core_radiation_all = []
	real_core_radiation_sigma_all = []
	real_non_core_radiation_all = []
	real_non_core_radiation_sigma_all = []
	out_VV_radiation_all = []
	out_VV_radiation_sigma_all = []
	real_inner_core_radiation_all = []
	real_inner_core_radiation_sigma_all = []
	real_outer_core_radiation_all = []
	real_outer_core_radiation_sigma_all = []

	polygon = Polygon(FULL_MASTU_CORE_GRID_POLYGON)
	selected_VV = select_cells_inside_polygon(polygon,[inversion_R,inversion_Z])
	selected_sigma_VV = select_cells_inside_polygon(polygon,[voxels_centre])

	for i_t in range(len(time_full_binned_crop)):
		print('starting t=%.4gms' %(time_full_binned_crop[i_t]*1e3))
		try:
			i_efit_time = np.abs(efit_reconstruction.time-time_full_binned_crop[i_t]).argmin()
			if True:	# method relying on selecting psiN<1
				gna = efit_data['psiN'][i_efit_time]
				psi_interpolator = interp2d(efit_reconstruction.R,efit_reconstruction.Z,gna)
				psiN = psi_interpolator(inversion_R,inversion_Z)
				selected = psiN<=1
				selected[inversion_Z<efit_data['lower_xpoint_z'][i_efit_time]]=False
				selected[:,inversion_R-grid_resolution/2<efit_data['rmidplaneIn'][i_efit_time]]=False
				selected = selected.T
				psiN = np.diag(psi_interpolator(voxels_centre[:,0],voxels_centre[:,1]))
				selected_sigma = psiN<=1
				selected_sigma[voxels_centre[:,1]<efit_data['lower_xpoint_z'][i_efit_time]]=False
				selected_sigma[voxels_centre[:,0]-grid_resolution/2<efit_data['rmidplaneIn'][i_efit_time]]=False
				# selected_sigma = selected_sigma.T
			else:	# method relying on tracing field lines
				trace_flux_surface = ft.trace_flux_surface(efit_data, i_efit_time,efit_data['lower_xpoint_r'][i_efit_time],efit_data['lower_xpoint_z'][i_efit_time]+0.05)
				r_coord_smooth = np.array(trace_flux_surface.r)
				z_coord_smooth = np.array(trace_flux_surface.z)
				if False:
					plt.plot(trace_flux_surface.r,trace_flux_surface.z,'+')

				ref_points = np.array([r_coord_smooth,z_coord_smooth]).T
				# ref_points_2[:,0] = scipy.signal.savgol_filter(ref_points_2[:,0],7,2)
				# ref_points_2[:,1] = scipy.signal.savgol_filter(ref_points_2[:,1],7,2)
				polygon = Polygon(ref_points)
				selected1 = select_cells_inside_polygon(polygon,[inversion_R-grid_resolution/2,inversion_Z-grid_resolution/2],center_line=ref_points)
				selected2 = select_cells_inside_polygon(polygon,[inversion_R+grid_resolution/2,inversion_Z-grid_resolution/2],center_line=ref_points)
				selected3 = select_cells_inside_polygon(polygon,[inversion_R-grid_resolution/2,inversion_Z+grid_resolution/2],center_line=ref_points)
				selected4 = select_cells_inside_polygon(polygon,[inversion_R+grid_resolution/2,inversion_Z+grid_resolution/2],center_line=ref_points)
				selected = np.logical_or(np.logical_or(selected1,selected2),np.logical_or(selected3,selected4))
				selected[:,inversion_Z<efit_reconstruction.mag_axis_z[i_efit_time]] = False
				selected1 = select_cells_inside_polygon(polygon,[voxels_centre+[-grid_resolution/2,-grid_resolution/2]])
				selected2 = select_cells_inside_polygon(polygon,[voxels_centre+[+grid_resolution/2,-grid_resolution/2]])
				selected3 = select_cells_inside_polygon(polygon,[voxels_centre+[-grid_resolution/2,+grid_resolution/2]])
				selected4 = select_cells_inside_polygon(polygon,[voxels_centre+[+grid_resolution/2,+grid_resolution/2]])
				selected = np.logical_or(np.logical_or(selected1,selected2),np.logical_or(selected3,selected4))
				selected_sigma[voxels_centre[:,1]<efit_reconstruction.mag_axis_z[i_efit_time]] = False

			# selected = np.zeros_like(inverted_data[0]).astype(bool)
			# for i_r in range(len(inversion_R)):
			# 	for i_z in range(len(inversion_Z)):
			# 		if inversion_Z[i_z]<efit_reconstruction.mag_axis_z[i_efit_time]:
			# 			point = Point((inversion_R[i_r]-grid_resolution/2,inversion_Z[i_z]-grid_resolution/2))
			# 			if polygon.contains(point):
			# 				selected[i_r,i_z] = True
			# 				continue
			# 			point = Point((inversion_R[i_r]-grid_resolution/2,inversion_Z[i_z]+grid_resolution/2))
			# 			if polygon.contains(point):
			# 				selected[i_r,i_z] = True
			# 				continue
			# 			point = Point((inversion_R[i_r]+grid_resolution/2,inversion_Z[i_z]-grid_resolution/2))
			# 			if polygon.contains(point):
			# 				selected[i_r,i_z] = True
			# 				continue
			# 			point = Point((inversion_R[i_r]+grid_resolution/2,inversion_Z[i_z]+grid_resolution/2))
			# 			if polygon.contains(point):
			# 				selected[i_r,i_z] = True
			# 				continue
			temp = cp.deepcopy(inverted_data[i_t])
			temp[:,inversion_Z>efit_reconstruction.mag_axis_z[i_efit_time]] = 0	# exclude the upper part of the machine, so I can do the lower times 2
			temp1 = covariance_out[i_t][:len(grid_data_masked_crop),:len(grid_data_masked_crop)]
			temp1[voxels_centre[:,1]>efit_reconstruction.mag_axis_z[i_efit_time]]=0
			temp1[:,voxels_centre[:,1]>efit_reconstruction.mag_axis_z[i_efit_time]]=0
			# temp_sigma = inverted_data_sigma[i_t]
			# temp[temp<0] = 0

			# selecting the core without the x-point
			selected[((np.meshgrid(inversion_Z,inversion_R)[0]-efit_reconstruction.lower_xpoint_z[i_efit_time])**2 + (np.meshgrid(inversion_Z,inversion_R)[1]-efit_reconstruction.lower_xpoint_r[i_efit_time])**2)**0.5<x_point_region_radious] = False
			selected_sigma[((voxels_centre[:,1]-efit_reconstruction.lower_xpoint_z[i_efit_time])**2 + (voxels_centre[:,0]-efit_reconstruction.lower_xpoint_r[i_efit_time])**2)**0.5<x_point_region_radious] = False
			real_core_radiation_all.append(np.nansum(selected*temp*2*np.pi*b*(grid_resolution**2)))
			temp_select = selected_sigma*2*np.pi*(grid_resolution**2)*(voxels_centre[:,0])
			# real_core_radiation_sigma_all.append(np.nansum((selected*temp_sigma*2*np.pi*b*(grid_resolution**2))**2)**0.5)
			real_core_radiation_sigma_all.append(np.nansum((temp1*temp_select).T*temp_select)**0.5)

			# inner side of the core without x-point
			delimiter = return_inner_outer_leg_delimiter(efit_reconstruction,i_efit_time)
			selected_temp = cp.deepcopy(selected)
			selected_temp[r_>=delimiter(z_)] = False
			real_inner_core_radiation_all.append(np.nansum(selected_temp*temp*2*np.pi*b*(grid_resolution**2)))
			selected_sigma_temp = cp.deepcopy(selected_sigma)
			selected_sigma_temp[voxels_centre[:,0]>=delimiter(voxels_centre[:,1])] = False
			temp_select = selected_sigma_temp*2*np.pi*(grid_resolution**2)*(voxels_centre[:,0])
			real_inner_core_radiation_sigma_all.append(np.nansum((temp1*temp_select).T*temp_select)**0.5)
			# outer side of the core without x-point
			selected_temp = cp.deepcopy(selected)
			selected_temp[r_<delimiter(z_)] = False
			real_outer_core_radiation_all.append(np.nansum(selected_temp*temp*2*np.pi*b*(grid_resolution**2)))
			selected_sigma_temp = cp.deepcopy(selected_sigma)
			selected_sigma_temp[voxels_centre[:,0]<delimiter(voxels_centre[:,1])] = False
			temp_select = selected_sigma_temp*2*np.pi*(grid_resolution**2)*(voxels_centre[:,0])
			real_outer_core_radiation_sigma_all.append(np.nansum((temp1*temp_select).T*temp_select)**0.5)

			selected[((np.meshgrid(inversion_Z,inversion_R)[0]-efit_reconstruction.lower_xpoint_z[i_efit_time])**2 + (np.meshgrid(inversion_Z,inversion_R)[1]-efit_reconstruction.lower_xpoint_r[i_efit_time])**2)**0.5<x_point_region_radious] = True
			selected_sigma[((voxels_centre[:,1]-efit_reconstruction.lower_xpoint_z[i_efit_time])**2 + (voxels_centre[:,0]-efit_reconstruction.lower_xpoint_r[i_efit_time])**2)**0.5<x_point_region_radious] = True
			real_non_core_radiation_all.append(np.nansum(np.logical_and(np.logical_not(selected),selected_VV)*temp*2*np.pi*b*(grid_resolution**2)))
			# real_non_core_radiation_sigma_all.append(np.nansum((np.logical_not(selected)*temp_sigma*2*np.pi*b*(grid_resolution**2))**2)**0.5)
			temp_select = np.logical_and(np.logical_not(selected_sigma),selected_sigma_VV)*2*np.pi*(grid_resolution**2)*(voxels_centre[:,0])
			real_non_core_radiation_sigma_all.append(np.nansum((temp1*temp_select).T*temp_select)**0.5)

			out_VV_radiation_all.append(np.nansum(np.logical_not(selected_VV)*temp*2*np.pi*b*(grid_resolution**2)))
			temp_select = np.logical_not(selected_sigma_VV)*2*np.pi*(grid_resolution**2)*(voxels_centre[:,0])
			# real_core_radiation_sigma_all.append(np.nansum((selected*temp_sigma*2*np.pi*b*(grid_resolution**2))**2)**0.5)
			out_VV_radiation_sigma_all.append(np.nansum((temp1*temp_select).T*temp_select)**0.5)
		except Exception as e:
			print('failed')
			# logging.exception('with error: ' + str(e))
			real_core_radiation_all.append(np.nan)
			real_core_radiation_sigma_all.append(np.nan)
			real_non_core_radiation_all.append(np.nan)
			real_non_core_radiation_sigma_all.append(np.nan)
			out_VV_radiation_all.append(np.nan)
			out_VV_radiation_sigma_all.append(np.nan)
			real_inner_core_radiation_all.append(np.nan)
			real_inner_core_radiation_sigma_all.append(np.nan)
			real_outer_core_radiation_all.append(np.nan)
			real_outer_core_radiation_sigma_all.append(np.nan)
	real_core_radiation_all = np.array(real_core_radiation_all)
	real_core_radiation_sigma_all = np.array(real_core_radiation_sigma_all)
	real_non_core_radiation_all = np.array(real_non_core_radiation_all)
	real_non_core_radiation_sigma_all = np.array(real_non_core_radiation_sigma_all)
	out_VV_radiation_all = np.array(out_VV_radiation_all)
	out_VV_radiation_sigma_all = np.array(out_VV_radiation_sigma_all)
	real_inner_core_radiation_all = np.array(real_inner_core_radiation_all)
	real_inner_core_radiation_sigma_all = np.array(real_inner_core_radiation_sigma_all)
	real_outer_core_radiation_all = np.array(real_outer_core_radiation_all)
	real_outer_core_radiation_sigma_all = np.array(real_outer_core_radiation_sigma_all)

	return real_core_radiation_all,real_core_radiation_sigma_all,real_non_core_radiation_all,real_non_core_radiation_sigma_all,out_VV_radiation_all,out_VV_radiation_sigma_all,real_inner_core_radiation_all,real_inner_core_radiation_sigma_all,real_outer_core_radiation_all,real_outer_core_radiation_sigma_all

def symplified_out_core_regions(inverted_data,inverted_data_sigma,inversion_R,inversion_Z,time_full_binned_crop,efit_reconstruction,covariance_out,grid_data_masked_crop,x_point_region_radious=0.1):
	# I copy the division of the regions outride the core from
	# Detachment evolution on the TCV tokamak, J.R.Harrison, 2017
	from shapely.geometry import Point
	from shapely.geometry.polygon import Polygon
	from scipy.ndimage import generic_filter
	import mastu_exhaust_analysis.fluxsurface_tracer as ft
	from mastu_exhaust_analysis.read_efit import read_epm
	from scipy.interpolate import interp1d,interp2d
	from scipy.interpolate import RegularGridInterpolator

	all_time_sep_r,all_time_sep_z,r_fine,z_fine = efit_reconstruction_to_separatrix_on_foil(efit_reconstruction)	# the separatrix are ordered as left_of_up_xpoints,right_of_up_xpoints,left_of_low_xpoints,right_of_low_xpoints
	all_time_strike_points_location = return_all_time_strike_points_location_radial(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine)
	fdir = uda_transfer(efit_reconstruction.shotnumber,'epm')
	efit_data = read_epm(fdir,calc_bfield=True)
	os.remove(fdir)

	dr = np.median(np.diff(inversion_R))
	dz = np.median(np.diff(inversion_Z))
	a,b = np.meshgrid(inversion_Z,inversion_R)
	a_flat = a.flatten()
	b_flat = b.flatten()
	grid_resolution = np.median(np.concatenate([np.diff(inversion_Z),np.diff(inversion_R)]))
	# leg_resolution = 0.1	# m
	# structure_radial_profile = return_structure_radial_profile()
	z_,r_ = np.meshgrid(inversion_Z,inversion_R)

	voxels_centre = np.mean(grid_data_masked_crop,axis=1)

	inner_SOL_all = []
	inner_SOL_sigma_all = []
	inner_SOL_leg_all = []
	inner_SOL_leg_sigma_all = []
	outer_SOL_leg_all = []
	outer_SOL_leg_sigma_all = []
	outer_SOL_all = []
	outer_SOL_sigma_all = []
	psiN_min_lower_baffle_all = []
	psiN_min_lower_target_all = []
	psiN_min_central_column_all = []
	psiN_min_upper_target_all = []
	psiN_min_upper_baffle_all = []

	polygon = Polygon(FULL_MASTU_CORE_GRID_POLYGON)
	selected_VV = select_cells_inside_polygon(polygon,[inversion_R,inversion_Z])
	selected_sigma_VV = select_cells_inside_polygon(polygon,[voxels_centre])

	for i_t in range(len(time_full_binned_crop)):
		print('starting t=%.4gms' %(time_full_binned_crop[i_t]*1e3))
		i_efit_time = np.abs(efit_reconstruction.time-time_full_binned_crop[i_t]).argmin()
		# 2024/11/25 I want the minimum psiN to see hot much the plasma goes close to the surface
		psi_interpolator = RegularGridInterpolator((efit_reconstruction.R,efit_reconstruction.Z),efit_data['psiN'][i_efit_time].T)
		psiN_min_lower_baffle_all.append(np.nanmin(psi_interpolator(FULL_MASTU_CORE_GRID_POLYGON_lower_baffle)))
		psiN_min_lower_target_all.append(np.nanmin(psi_interpolator(FULL_MASTU_CORE_GRID_POLYGON_lower_target)))
		psiN_min_central_column_all.append(np.nanmin(psi_interpolator(FULL_MASTU_CORE_GRID_POLYGON_central_column)))
		psiN_min_upper_target_all.append(np.nanmin(psi_interpolator(FULL_MASTU_CORE_GRID_POLYGON_upper_target)))
		psiN_min_upper_baffle_all.append(np.nanmin(psi_interpolator(FULL_MASTU_CORE_GRID_POLYGON_upper_baffle)))
		try:
			if True:	# method relying on selecting psiN<1 to select the core
				psi_interpolator = interp2d(efit_reconstruction.R,efit_reconstruction.Z,efit_data['psiN'][i_efit_time])
				psiN = psi_interpolator(inversion_R,inversion_Z)
				selected = psiN<=1
				selected[inversion_Z<efit_data['lower_xpoint_z'][i_efit_time]]=False
				selected[:,inversion_R-grid_resolution/2<efit_data['rmidplaneIn'][i_efit_time]]=False
				selected = selected.T
				psiN = np.diag(psi_interpolator(voxels_centre[:,0],voxels_centre[:,1]))
				selected_sigma = psiN<=1
				selected_sigma[voxels_centre[:,1]<efit_data['lower_xpoint_z'][i_efit_time]]=False
				selected_sigma[voxels_centre[:,0]-grid_resolution/2<efit_data['rmidplaneIn'][i_efit_time]]=False
				# selected_sigma = selected_sigma.T
			else:	# method relying on tracing field lines to select the core
				trace_flux_surface = ft.trace_flux_surface(efit_data, i_efit_time,efit_data['lower_xpoint_r'][i_efit_time],efit_data['lower_xpoint_z'][i_efit_time]+0.05)
				r_coord_smooth = np.array(trace_flux_surface.r)
				z_coord_smooth = np.array(trace_flux_surface.z)
				if False:
					plt.plot(trace_flux_surface.r,trace_flux_surface.z,'+')

				ref_points = np.array([r_coord_smooth,z_coord_smooth]).T
				# ref_points_2[:,0] = scipy.signal.savgol_filter(ref_points_2[:,0],7,2)
				# ref_points_2[:,1] = scipy.signal.savgol_filter(ref_points_2[:,1],7,2)
				polygon = Polygon(ref_points)
				selected1 = select_cells_inside_polygon(polygon,[inversion_R-grid_resolution/2,inversion_Z-grid_resolution/2],center_line=ref_points)
				selected2 = select_cells_inside_polygon(polygon,[inversion_R+grid_resolution/2,inversion_Z-grid_resolution/2],center_line=ref_points)
				selected3 = select_cells_inside_polygon(polygon,[inversion_R-grid_resolution/2,inversion_Z+grid_resolution/2],center_line=ref_points)
				selected4 = select_cells_inside_polygon(polygon,[inversion_R+grid_resolution/2,inversion_Z+grid_resolution/2],center_line=ref_points)
				selected = np.logical_or(np.logical_or(selected1,selected2),np.logical_or(selected3,selected4))
				selected[:,inversion_Z<efit_reconstruction.mag_axis_z[i_efit_time]] = False
				selected1 = select_cells_inside_polygon(polygon,[voxels_centre+[-grid_resolution/2,-grid_resolution/2]])
				selected2 = select_cells_inside_polygon(polygon,[voxels_centre+[+grid_resolution/2,-grid_resolution/2]])
				selected3 = select_cells_inside_polygon(polygon,[voxels_centre+[-grid_resolution/2,+grid_resolution/2]])
				selected4 = select_cells_inside_polygon(polygon,[voxels_centre+[+grid_resolution/2,+grid_resolution/2]])
				selected = np.logical_or(np.logical_or(selected1,selected2),np.logical_or(selected3,selected4))
				selected_sigma[voxels_centre[:,1]<efit_reconstruction.mag_axis_z[i_efit_time]] = False

			# selected = np.zeros_like(inverted_data[0]).astype(bool)
			# for i_r in range(len(inversion_R)):
			# 	for i_z in range(len(inversion_Z)):
			# 		if inversion_Z[i_z]<efit_reconstruction.mag_axis_z[i_efit_time]:
			# 			point = Point((inversion_R[i_r]-grid_resolution/2,inversion_Z[i_z]-grid_resolution/2))
			# 			if polygon.contains(point):
			# 				selected[i_r,i_z] = True
			# 				continue
			# 			point = Point((inversion_R[i_r]-grid_resolution/2,inversion_Z[i_z]+grid_resolution/2))
			# 			if polygon.contains(point):
			# 				selected[i_r,i_z] = True
			# 				continue
			# 			point = Point((inversion_R[i_r]+grid_resolution/2,inversion_Z[i_z]-grid_resolution/2))
			# 			if polygon.contains(point):
			# 				selected[i_r,i_z] = True
			# 				continue
			# 			point = Point((inversion_R[i_r]+grid_resolution/2,inversion_Z[i_z]+grid_resolution/2))
			# 			if polygon.contains(point):
			# 				selected[i_r,i_z] = True
			# 				continue
			temp = cp.deepcopy(inverted_data[i_t])
			temp[:,inversion_Z>efit_reconstruction.mag_axis_z[i_efit_time]] = 0
			temp1 = covariance_out[i_t][:len(grid_data_masked_crop),:len(grid_data_masked_crop)]
			temp1[voxels_centre[:,1]>efit_reconstruction.mag_axis_z[i_efit_time]]=0
			temp1[:,voxels_centre[:,1]>efit_reconstruction.mag_axis_z[i_efit_time]]=0
			# temp_sigma = inverted_data_sigma[i_t]
			# temp[temp<0] = 0
			# I want the outside of the core, not the inside
			selected = np.logical_not(selected)
			selected_sigma = np.logical_not(selected_sigma)
			selected = np.logical_and(selected,selected_VV)
			selected_sigma = np.logical_and(selected_sigma,selected_sigma_VV)
			# I exclude the X-point
			selected[((np.meshgrid(inversion_Z,inversion_R)[0]-efit_reconstruction.lower_xpoint_z[i_efit_time])**2 + (np.meshgrid(inversion_Z,inversion_R)[1]-efit_reconstruction.lower_xpoint_r[i_efit_time])**2)**0.5<x_point_region_radious] = False
			selected_sigma[((voxels_centre[:,1]-efit_reconstruction.lower_xpoint_z[i_efit_time])**2 + (voxels_centre[:,0]-efit_reconstruction.lower_xpoint_r[i_efit_time])**2)**0.5<x_point_region_radious] = False

			# I define an interpolator fot the conjunction of x-point and magnetic axi
			leg_separator = interp1d([efit_reconstruction.lower_xpoint_r[i_efit_time],efit_reconstruction.mag_axis_r[i_efit_time]],[efit_reconstruction.lower_xpoint_z[i_efit_time],efit_reconstruction.mag_axis_z[i_efit_time]],bounds_error=None, fill_value='extrapolate')

			# inner+SOL
			temp_selected = cp.deepcopy(selected)
			temp_selected[z_<leg_separator(r_)] = False
			temp_selected[z_>efit_reconstruction.lower_xpoint_z[i_efit_time]] = False
			temp_selected_sigma = cp.deepcopy(selected_sigma)
			temp_selected_sigma[voxels_centre[:,1]<leg_separator(voxels_centre[:,0])] = False
			temp_selected_sigma[voxels_centre[:,1]>efit_reconstruction.lower_xpoint_z[i_efit_time]] = False
			inner_SOL_leg_all.append(np.nansum(temp_selected*temp*2*np.pi*b*(grid_resolution**2)))
			temp_select = temp_selected_sigma*2*np.pi*(grid_resolution**2)*(voxels_centre[:,0])
			inner_SOL_leg_sigma_all.append(np.nansum((temp1*temp_select).T*temp_select)**0.5)

			# inner SOL (including part of X-point region)
			temp_selected = cp.deepcopy(selected)
			temp_selected[z_<leg_separator(r_)] = False
			temp_selected[z_<=efit_reconstruction.lower_xpoint_z[i_efit_time]] = False
			temp_selected_sigma = cp.deepcopy(selected_sigma)
			temp_selected_sigma[voxels_centre[:,1]<leg_separator(voxels_centre[:,0])] = False
			temp_selected_sigma[voxels_centre[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]] = False
			inner_SOL_all.append(np.nansum(temp_selected*temp*2*np.pi*b*(grid_resolution**2)))
			temp_select = temp_selected_sigma*2*np.pi*(grid_resolution**2)*(voxels_centre[:,0])
			inner_SOL_sigma_all.append(np.nansum((temp1*temp_select).T*temp_select)**0.5)

			# outer leg + SOL (including part of X-point region)
			temp_selected = cp.deepcopy(selected)
			temp_selected[z_>=leg_separator(r_)] = False
			temp_selected[z_>=efit_reconstruction.lower_xpoint_z[i_efit_time]] = False
			temp_selected_sigma = cp.deepcopy(selected_sigma)
			temp_selected_sigma[voxels_centre[:,1]>=leg_separator(voxels_centre[:,0])] = False
			temp_selected_sigma[voxels_centre[:,1]>=efit_reconstruction.lower_xpoint_z[i_efit_time]] = False
			outer_SOL_leg_all.append(np.nansum(temp_selected*temp*2*np.pi*b*(grid_resolution**2)))
			temp_select = temp_selected_sigma*2*np.pi*(grid_resolution**2)*(voxels_centre[:,0])
			outer_SOL_leg_sigma_all.append(np.nansum((temp1*temp_select).T*temp_select)**0.5)

			# outer SOL
			temp_selected = cp.deepcopy(selected)
			temp_selected[r_<=efit_reconstruction.lower_xpoint_r[i_efit_time]] = False
			temp_selected[z_<=efit_reconstruction.lower_xpoint_z[i_efit_time]] = False
			temp_selected_sigma = cp.deepcopy(selected_sigma)
			temp_selected_sigma[voxels_centre[:,0]<=efit_reconstruction.lower_xpoint_r[i_efit_time]] = False
			temp_selected_sigma[voxels_centre[:,1]<=efit_reconstruction.lower_xpoint_z[i_efit_time]] = False
			outer_SOL_all.append(np.nansum(temp_selected*temp*2*np.pi*b*(grid_resolution**2)))
			temp_select = temp_selected_sigma*2*np.pi*(grid_resolution**2)*(voxels_centre[:,0])
			outer_SOL_sigma_all.append(np.nansum((temp1*temp_select).T*temp_select)**0.5)
		except Exception as e:
			print('failed')
			# logging.exception('with error: ' + str(e))
			inner_SOL_leg_all.append(np.nan)
			inner_SOL_leg_sigma_all.append(np.nan)
			inner_SOL_all.append(np.nan)
			inner_SOL_sigma_all.append(np.nan)
			outer_SOL_leg_all.append(np.nan)
			outer_SOL_leg_sigma_all.append(np.nan)
			outer_SOL_all.append(np.nan)
			outer_SOL_sigma_all.append(np.nan)
	inner_SOL_leg_all = np.array(inner_SOL_leg_all)
	inner_SOL_leg_sigma_all = np.array(inner_SOL_leg_sigma_all)
	inner_SOL_all = np.array(inner_SOL_all)
	inner_SOL_sigma_all = np.array(inner_SOL_sigma_all)
	outer_SOL_leg_all = np.array(outer_SOL_leg_all)
	outer_SOL_leg_sigma_all = np.array(outer_SOL_leg_sigma_all)
	outer_SOL_all = np.array(outer_SOL_all)
	outer_SOL_sigma_all = np.array(outer_SOL_sigma_all)

	return inner_SOL_leg_all,inner_SOL_leg_sigma_all,outer_SOL_leg_all,outer_SOL_leg_sigma_all,outer_SOL_all,outer_SOL_sigma_all,inner_SOL_all,inner_SOL_sigma_all,psiN_min_lower_baffle_all,psiN_min_lower_target_all,psiN_min_central_column_all,psiN_min_upper_target_all,psiN_min_upper_baffle_all


def equivalent_res_bolo_view(inverted_data,inverted_data_sigma,inversion_R,inversion_Z,time_full_binned_crop,efit_reconstruction,covariance_out,grid_data_masked_crop):
	# I copy the division of the regions outside the core from
	# Detachment evolution on the TCV tokamak, J.R.Harrison, 2017
	from shapely.geometry import Point
	from shapely.geometry.polygon import Polygon
	from scipy.ndimage import generic_filter
	import mastu_exhaust_analysis.fluxsurface_tracer as ft
	from mastu_exhaust_analysis.read_efit import read_epm
	from scipy.interpolate import interp1d,interp2d

	all_time_sep_r,all_time_sep_z,r_fine,z_fine = efit_reconstruction_to_separatrix_on_foil(efit_reconstruction)	# the separatrix are ordered as left_of_up_xpoints,right_of_up_xpoints,left_of_low_xpoints,right_of_low_xpoints
	all_time_strike_points_location = return_all_time_strike_points_location_radial(efit_reconstruction,all_time_sep_r,all_time_sep_z,r_fine,z_fine)
	fdir = uda_transfer(efit_reconstruction.shotnumber,'epm')
	efit_data = read_epm(fdir,calc_bfield=True)
	os.remove(fdir)

	dr = np.median(np.diff(inversion_R))
	dz = np.median(np.diff(inversion_Z))
	a,b = np.meshgrid(inversion_Z,inversion_R)
	a_flat = a.flatten()
	b_flat = b.flatten()
	grid_resolution = np.median(np.concatenate([np.diff(inversion_Z),np.diff(inversion_R)]))
	# leg_resolution = 0.1	# m
	# structure_radial_profile = return_structure_radial_profile()
	z_,r_ = np.meshgrid(inversion_Z,inversion_R)

	MU01_sxd_region_delimiter = return_MU01_sxd_region_delimiter()

	voxels_centre = np.mean(grid_data_masked_crop,axis=1)

	equivalent_res_bolo_view_all = []
	equivalent_res_bolo_view_sigma_all = []
	all_out_of_sxd_all = []
	all_out_of_sxd_sigma_all = []

	polygon = Polygon(FULL_MASTU_CORE_GRID_POLYGON)
	selected_VV = select_cells_inside_polygon(polygon,[inversion_R,inversion_Z])
	selected_sigma_VV = select_cells_inside_polygon(polygon,[voxels_centre])

	for i_t in range(len(time_full_binned_crop)):
		print('starting t=%.4gms' %(time_full_binned_crop[i_t]*1e3))
		try:
			i_efit_time = np.abs(efit_reconstruction.time-time_full_binned_crop[i_t]).argmin()

			selected = np.isfinite(inverted_data[0]).T
			# selected[inversion_Z<efit_data['lower_xpoint_z'][i_efit_time]]=False
			selected[inversion_Z>0]=False
			selected[:,inversion_R-grid_resolution/2<efit_data['rmidplaneIn'][i_efit_time]]=False
			selected = selected.T
			selected_sigma = np.isfinite(voxels_centre[:,0])
			# selected_sigma[voxels_centre[:,1]<efit_data['lower_xpoint_z'][i_efit_time]]=False
			selected_sigma[voxels_centre[:,0]-grid_resolution/2<efit_data['rmidplaneIn'][i_efit_time]]=False

			selected = np.logical_and(selected,selected_VV)
			selected_sigma = np.logical_and(selected_sigma,selected_sigma_VV)

			temp = cp.deepcopy(inverted_data[i_t])
			temp[:,inversion_Z>efit_reconstruction.mag_axis_z[i_efit_time]] = 0
			temp1 = covariance_out[i_t][:len(grid_data_masked_crop),:len(grid_data_masked_crop)]
			temp1[voxels_centre[:,1]>efit_reconstruction.mag_axis_z[i_efit_time]]=0
			temp1[:,voxels_centre[:,1]>efit_reconstruction.mag_axis_z[i_efit_time]]=0

			# equivalent_res_bolo_view
			temp_selected = cp.deepcopy(selected)
			# temp_selected[z_<leg_separator(r_)] = False
			temp_selected[z_<efit_reconstruction.lower_xpoint_z[i_efit_time]] = False
			temp_selected_sigma = cp.deepcopy(selected_sigma)
			# temp_selected_sigma[voxels_centre[:,1]<leg_separator(voxels_centre[:,0])] = False
			temp_selected_sigma[voxels_centre[:,1]<efit_reconstruction.lower_xpoint_z[i_efit_time]] = False
			equivalent_res_bolo_view_all.append(np.nansum(temp_selected*temp*2*np.pi*b*(grid_resolution**2)))
			temp_select = temp_selected_sigma*2*np.pi*(grid_resolution**2)*(voxels_centre[:,0])
			equivalent_res_bolo_view_sigma_all.append(np.nansum((temp1*temp_select).T*temp_select)**0.5)

			# equivalent_res_bolo_view
			temp_selected = cp.deepcopy(selected)
			# temp_selected[z_<leg_separator(r_)] = False
			# temp_selected[z_<efit_reconstruction.lower_xpoint_z[i_efit_time]] = False
			temp_selected[z_<MU01_sxd_region_delimiter(r_)] = False	# delimiter as used in the thesis
			temp_selected_sigma = cp.deepcopy(selected_sigma)
			# temp_selected_sigma[voxels_centre[:,1]<leg_separator(voxels_centre[:,0])] = False
			# temp_selected_sigma[voxels_centre[:,1]<efit_reconstruction.lower_xpoint_z[i_efit_time]] = False
			temp_selected_sigma[voxels_centre[:,1]<MU01_sxd_region_delimiter(voxels_centre[:,0])] = False	# delimiter as used in the thesis
			all_out_of_sxd_all.append(np.nansum(temp_selected*temp*2*np.pi*b*(grid_resolution**2)))
			temp_select = temp_selected_sigma*2*np.pi*(grid_resolution**2)*(voxels_centre[:,0])
			all_out_of_sxd_sigma_all.append(np.nansum((temp1*temp_select).T*temp_select)**0.5)

		except Exception as e:
			print('failed')
			# logging.exception('with error: ' + str(e))
			equivalent_res_bolo_view_all.append(np.nan)
			equivalent_res_bolo_view_sigma_all.append(np.nan)
			all_out_of_sxd_all.append(np.nan)
			all_out_of_sxd_sigma_all.append(np.nan)
	equivalent_res_bolo_view_all = np.array(equivalent_res_bolo_view_all)
	equivalent_res_bolo_view_sigma_all = np.array(equivalent_res_bolo_view_sigma_all)
	all_out_of_sxd_all = np.array(all_out_of_sxd_all)
	all_out_of_sxd_sigma_all = np.array(all_out_of_sxd_sigma_all)

	return equivalent_res_bolo_view_all,equivalent_res_bolo_view_sigma_all,all_out_of_sxd_all,all_out_of_sxd_sigma_all

def read_LP_data(shot,path = '/home/ffederic/work/irvb/from_pryan_LP',path_alternate='/common/uda-scratch/pryan',LP_file_type=''):
	from mastu_exhaust_analysis.pyLangmuirProbe import LangmuirProbe
	if LP_file_type == '':
		tag_cycle_all = ['elp','alp']
	else:
		tag_cycle_all = [str(LP_file_type[:-1])]	# use this is I already know the file type, maybe it saves time
	# try:
	for tag_cycle in tag_cycle_all:
		try:
			try:
				try:
					lp_data = LangmuirProbe(filename=path+'/'+tag_cycle+'0'+str(shot)+'.nc')
				except:
					lp_data = LangmuirProbe(filename=path_alternate+'/'+tag_cycle+'0'+str(shot)+'.nc')
			except:
				lp_data = LangmuirProbe(shot=shot,tag=tag_cycle)
			done = False	# 2025/02/26 this test gives a positive output even if some of the data is missing in the LP file, as long as some is present
			for divertor in ['lower','upper']:
				for sectors in [4,10]:
					try:
						output_contour1=lp_data.contour_plot(trange=[0,1.5],bad_probes=None,divertor=divertor, sectors=sectors, quantity = 'isat', coordinate='R',tiles=['C5','C6','T2','T3','T4','T5'],show=False)
						done = True
					except:
						pass
			if not done:
				dsaf = sdfdsf	# I want an error to be created
		except:
			try:
				lp_data = LangmuirProbe(filename=path+'/'+tag_cycle+'0'+str(shot)+'.nc',version='new')
				output_contour1=lp_data.contour_plot(trange=[0,1.5],bad_probes=None,divertor='lower', sectors=10, quantity = 'isat', coordinate='R',tiles=['C5','C6','T2','T3','T4','T5'],show=False)
				done = True
			except:
				lp_data = LangmuirProbe(filename=path+'/'+tag_cycle+'0'+str(shot)+'.nc',version='old')
				output_contour1=lp_data.contour_plot(trange=[0,1.5],bad_probes=None,divertor='lower', sectors=10, quantity = 'isat', coordinate='R',tiles=['C5','C6','T2','T3','T4','T5'],show=False)
				done = True
		if done:
			break
	# except:
	# 	tag_cycle='alp'
	# 	try:
	# 		try:
	# 			try:
	# 				lp_data = LangmuirProbe(filename=path+'/'+tag_cycle+'0'+str(shot)+'.nc')
	# 			except:
	# 				lp_data = LangmuirProbe(filename=path_alternate+'/'+tag_cycle+'0'+str(shot)+'.nc')
	# 		except:
	# 			lp_data = LangmuirProbe(shot=shot,tag=tag_cycle)
	# 		done = False	# 2025/02/26 this test gives a positive output even if some of the data is missing in the LP file, as long as some is present
	# 		for divertor in ['lower','upper']:
	# 			for sectors in [4,10]:
	# 				try:
	# 					output_contour1=lp_data.contour_plot(trange=[0,1.5],bad_probes=None,divertor=divertor, sectors=sectors, quantity = 'isat', coordinate='R',tiles=['C5','C6','T2','T3','T4','T5'],show=False)
	# 					done = True
	# 				except:
	# 					pass
	# 		if not done:
	# 			dsaf = sdfdsf	# I want an error to be created
	# 	except:
	# 		try:
	# 			lp_data = LangmuirProbe(filename=path+'/'+tag_cycle+'0'+str(shot)+'.nc',version='new')
	# 			output_contour1=lp_data.contour_plot(trange=[0,1.5],bad_probes=None,divertor='lower', sectors=10, quantity = 'isat', coordinate='R',tiles=['C5','C6','T2','T3','T4','T5'],show=False)
	# 		except:
	# 			lp_data = LangmuirProbe(filename=path+'/'+tag_cycle+'0'+str(shot)+'.nc',version='old')
	# 			output_contour1=lp_data.contour_plot(trange=[0,1.5],bad_probes=None,divertor='lower', sectors=10, quantity = 'isat', coordinate='R',tiles=['C5','C6','T2','T3','T4','T5'],show=False)
	return lp_data,output_contour1

def shift_beam_to_pulse_time(xnb_time, beam, shot):
	# modified function from Andrew Jackson 2022/06/08

	if beam not in ["SS", "SW"]:
		raise ValueError('Beam needs to be SS or SW')

	client_int = pyuda.Client()

	try:
		tssig = client_int.get(f"/XNB/{beam}/TSSIG", shot).data
	except pyuda.ServerException:
		print(f'No TSSIG for {beam} beam, setting shift to 0\n')
		shifted_time = xnb_time
		return shifted_time

	try:
		client_int.reset_connection()
		# reset_connection(client_int)
	except:
		pass
	# try:
	# 	del client_int
	# except:
	# 	pass


	# Smallest interval between data (times 2 to avoid rounding error)
	time_interval = (xnb_time[1] - xnb_time[0]) * 2

	# Times when signal is off
	times_when_off = xnb_time[np.argwhere(tssig < max(tssig)*0.9)[:, 0]]

	# Difference between adjacent times when off
	time_difference = times_when_off[1:] - times_when_off[:-1]

	# Find points where difference is larger than interval
	pulse_length = time_difference[np.argwhere(time_difference > time_interval)[:, 0]]
	pulse_time = times_when_off[np.argwhere(time_difference > time_interval)[:, 0]]

	# t = 0.004995 with pulse of length 0.015
	ref_pulse_length = 0.015
	ref_pulse_time = 0.004995

	# Find index of closest pulse length
	matching_pulse_index = np.argmin(abs(pulse_length - ref_pulse_length))

	# Time to shift by
	shift = ref_pulse_time - pulse_time[matching_pulse_index]

	print(f"Shifting {beam} NBI signal by {shift*1000:.3f}ms")
	# Shifted time
	shifted_time = xnb_time + shift

	# index_shift = np.argmin(abs(time - shift))
	# new_signal = np.roll(signal, int(shift/(time[1] - time[0])))

	return shifted_time


# 9/9/22 from James Harrison, functions to read fast camera data

def high_speed_visible_load_cih(filename):
	"""
	Load HSV metadata.

	source: https://github.com/ladisk/pyMRAW

	:param int/str shot: MAST-U shot number.
	:param str fpath_data: Path to data directory, see above.
	:return: dict of metadata.
	"""

	cih = dict()
	with open(filename, 'r') as f:
		for line in f:
			if line == '\n':  # end of cif header
				break
			line_sp = line.replace('\n', '').split(' : ')
			if len(line_sp) == 2:
				key, value = line_sp
				try:
					if '.' in value:
						value = float(value)
					else:
						value = int(value)
					cih[key] = value
				except:
					cih[key] = value
	return cih

def high_speed_visible_mraw_to_numpy(data_dir,filename,T0,downsample=1):
	full_filename = data_dir+filename+'.mraw'
	fname, ext = full_filename.split('.')
	cih = high_speed_visible_load_cih(fname+'.cih')
	num, denom = cih['Shutter Speed(s)'].split('/')
	cih['Shutter Speed(s)'] = float(num) / float(denom)
	nframes = cih['Total Frame']
	height = cih['Image Height']
	width = cih['Image Width']
	fps = cih['Record Rate(fps)']

	HEIGHT_FULL=1024
	WIDTH_FULL=1024


	xpix = np.arange(width) - width // 2 + WIDTH_FULL // 2
	ypix = np.arange(height)

	times = np.arange(nframes) * 1 / fps + T0
	with open(full_filename, 'rb') as f:
		data = np.frombuffer(f.read(), dtype=np.uint8)

	# optimised loading of 12-bit uint data from binary file into numpy array
	# source: https://stackoverflow.com/questions/44735756/python-reading-12-bit-binary-files
	fst_uint8, mid_uint8, lst_uint8 = np.reshape(data, (data.shape[0] // 3, 3)).T
	fst_uint12 = (fst_uint8.astype(np.uint16) << 4) + (mid_uint8.astype(np.uint16) >> 4)
	snd_uint12 = ((mid_uint8.astype(np.uint16) % 16) << 8) + lst_uint8.astype(np.uint16)
	data = np.reshape(np.concatenate((fst_uint12[:, None], snd_uint12[:, None]), axis=1), 2 * fst_uint12.shape[0])
	data = data.reshape([nframes, height, width, ])
	return data,times


def calculate_temperature_from_phantom(phantom_int,grid_data_masked_crop,grid_data,sensitivities_reshaped,params_BB,errparams_BB,dt,dx,foil_position_dict,diffusivity,thickness,emissivity,Ptthermalconductivity,photon_dict,shrink_factor_t,peak_oscillation_freq=29,ref_temperature = 25):
	original_x_optimal = np.concatenate([phantom_int,[0],[0]])
	x_optimal_input_full_res,recompose_voxel_emissivity_input = translate_emissivity_profile_with_homo_temp(np.mean(grid_data_masked_crop,axis=1),original_x_optimal,np.mean(grid_data,axis=1))

	if False:	# visualisation only
		plt.figure(figsize=(12,13))
		# plt.scatter(np.mean(grid_data_masked_crop,axis=1)[:,0],np.mean(grid_data_masked_crop,axis=1)[:,1],c=x_optimal,s=100,marker='s',cmap='rainbow')
		plt.imshow(np.flip(np.flip(np.flip(np.transpose(recompose_voxel_emissivity_input,(1,0)),axis=1),axis=1),axis=0),'rainbow',extent=[grid_data[:,:,0].min(),grid_data[:,:,0].max(),grid_data[:,:,1].min(),grid_data[:,:,1].max()])
		plt.plot(_MASTU_CORE_GRID_POLYGON[:, 0], _MASTU_CORE_GRID_POLYGON[:, 1], 'k')
		plt.plot(FULL_MASTU_CORE_GRID_POLYGON[:, 0], FULL_MASTU_CORE_GRID_POLYGON[:, 1], 'k')
		temp = np.abs(efit_reconstruction.time-time_full_binned_crop[i_t]).argmin()
		try:
			for i in range(len(all_time_sep_r[temp])):
				plt.plot(r_fine[all_time_sep_r[temp][i]],z_fine[all_time_sep_z[temp][i]],'--b')
		except:
			pass
		plt.plot(efit_reconstruction.lower_xpoint_r[temp],efit_reconstruction.lower_xpoint_z[temp],'xr')
		plt.plot(efit_reconstruction.strikepointR[temp],efit_reconstruction.strikepointZ[temp],'xr')
		plt.colorbar().set_label('input emissivity [W/m3]')
		plt.ylim(top=0.5)
		plt.title(csv_file.name[-60:-28])
		plt.pause(0.01)
	else:
		pass

	# time = time_full_binned_crop[i_t]
	fitted_foil_power_full_res_no_homo = (np.dot(sensitivities_reshaped,x_optimal_input_full_res[:-2])).reshape(sensitivities_reshaped.shape[:-1])#+x_optimal_input_full_res[-2]*select_foil_region_with_plasma_full_res*homogeneous_scaling+x_optimal_input_full_res[-1]*(selected_ROI_full_res.flatten())*homogeneous_scaling).reshape(selected_ROI_full_res.shape)
	fitted_foil_power_full_res_no_homo = np.flip(fitted_foil_power_full_res_no_homo,axis=1)
	temperature_full_resolution = np.zeros(np.array(np.shape(fitted_foil_power_full_res_no_homo))+2)
	horizontal_coord = np.arange(np.shape(temperature_full_resolution)[1])
	vertical_coord = np.arange(np.shape(temperature_full_resolution)[0])
	horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
	grid = np.array([[horizontal_coord.flatten()]*4,[vertical_coord.flatten()]*4]).T
	grid_laplacian = -build_laplacian(grid,diagonal_factor=0.5) / (dx**2) / 2	# the /2 comes from the fact that including the diagonals amounts to double counting, so i do a mean by summing half of it
	# ref_temperature = 25
	# thickness=2.0531473351462095e-06
	# emissivity=0.9999999999999
	# diffusivity=1.0283685197530968e-05
	# from 2022/09/29, Laser_data_analysis3_3.py
	temperature_evolution = []
	temperature_evolution.append(temperature_full_resolution)
	guess = temperature_full_resolution
	time_full_res_int = []
	time_full_res_int.append(0)
	for i in range(shrink_factor_t*3):
		x_optimal, y_opt, opt_info = scipy.optimize.fmin_l_bfgs_b(find_temperature_from_power_residuals(dt,grid_laplacian,fitted_foil_power_full_res_no_homo,temperature_evolution[-1],ref_temperature=25,thickness=thickness,emissivity=emissivity,diffusivity=diffusivity,Ptthermalconductivity=Ptthermalconductivity), x0=guess[1:-1,1:-1], iprint=2, factr=1e7,pgtol=1e-6)#,m=1000, maxls=1000, pgtol=1e-10, factr=1e0)#,approx_grad = True)
		temperature_full_resolution = np.zeros_like(temperature_evolution[0])
		temperature_full_resolution[1:-1,1:-1] = x_optimal.reshape(temperature_full_resolution[1:-1,1:-1].shape)
		temperature_evolution.append(temperature_full_resolution)
		time_full_res_int.append((i+1)*dt)
		guess = (temperature_evolution[-1]-temperature_evolution[0])/(time_full_res_int[-1]/dt) + temperature_evolution[-1]
	temperature_evolution = np.array(temperature_evolution)
	time_full_res_int = np.array(time_full_res_int)

	if False:	# only to check derivatives
		target = 2
		scale = 1e-5
		# guess[target] = 1e5
		temp1 = find_temperature_from_power_residuals(dt,grid_laplacian,fitted_foil_power_full_res_no_homo,temperature_full_resolution)(guess)
		guess[target,10] +=scale
		temp2 = find_temperature_from_power_residuals(dt,grid_laplacian,fitted_foil_power_full_res_no_homo,temperature_full_resolution)(guess)
		guess[target,10] += -2*scale
		temp3 = find_temperature_from_power_residuals(dt,grid_laplacian,fitted_foil_power_full_res_no_homo,temperature_full_resolution)(guess)
		guess[target,10] += scale
		print('calculated derivated of %.7g vs true of %.7g' %(temp1[1][np.ravel_multi_index((target,10),np.shape(guess))],((temp2[0]-temp3[0])/(2*scale))))


	temperature_evolution = np.array(temperature_evolution)
	temperature_evolution += ref_temperature
	temperature_evolution = temperature_evolution[:,1:-1,1:-1]	# because I added a pixel with fixed temperature before, and now I remove it
	# temperature_evolution = np.flip(temperature_evolution,axis=2)


	# ani = coleval.movie_from_data(np.array([np.flip(np.transpose(temperature_evolution,(0,2,1)),axis=2)]), 1/np.median(np.diff(time_full_res)),integration=laser_int_time/1000,time_offset=time_full_res[0],xlabel='horizontal coord [pixels]', ylabel='vertical coord [pixels]',barlabel='brightness [W/m2]', prelude='shot ' + laser_to_analyse[-9:-4]+'\n'+binning_type+'\n',overlay_structure=True,include_EFIT=True,pulse_ID=laser_to_analyse[-9:-4],overlay_x_point=True,overlay_mag_axis=True,overlay_strike_points=True,overlay_separatrix=True)

	photon_flux_interpolator = photon_dict['photon_flux_interpolator']
	photon_flux_over_temperature_interpolator = photon_dict['photon_flux_over_temperature_interpolator']


	counts_full_resolution = params_BB[:,:,0]*photon_flux_interpolator(temperature_evolution)+params_BB[:,:,1]
	counts_full_resolution_std = estimate_counts_std(counts_full_resolution,int_time=2)
	counts_full_resolution_with_noise = np.random.normal(loc=counts_full_resolution,scale=counts_full_resolution_std/(2**0.5))	# the /2 coles so that I don't need to calculate the 2 digitizers
	# peak_oscillation_freq=29	# Hz
	counts_full_resolution_with_noise = (counts_full_resolution_with_noise.T + 2*np.sin(2*np.pi*peak_oscillation_freq*time_full_res_int)).T	# I add the oscillation
	temperature_evolution_with_noise,temperature_std_crop = count_to_temp_BB_multi_digitizer_no_reference([counts_full_resolution_with_noise],[counts_full_resolution_std],[params_BB],[errparams_BB],[0],inttime=2)

	temperature_evolution_with_noise = temperature_evolution_with_noise[0]
	temperature_std_crop = temperature_std_crop[0]
	return temperature_evolution_with_noise,temperature_std_crop,peak_oscillation_freq,ref_temperature,counts_full_resolution_std,time_full_res_int

def calculate_temperature_from_phantom2(phantom_int,grid_data_masked_crop,grid_data,sensitivities_reshaped,params_BB,errparams_BB,dt,dx,foil_position_dict,diffusivity,thickness,emissivity,Ptthermalconductivity,photon_dict,shrink_factor_t,peak_oscillation_freq=29,ref_temperature = 25):
	# copied from calculate_temperature_from_phantom
	# but here, to increase the signal, rather than calculating the temperature increase at each step from the previous, I do it from the initial state.
	# the later stage have a much larger signal to use, so it should converge much quicker!
	# warning! this requires phantoms in W/m^3/st NOT W/m^3

	original_x_optimal = np.concatenate([phantom_int,[0],[0]])
	x_optimal_input_full_res,recompose_voxel_emissivity_input = translate_emissivity_profile_with_homo_temp(np.mean(grid_data_masked_crop,axis=1),original_x_optimal,np.mean(grid_data,axis=1))

	if False:	# visualisation only
		plt.figure(figsize=(12,13))
		# plt.scatter(np.mean(grid_data_masked_crop,axis=1)[:,0],np.mean(grid_data_masked_crop,axis=1)[:,1],c=x_optimal,s=100,marker='s',cmap='rainbow')
		plt.imshow(np.flip(np.flip(np.flip(np.transpose(recompose_voxel_emissivity_input,(1,0)),axis=1),axis=1),axis=0),'rainbow',extent=[grid_data[:,:,0].min(),grid_data[:,:,0].max(),grid_data[:,:,1].min(),grid_data[:,:,1].max()])
		plt.plot(_MASTU_CORE_GRID_POLYGON[:, 0], _MASTU_CORE_GRID_POLYGON[:, 1], 'k')
		plt.plot(FULL_MASTU_CORE_GRID_POLYGON[:, 0], FULL_MASTU_CORE_GRID_POLYGON[:, 1], 'k')
		temp = np.abs(efit_reconstruction.time-time_full_binned_crop[i_t]).argmin()
		try:
			for i in range(len(all_time_sep_r[temp])):
				plt.plot(r_fine[all_time_sep_r[temp][i]],z_fine[all_time_sep_z[temp][i]],'--b')
		except:
			pass
		plt.plot(efit_reconstruction.lower_xpoint_r[temp],efit_reconstruction.lower_xpoint_z[temp],'xr')
		plt.plot(efit_reconstruction.strikepointR[temp],efit_reconstruction.strikepointZ[temp],'xr')
		plt.colorbar().set_label('input emissivity [W/m3]')
		plt.ylim(top=0.5)
		plt.title(csv_file.name[-60:-28])
		plt.pause(0.01)
	else:
		pass

	# time = time_full_binned_crop[i_t]
	fitted_foil_power_full_res_no_homo = (np.dot(sensitivities_reshaped,x_optimal_input_full_res[:-2])).reshape(sensitivities_reshaped.shape[:-1])#+x_optimal_input_full_res[-2]*select_foil_region_with_plasma_full_res*homogeneous_scaling+x_optimal_input_full_res[-1]*(selected_ROI_full_res.flatten())*homogeneous_scaling).reshape(selected_ROI_full_res.shape)
	fitted_foil_power_full_res_no_homo = np.flip(fitted_foil_power_full_res_no_homo,axis=1)
	temperature_full_resolution = np.zeros(np.array(np.shape(fitted_foil_power_full_res_no_homo))+2)
	horizontal_coord = np.arange(np.shape(temperature_full_resolution)[1])
	vertical_coord = np.arange(np.shape(temperature_full_resolution)[0])
	horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
	grid = np.array([[horizontal_coord.flatten()]*4,[vertical_coord.flatten()]*4]).T
	grid_laplacian = -build_laplacian(grid,diagonal_factor=0.5) / (dx**2) / 2	# the /2 comes from the fact that including the diagonals amounts to double counting, so i do a mean by summing half of it
	# ref_temperature = 25
	# thickness=2.0531473351462095e-06
	# emissivity=0.9999999999999
	# diffusivity=1.0283685197530968e-05
	# from 2022/09/29, Laser_data_analysis3_3.py
	temperature_evolution = []
	temperature_evolution.append(temperature_full_resolution)
	guess = temperature_full_resolution
	time_full_res_int = []
	time_full_res_int.append(0)
	for i in range(shrink_factor_t*3):
		x_optimal, y_opt, opt_info = scipy.optimize.fmin_l_bfgs_b(find_temperature_from_power_residuals((i+1)*dt,grid_laplacian,fitted_foil_power_full_res_no_homo,temperature_evolution[0],ref_temperature=25,thickness=thickness,emissivity=emissivity,diffusivity=diffusivity,Ptthermalconductivity=Ptthermalconductivity), x0=guess[1:-1,1:-1], iprint=0, factr=1e10,pgtol=1e-7)#,m=1000, maxls=1000, pgtol=1e-10, factr=1e0)#,approx_grad = True)
		temperature_full_resolution = np.zeros_like(temperature_evolution[0])
		temperature_full_resolution[1:-1,1:-1] = x_optimal.reshape(temperature_full_resolution[1:-1,1:-1].shape)
		temperature_evolution.append(temperature_full_resolution)
		time_full_res_int.append((i+1)*dt)
		guess = (temperature_evolution[-1]-temperature_evolution[0])/(time_full_res_int[-1]/dt) + temperature_evolution[-1]
	temperature_evolution = np.array(temperature_evolution)
	time_full_res_int = np.array(time_full_res_int)

	if False:	# only to check derivatives
		target = 2
		scale = 1e-5
		# guess[target] = 1e5
		temp1 = find_temperature_from_power_residuals(dt,grid_laplacian,fitted_foil_power_full_res_no_homo,temperature_full_resolution)(guess)
		guess[target,10] +=scale
		temp2 = find_temperature_from_power_residuals(dt,grid_laplacian,fitted_foil_power_full_res_no_homo,temperature_full_resolution)(guess)
		guess[target,10] += -2*scale
		temp3 = find_temperature_from_power_residuals(dt,grid_laplacian,fitted_foil_power_full_res_no_homo,temperature_full_resolution)(guess)
		guess[target,10] += scale
		print('calculated derivated of %.7g vs true of %.7g' %(temp1[1][np.ravel_multi_index((target,10),np.shape(guess))],((temp2[0]-temp3[0])/(2*scale))))


	temperature_evolution = np.array(temperature_evolution)
	temperature_evolution += ref_temperature
	temperature_evolution = temperature_evolution[:,1:-1,1:-1]	# because I added a pixel with fixed temperature before, and now I remove it
	# temperature_evolution = np.flip(temperature_evolution,axis=2)


	# ani = coleval.movie_from_data(np.array([np.flip(np.transpose(temperature_evolution,(0,2,1)),axis=2)]), 1/np.median(np.diff(time_full_res)),integration=laser_int_time/1000,time_offset=time_full_res[0],xlabel='horizontal coord [pixels]', ylabel='vertical coord [pixels]',barlabel='brightness [W/m2]', prelude='shot ' + laser_to_analyse[-9:-4]+'\n'+binning_type+'\n',overlay_structure=True,include_EFIT=True,pulse_ID=laser_to_analyse[-9:-4],overlay_x_point=True,overlay_mag_axis=True,overlay_strike_points=True,overlay_separatrix=True)

	photon_flux_interpolator = photon_dict['photon_flux_interpolator']
	photon_flux_over_temperature_interpolator = photon_dict['photon_flux_over_temperature_interpolator']


	counts_full_resolution = params_BB[:,:,0]*photon_flux_interpolator(temperature_evolution)+params_BB[:,:,1]
	counts_full_resolution_std = estimate_counts_std(counts_full_resolution,int_time=2)/(2**0.5)	# the /2 coles so that I don't need to calculate the 2 digitizers
	print(np.max(counts_full_resolution_std))
	print(np.min(counts_full_resolution_std))
	counts_full_resolution_with_noise = np.random.normal(loc=counts_full_resolution,scale=counts_full_resolution_std)
	# peak_oscillation_freq=29	# Hz
	counts_full_resolution_with_noise = (counts_full_resolution_with_noise.T + 2*np.sin(2*np.pi*peak_oscillation_freq*time_full_res_int)).T	# I add the oscillation
	temperature_evolution_with_noise,temperature_std_crop = count_to_temp_BB_multi_digitizer_no_reference([counts_full_resolution_with_noise],[counts_full_resolution_std],[params_BB],[errparams_BB],[0],inttime=2)

	temperature_evolution_with_noise = temperature_evolution_with_noise[0]
	temperature_std_crop = temperature_std_crop[0]
	return temperature_evolution_with_noise,temperature_std_crop,peak_oscillation_freq,ref_temperature,counts_full_resolution_std,time_full_res_int




def from_temperature_to_power_on_foil(temperature_evolution_with_noise,counts_full_resolution_std,peak_oscillation_freq,dt,shrink_factor_t,shrink_factor_x,params_BB,errparams_BB,emissivity,thickness,diffusivity,Ptthermalconductivity,sigma_thickness,sigma_rec_diffusivity,sigma_emissivity,ref_temperature,temperature_std_crop,time_full_res_int,foil_position_dict,photon_dict):

	frames_to_average = 1/peak_oscillation_freq/dt
	temperature_evolution_with_noise = real_mean_filter_agent(temperature_evolution_with_noise,frames_to_average)

	temperature_crop_binned,nan_ROI_mask = proper_homo_binning_t_2D(temperature_evolution_with_noise,shrink_factor_t,shrink_factor_x)
	temperature_minus_background_crop_binned = temperature_crop_binned-ref_temperature
	counts_std_crop_binned = 1/(shrink_factor_t*shrink_factor_x**2)*(proper_homo_binning_t_2D(counts_full_resolution_std**2,shrink_factor_t,shrink_factor_x,type='np.nansum')[0]**0.5)
	averaged_BB_proportional_crop = params_BB[:,:,0]
	averaged_BB_proportional_std_crop = errparams_BB[:,:,0,0]
	averaged_BB_proportional_crop_binned = proper_homo_binning_2D(averaged_BB_proportional_crop,shrink_factor_x)
	averaged_BB_proportional_std_crop_binned = 1/(shrink_factor_x**2)*(proper_homo_binning_2D(averaged_BB_proportional_std_crop**2,shrink_factor_x,type='np.nansum')**0.5)


	temp_ref_counts_std_crop_binned = 5.077188644392399/((shrink_factor_t**0.5)*shrink_factor_x)*np.ones_like(temperature_minus_background_crop_binned)/(2**0.5)	# the /2 coles so that I don't need to calculate the 2 digitizers
	# photon_flux_over_temperature_interpolator = interp1d([20,30,40,50],[2.22772831e+14, 3.10040403e+14, 4.22648744e+14, 5.65430883e+14],fill_value="extrapolate",bounds_error=False)
	# photon_flux_over_temperature = photon_flux_over_temperature_interpolator(temperature_evolution_with_noise)
	photon_flux_over_temperature_interpolator = photon_dict['photon_flux_over_temperature_interpolator']
	# temperature_std_crop = counts_full_resolution_std/(photon_flux_over_temperature*np.mean(averaged_BB_proportional_crop_binned))
	temperature_std_crop_binned = 1/(shrink_factor_t*shrink_factor_x**2)*(proper_homo_binning_t_2D(temperature_std_crop**2,shrink_factor_t,shrink_factor_x,type='np.nansum')[0]**0.5)
	time_binned = proper_homo_binning_t(time_full_res_int,shrink_factor_t)

	dx=foil_position_dict['foilhorizw']/foil_position_dict['foilhorizwpixel']*shrink_factor_x
	horizontal_coord = np.arange(np.shape(temperature_minus_background_crop_binned)[2])
	vertical_coord = np.arange(np.shape(temperature_minus_background_crop_binned)[1])
	horizontal_coord,vertical_coord = np.meshgrid(horizontal_coord,vertical_coord)
	grid_binned = np.array([[horizontal_coord.flatten()]*4,[vertical_coord.flatten()]*4]).T
	grid_laplacian_binned = -build_laplacian(grid_binned,diagonal_factor=0.5) / (dx**2) / 2	# the /2 comes from the fact that including the diagonals amounts to double counting, so i do a mean by summing half of it

	dTdt,dTdt_std,d2Tdxy,d2Tdxy_std,negd2Tdxy,negd2Tdxy_std,T4_T04,T4_T04_std = calc_temp_to_power_BB_1(photon_flux_over_temperature_interpolator,temperature_minus_background_crop_binned,ref_temperature,time_binned,dx,counts_std_crop_binned,averaged_BB_proportional_crop_binned,averaged_BB_proportional_std_crop_binned,temp_ref_counts_std_crop_binned,temperature_std_crop_binned,nan_ROI_mask,grid_laplacian=grid_laplacian_binned)
	BBrad,diffusion,timevariation,powernoback_full,BBrad_std,diffusion_std,timevariation_std,powernoback_full_std = calc_temp_to_power_BB_2(dTdt,dTdt_std,d2Tdxy,d2Tdxy_std,negd2Tdxy,negd2Tdxy_std,T4_T04,T4_T04_std,nan_ROI_mask,emissivity,thickness,1/diffusivity,Ptthermalconductivity)

	powernoback_full_std = ( (diffusion**2)*((diffusion_std/diffusion)**2+sigma_thickness**2) + (timevariation**2)*((timevariation_std/timevariation)**2+sigma_thickness**2+sigma_rec_diffusivity**2) + (BBrad**2)*((BBrad_std/BBrad)**2+sigma_emissivity**2) )**0.5

	time_binned = time_binned[1:-1]

	return time_binned,powernoback_full,powernoback_full_std

######################################################################################################

def peak_prominences_as_area(data,peaks):
	# The area is defined as the area above the line that unites the two neibouring trough around the peak
	data = np.array(data)
	peaks = np.array(peaks)
	num_peaks = len(peaks)
	prominence = []
	if num_peaks==0:
		return prominence
	elif num_peaks==1:
		start = data[:peaks[0]].argmin()
		end = peaks[0]+data[peaks[0]:].argmin()
		prominence.append(np.trapz(data[start:end]) + (end-start)*(np.abs(data[start]-data[end]))/2)
	elif num_peaks>0:
		start = data[:peaks[0]].argmin()
		end = peaks[0]+data[peaks[0]:peaks[1]].argmin()
		prominence.append(np.trapz(data[start:end]) + (end-start)*(np.abs(data[start]-data[end]))/2)
	if num_peaks>2:
		for i in np.arange(1,num_peaks-1):
			start = peaks[i-1]+data[peaks[i-1]:peaks[i]].argmin()
			end = peaks[i]+data[peaks[i]:peaks[i+1]].argmin()
			prominence.append(np.trapz(data[start:end]) + (end-start)*(np.abs(data[start]-data[end]))/2)
	if num_peaks>1:
		start = peaks[-2]+data[peaks[-2]:peaks[-1]].argmin()
		end = peaks[-1]+data[peaks[-1]:].argmin()
		prominence.append(np.trapz(data[start:end]) + (end-start)*(np.abs(data[start]-data[end]))/2)
	return np.array(prominence)

#########################################################################################################

def np_trapz_error(sigma,x=[None]):
	# error propagation of the np.trapz function
	if list(x)==[None]:
		x = np.arange(len(sigma))
	sigma = np.array(sigma)
	x = np.array(x)
	if len(sigma)<=1:
		int_sigma = 0
	else:
		int_sigma = np.nansum((sigma[:-1]**2 + sigma[1:]**2)*(np.diff(x)**2)/4)**0.5
	return int_sigma


def select_cells_inside_polygon(polygon,geometry,center_line = []):
	from shapely.geometry import Point
	from shapely.geometry.polygon import Polygon
	if len(geometry)==2:
		inversion_R = geometry[0]
		inversion_Z = geometry[1]
		selected = np.zeros((len(inversion_R),len(inversion_Z))).astype(bool)
		polygon_bounds = polygon.bounds
		for i_z in range(len(inversion_Z)):
			if inversion_Z[i_z]>polygon_bounds[1] and inversion_Z[i_z]<polygon_bounds[3]:
				if len(center_line)==0:
					for i_r in range(len(inversion_R)):
						if inversion_R[i_r]>polygon_bounds[0] and inversion_R[i_r]<polygon_bounds[2]:
							point = Point((inversion_R[i_r],inversion_Z[i_z]))
							if polygon.contains(point):
								selected[i_r,i_z] = True
				else:
					start = np.abs(center_line[:,0][np.abs(center_line[:,1] - inversion_Z[i_z]).argmin()] - inversion_R).argmin()
					delta = 0
					there_was_at_least_one_true = False
					while start-delta>0:
						point = Point((inversion_R[start-delta],inversion_Z[i_z]))
						if polygon.contains(point):
							selected[start-delta,i_z] = True
							there_was_at_least_one_true = True
						elif there_was_at_least_one_true:
							break
						delta += 1
					delta = 0
					there_was_at_least_one_true = False
					while start+delta<len(inversion_R):
						point = Point((inversion_R[start+delta],inversion_Z[i_z]))
						if polygon.contains(point):
							selected[start+delta,i_z] = True
							there_was_at_least_one_true = True
						elif there_was_at_least_one_true:
							break
						delta += 1
		return selected
	elif len(geometry)==1:
		geometry = geometry[0]
		selected = np.zeros_like(geometry[:,0]).astype(bool)
		for i_ in range(len(selected)):
			point = Point((geometry[i_,0],geometry[i_,1]))
			if polygon.contains(point):
				selected[i_] = True
		return selected
	else:
		print('only allowed options in select_cells_inside_polygon is [inversion_R,inversion_Z] or [np.mean(grid_data,axis=1)]')
		exit()

#######################################################################

def savez_protocol4(file, *args, **kwds):
	# code copyed from numpy/npyio and format , python 3.9
	allow_pickle=True
	pickle_kwargs=None
	compress=False

	from numpy.compat import (
		asbytes, asstr, asunicode, os_fspath, os_PathLike,
		pickle
		)
	import zipfile

	if not hasattr(file, 'write'):
		file = os_fspath(file)
		if not file.endswith('.npz'):
			file = file + '.npz'

	namedict = kwds
	for i, val in enumerate(args):
		key = 'arr_%d' % i
		if key in namedict.keys():
			raise ValueError(
				"Cannot use un-named variables and keyword %s" % key)
		namedict[key] = val

	if compress:
		compression = zipfile.ZIP_DEFLATED
	else:
		compression = zipfile.ZIP_STORED

	def zipfile_factory(file, *args, **kwargs):
	    """
	    Create a ZipFile.

	    Allows for Zip64, and the `file` argument can accept file, str, or
	    pathlib.Path objects. `args` and `kwargs` are passed to the zipfile.ZipFile
	    constructor.
	    """
	    if not hasattr(file, 'read'):
	        file = os_fspath(file)
	    import zipfile
	    kwargs['allowZip64'] = True
	    return zipfile.ZipFile(file, *args, **kwargs)

	zipf = zipfile_factory(file, mode="w", compression=compression)

	# functions from format.py
	import numpy
	import io
	import warnings
	from numpy.lib.utils import safe_eval
	from numpy.compat import (
		isfileobj, os_fspath, pickle
		)


	__all__ = []


	EXPECTED_KEYS = {'descr', 'fortran_order', 'shape'}
	MAGIC_PREFIX = b'\x93NUMPY'
	MAGIC_LEN = len(MAGIC_PREFIX) + 2
	ARRAY_ALIGN = 64 # plausible values are powers of 2 between 16 and 4096
	BUFFER_SIZE = 2**18  # size of buffer for reading npz files in bytes

	# difference between version 1.0 and 2.0 is a 4 byte (I) header length
	# instead of 2 bytes (H) allowing storage of large structured arrays
	_header_size_info = {
		(1, 0): ('<H', 'latin1'),
		(2, 0): ('<I', 'latin1'),
		(3, 0): ('<I', 'utf8'),
	}


	def _check_version(version):
		if version not in [(1, 0), (2, 0), (3, 0), None]:
			msg = "we only support format version (1,0), (2,0), and (3,0), not %s"
			raise ValueError(msg % (version,))

	def magic(major, minor):
		""" Return the magic string for the given file format version.

		Parameters
		----------
		major : int in [0, 255]
		minor : int in [0, 255]

		Returns
		-------
		magic : str

		Raises
		------
		ValueError if the version cannot be formatted.
		"""
		if major < 0 or major > 255:
			raise ValueError("major version must be 0 <= major < 256")
		if minor < 0 or minor > 255:
			raise ValueError("minor version must be 0 <= minor < 256")
		return MAGIC_PREFIX + bytes([major, minor])

	def read_magic(fp):
		""" Read the magic string to get the version of the file format.

		Parameters
		----------
		fp : filelike object

		Returns
		-------
		major : int
		minor : int
		"""
		magic_str = _read_bytes(fp, MAGIC_LEN, "magic string")
		if magic_str[:-2] != MAGIC_PREFIX:
			msg = "the magic string is not correct; expected %r, got %r"
			raise ValueError(msg % (MAGIC_PREFIX, magic_str[:-2]))
		major, minor = magic_str[-2:]
		return major, minor

	def _has_metadata(dt):
		if dt.metadata is not None:
			return True
		elif dt.names is not None:
			return any(_has_metadata(dt[k]) for k in dt.names)
		elif dt.subdtype is not None:
			return _has_metadata(dt.base)
		else:
			return False

	def dtype_to_descr(dtype):
		"""
		Get a serializable descriptor from the dtype.

		The .descr attribute of a dtype object cannot be round-tripped through
		the dtype() constructor. Simple types, like dtype('float32'), have
		a descr which looks like a record array with one field with '' as
		a name. The dtype() constructor interprets this as a request to give
		a default name.  Instead, we construct descriptor that can be passed to
		dtype().

		Parameters
		----------
		dtype : dtype
			The dtype of the array that will be written to disk.

		Returns
		-------
		descr : object
			An object that can be passed to `numpy.dtype()` in order to
			replicate the input dtype.

		"""
		if _has_metadata(dtype):
			warnings.warn("metadata on a dtype may be saved or ignored, but will "
						  "raise if saved when read. Use another form of storage.",
						  UserWarning, stacklevel=2)
		if dtype.names is not None:
			# This is a record array. The .descr is fine.  XXX: parts of the
			# record array with an empty name, like padding bytes, still get
			# fiddled with. This needs to be fixed in the C implementation of
			# dtype().
			return dtype.descr
		else:
			return dtype.str

	def descr_to_dtype(descr):
		"""
		Returns a dtype based off the given description.

		This is essentially the reverse of `dtype_to_descr()`. It will remove
		the valueless padding fields created by, i.e. simple fields like
		dtype('float32'), and then convert the description to its corresponding
		dtype.

		Parameters
		----------
		descr : object
			The object retreived by dtype.descr. Can be passed to
			`numpy.dtype()` in order to replicate the input dtype.

		Returns
		-------
		dtype : dtype
			The dtype constructed by the description.

		"""
		if isinstance(descr, str):
			# No padding removal needed
			return numpy.dtype(descr)
		elif isinstance(descr, tuple):
			# subtype, will always have a shape descr[1]
			dt = descr_to_dtype(descr[0])
			return numpy.dtype((dt, descr[1]))

		titles = []
		names = []
		formats = []
		offsets = []
		offset = 0
		for field in descr:
			if len(field) == 2:
				name, descr_str = field
				dt = descr_to_dtype(descr_str)
			else:
				name, descr_str, shape = field
				dt = numpy.dtype((descr_to_dtype(descr_str), shape))

			# Ignore padding bytes, which will be void bytes with '' as name
			# Once support for blank names is removed, only "if name == ''" needed)
			is_pad = (name == '' and dt.type is numpy.void and dt.names is None)
			if not is_pad:
				title, name = name if isinstance(name, tuple) else (None, name)
				titles.append(title)
				names.append(name)
				formats.append(dt)
				offsets.append(offset)
			offset += dt.itemsize

		return numpy.dtype({'names': names, 'formats': formats, 'titles': titles,
							'offsets': offsets, 'itemsize': offset})

	def header_data_from_array_1_0(array):
		""" Get the dictionary of header metadata from a numpy.ndarray.

		Parameters
		----------
		array : numpy.ndarray

		Returns
		-------
		d : dict
			This has the appropriate entries for writing its string representation
			to the header of the file.
		"""
		d = {'shape': array.shape}
		if array.flags.c_contiguous:
			d['fortran_order'] = False
		elif array.flags.f_contiguous:
			d['fortran_order'] = True
		else:
			# Totally non-contiguous data. We will have to make it C-contiguous
			# before writing. Note that we need to test for C_CONTIGUOUS first
			# because a 1-D array is both C_CONTIGUOUS and F_CONTIGUOUS.
			d['fortran_order'] = False

		d['descr'] = dtype_to_descr(array.dtype)
		return d


	def _wrap_header(header, version):
		"""
		Takes a stringified header, and attaches the prefix and padding to it
		"""
		import struct
		assert version is not None
		fmt, encoding = _header_size_info[version]
		if not isinstance(header, bytes):  # always true on python 3
			header = header.encode(encoding)
		hlen = len(header) + 1
		padlen = ARRAY_ALIGN - ((MAGIC_LEN + struct.calcsize(fmt) + hlen) % ARRAY_ALIGN)
		try:
			header_prefix = magic(*version) + struct.pack(fmt, hlen + padlen)
		except struct.error:
			msg = "Header length {} too big for version={}".format(hlen, version)
			raise ValueError(msg) from None

		# Pad the header with spaces and a final newline such that the magic
		# string, the header-length short and the header are aligned on a
		# ARRAY_ALIGN byte boundary.  This supports memory mapping of dtypes
		# aligned up to ARRAY_ALIGN on systems like Linux where mmap()
		# offset must be page-aligned (i.e. the beginning of the file).
		return header_prefix + header + b' '*padlen + b'\n'


	def _wrap_header_guess_version(header):
		"""
		Like `_wrap_header`, but chooses an appropriate version given the contents
		"""
		try:
			return _wrap_header(header, (1, 0))
		except ValueError:
			pass

		try:
			ret = _wrap_header(header, (2, 0))
		except UnicodeEncodeError:
			pass
		else:
			warnings.warn("Stored array in format 2.0. It can only be"
						  "read by NumPy >= 1.9", UserWarning, stacklevel=2)
			return ret

		header = _wrap_header(header, (3, 0))
		warnings.warn("Stored array in format 3.0. It can only be "
					  "read by NumPy >= 1.17", UserWarning, stacklevel=2)
		return header


	def _write_array_header(fp, d, version=None):
		""" Write the header for an array and returns the version used

		Parameters
		----------
		fp : filelike object
		d : dict
			This has the appropriate entries for writing its string representation
			to the header of the file.
		version: tuple or None
			None means use oldest that works
			explicit version will raise a ValueError if the format does not
			allow saving this data.  Default: None
		"""
		header = ["{"]
		for key, value in sorted(d.items()):
			# Need to use repr here, since we eval these when reading
			header.append("'%s': %s, " % (key, repr(value)))
		header.append("}")
		header = "".join(header)
		if version is None:
			header = _wrap_header_guess_version(header)
		else:
			header = _wrap_header(header, version)
		fp.write(header)

	def write_array_header_1_0(fp, d):
		""" Write the header for an array using the 1.0 format.

		Parameters
		----------
		fp : filelike object
		d : dict
			This has the appropriate entries for writing its string
			representation to the header of the file.
		"""
		_write_array_header(fp, d, (1, 0))


	def write_array_header_2_0(fp, d):
		""" Write the header for an array using the 2.0 format.
			The 2.0 format allows storing very large structured arrays.

		.. versionadded:: 1.9.0

		Parameters
		----------
		fp : filelike object
		d : dict
			This has the appropriate entries for writing its string
			representation to the header of the file.
		"""
		_write_array_header(fp, d, (2, 0))

	def read_array_header_1_0(fp):
		"""
		Read an array header from a filelike object using the 1.0 file format
		version.

		This will leave the file object located just after the header.

		Parameters
		----------
		fp : filelike object
			A file object or something with a `.read()` method like a file.

		Returns
		-------
		shape : tuple of int
			The shape of the array.
		fortran_order : bool
			The array data will be written out directly if it is either
			C-contiguous or Fortran-contiguous. Otherwise, it will be made
			contiguous before writing it out.
		dtype : dtype
			The dtype of the file's data.

		Raises
		------
		ValueError
			If the data is invalid.

		"""
		return _read_array_header(fp, version=(1, 0))

	def read_array_header_2_0(fp):
		"""
		Read an array header from a filelike object using the 2.0 file format
		version.

		This will leave the file object located just after the header.

		.. versionadded:: 1.9.0

		Parameters
		----------
		fp : filelike object
			A file object or something with a `.read()` method like a file.

		Returns
		-------
		shape : tuple of int
			The shape of the array.
		fortran_order : bool
			The array data will be written out directly if it is either
			C-contiguous or Fortran-contiguous. Otherwise, it will be made
			contiguous before writing it out.
		dtype : dtype
			The dtype of the file's data.

		Raises
		------
		ValueError
			If the data is invalid.

		"""
		return _read_array_header(fp, version=(2, 0))


	def _filter_header(s):
		"""Clean up 'L' in npz header ints.

		Cleans up the 'L' in strings representing integers. Needed to allow npz
		headers produced in Python2 to be read in Python3.

		Parameters
		----------
		s : string
			Npy file header.

		Returns
		-------
		header : str
			Cleaned up header.

		"""
		import tokenize
		from io import StringIO

		tokens = []
		last_token_was_number = False
		for token in tokenize.generate_tokens(StringIO(s).readline):
			token_type = token[0]
			token_string = token[1]
			if (last_token_was_number and
					token_type == tokenize.NAME and
					token_string == "L"):
				continue
			else:
				tokens.append(token)
			last_token_was_number = (token_type == tokenize.NUMBER)
		return tokenize.untokenize(tokens)


	def _read_array_header(fp, version):
		"""
		see read_array_header_1_0
		"""
		# Read an unsigned, little-endian short int which has the length of the
		# header.
		import struct
		hinfo = _header_size_info.get(version)
		if hinfo is None:
			raise ValueError("Invalid version {!r}".format(version))
		hlength_type, encoding = hinfo

		hlength_str = _read_bytes(fp, struct.calcsize(hlength_type), "array header length")
		header_length = struct.unpack(hlength_type, hlength_str)[0]
		header = _read_bytes(fp, header_length, "array header")
		header = header.decode(encoding)

		# The header is a pretty-printed string representation of a literal
		# Python dictionary with trailing newlines padded to a ARRAY_ALIGN byte
		# boundary. The keys are strings.
		#   "shape" : tuple of int
		#   "fortran_order" : bool
		#   "descr" : dtype.descr
		# Versions (2, 0) and (1, 0) could have been created by a Python 2
		# implementation before header filtering was implemented.
		if version <= (2, 0):
			header = _filter_header(header)
		try:
			d = safe_eval(header)
		except SyntaxError as e:
			msg = "Cannot parse header: {!r}"
			raise ValueError(msg.format(header)) from e
		if not isinstance(d, dict):
			msg = "Header is not a dictionary: {!r}"
			raise ValueError(msg.format(d))

		if EXPECTED_KEYS != d.keys():
			keys = sorted(d.keys())
			msg = "Header does not contain the correct keys: {!r}"
			raise ValueError(msg.format(d.keys()))

		# Sanity-check the values.
		if (not isinstance(d['shape'], tuple) or
				not all(isinstance(x, int) for x in d['shape'])):
			msg = "shape is not valid: {!r}"
			raise ValueError(msg.format(d['shape']))
		if not isinstance(d['fortran_order'], bool):
			msg = "fortran_order is not a valid bool: {!r}"
			raise ValueError(msg.format(d['fortran_order']))
		try:
			dtype = descr_to_dtype(d['descr'])
		except TypeError as e:
			msg = "descr is not a valid dtype descriptor: {!r}"
			raise ValueError(msg.format(d['descr'])) from e

		return d['shape'], d['fortran_order'], dtype

	def write_array(fp, array, version=None, allow_pickle=True, pickle_kwargs=None):
		"""
		Write an array to an NPY file, including a header.

		If the array is neither C-contiguous nor Fortran-contiguous AND the
		file_like object is not a real file object, this function will have to
		copy data in memory.

		Parameters
		----------
		fp : file_like object
			An open, writable file object, or similar object with a
			``.write()`` method.
		array : ndarray
			The array to write to disk.
		version : (int, int) or None, optional
			The version number of the format. None means use the oldest
			supported version that is able to store the data.  Default: None
		allow_pickle : bool, optional
			Whether to allow writing pickled data. Default: True
		pickle_kwargs : dict, optional
			Additional keyword arguments to pass to pickle.dump, excluding
			'protocol'. These are only useful when pickling objects in object
			arrays on Python 3 to Python 2 compatible format.

		Raises
		------
		ValueError
			If the array cannot be persisted. This includes the case of
			allow_pickle=False and array being an object array.
		Various other errors
			If the array contains Python objects as part of its dtype, the
			process of pickling them may raise various errors if the objects
			are not picklable.

		"""
		_check_version(version)
		_write_array_header(fp, header_data_from_array_1_0(array), version)

		if array.itemsize == 0:
			buffersize = 0
		else:
			# Set buffer size to 16 MiB to hide the Python loop overhead.
			buffersize = max(16 * 1024 ** 2 // array.itemsize, 1)

		if array.dtype.hasobject:
			# We contain Python objects so we cannot write out the data
			# directly.  Instead, we will pickle it out
			if not allow_pickle:
				raise ValueError("Object arrays cannot be saved when "
								 "allow_pickle=False")
			if pickle_kwargs is None:
				pickle_kwargs = {}
			pickle.dump(array, fp, protocol=4, **pickle_kwargs)
			# print('gna')
		elif array.flags.f_contiguous and not array.flags.c_contiguous:
			if isfileobj(fp):
				array.T.tofile(fp)
			else:
				for chunk in numpy.nditer(
						array, flags=['external_loop', 'buffered', 'zerosize_ok'],
						buffersize=buffersize, order='F'):
					fp.write(chunk.tobytes('C'))
		else:
			if isfileobj(fp):
				array.tofile(fp)
			else:
				for chunk in numpy.nditer(
						array, flags=['external_loop', 'buffered', 'zerosize_ok'],
						buffersize=buffersize, order='C'):
					fp.write(chunk.tobytes('C'))



	for key, val in namedict.items():
		fname = key + '.npy'
		val = np.asanyarray(val)
		# always force zip64, gh-10776
		with zipf.open(fname, 'w', force_zip64=True) as fid:
			write_array(fid, val,
							   allow_pickle=allow_pickle,
							   pickle_kwargs=pickle_kwargs)

	zipf.close()

def find_reliable_peaks_and_throughs(signal,time,freq_signal,duty_signal,treshold_for_slow_signal=2.1, scrap_edge_peaks=True):
	from scipy.signal import find_peaks, peak_prominences as get_proms
	from scipy.ndimage import generic_filter
	from scipy.signal import savgol_filter

	if duty_signal!=0.5:
		print('this has to be fixed for duty different from 0.5')
		sadasd=asdasdasd

	# i do this to differenciate the type of filtering and how I find the peaks of things
	if 1/freq_signal>treshold_for_slow_signal:
		slow = True
	else:
		slow = False

	framerate = 1/np.median(np.diff(time))
	frames_for_one_pulse = framerate/freq_signal
	signal_filtered = generic_filter(signal,np.mean,size=[max(1,int(frames_for_one_pulse*duty_signal)-2)])
	if scrap_edge_peaks:
		temp = max(1,np.round((len(signal_filtered)-len(signal_filtered)//frames_for_one_pulse*frames_for_one_pulse)/2).astype(int))
	else:
		temp=1
	signal_filtered = signal_filtered[temp:-temp]
	peaks_loc = find_peaks(signal_filtered,distance=frames_for_one_pulse*0.95)[0]
	peaks_loc = peaks_loc[np.logical_and(peaks_loc>5,peaks_loc<len(signal_filtered)-6)]
	# if len(peaks_loc)==0:
	# 	peaks_loc = find_peaks(signal_filtered,distance=frames_for_one_pulse*0.95)[0]
	# 	if len(peaks_loc)==0:
	# 		peaks_loc = signal_filtered.argmax()
	# 	else:
	# 		peaks_loc = peaks_loc[0]
	# if len(peaks_loc)>=3:
	# 	peaks_loc = peaks_loc[np.logical_and(peaks_loc>peaks_loc.min(),peaks_loc<peaks_loc.max())]
	peaks_loc += temp

	if slow:
		signal_filtered = generic_filter(signal,np.mean,size=[max(1,int(frames_for_one_pulse*duty_signal*0.01)-2)])
		# signal_filtered = signal_filtered[temp:-temp]
		signal_filtered = np.diff(signal_filtered)
		start_loc = find_peaks(signal_filtered,distance=frames_for_one_pulse*0.95)[0]
		start_loc = start_loc[np.logical_and(start_loc>5,start_loc<len(signal_filtered)-6)]
		start_loc += 1
	else:
		signal_filtered = savgol_filter(signal,max(4,int(frames_for_one_pulse*duty_signal*0.15)),2)
		start_loc = find_peaks(-signal_filtered,distance=frames_for_one_pulse*0.95)[0]
		start_loc = start_loc[np.logical_and(start_loc>5,start_loc<len(signal_filtered)-6)]
	# if len(start_loc)==0:
	# 	start_loc = find_peaks(signal_filtered,distance=frames_for_one_pulse*0.95)[0]
	# 	if len(start_loc)==0:
	# 		start_loc = signal_filtered.argmax()
	# 	else:
	# 		start_loc = start_loc[0]
	# if len(start_loc)>=3:
	# 	start_loc = start_loc[np.logical_and(start_loc>start_loc.min(),start_loc<start_loc.max())]


	signal_filtered = generic_filter(signal,np.mean,size=[max(1,int(frames_for_one_pulse*(1-duty_signal)))])
	# totalpower_filtered_2 = totalpower_filtered_2_full_negative[int(max(1,int(frames_for_one_pulse*(1-experimental_laser_duty)))*0.5):-max(1,int(max(1,int(frames_for_one_pulse*(1-experimental_laser_duty)))*0.5))]
	# time_axis_crop = time_axis[int(max(1,int(frames_for_one_pulse*(1-experimental_laser_duty)))*0.5):-max(1,int(max(1,int(frames_for_one_pulse*(1-experimental_laser_duty)))*0.5))]
	temp = max(1,int((len(signal_filtered)-len(signal_filtered)//frames_for_one_pulse*frames_for_one_pulse)/2))
	signal_filtered = signal_filtered[temp:-temp]
	throughs_loc = find_peaks(-signal_filtered,distance=frames_for_one_pulse*0.95)[0]
	throughs_loc = throughs_loc[np.logical_and(throughs_loc>1,throughs_loc<len(signal_filtered)-1)]
	# if len(throughs_loc)==0:
	# 	throughs_loc = find_peaks(-signal_filtered,distance=frames_for_one_pulse*0.95)[0]
	# 	if len(throughs_loc)==0:
	# 		throughs_loc = signal_filtered.argmin()
	# 	else:
	# 		throughs_loc[0]
	# if len(throughs_loc)>=3:
	# 	throughs_loc = throughs_loc[np.logical_and(throughs_loc>throughs_loc.min(),throughs_loc<throughs_loc.max())]
	throughs_loc +=temp

	if slow:
		signal_filtered = generic_filter(signal,np.mean,size=[max(1,int(frames_for_one_pulse*(1-duty_signal)*0.01))])
		# signal_filtered = signal_filtered[temp:-temp]
		signal_filtered = np.diff(signal_filtered)
		end_loc = find_peaks(-signal_filtered,distance=frames_for_one_pulse*0.95)[0]
		end_loc = end_loc[np.logical_and(end_loc>1,end_loc<len(signal_filtered)-1)]
		end_loc +=1
	else:
		signal_filtered = savgol_filter(signal,max(4,int(frames_for_one_pulse*duty_signal*0.15)),2)
		end_loc = find_peaks(signal_filtered,distance=frames_for_one_pulse*0.95)[0]
		end_loc = end_loc[np.logical_and(end_loc>1,end_loc<len(signal_filtered)-1)]
	# if len(end_loc)==0:
	# 	end_loc = find_peaks(-signal_filtered,distance=frames_for_one_pulse*0.95)[0]
	# 	if len(end_loc)==0:
	# 		end_loc = signal_filtered.argmin()
	# 	else:
	# 		end_loc[0]
	# if len(end_loc)>=3:
	# 	end_loc = end_loc[np.logical_and(end_loc>end_loc.min(),end_loc<end_loc.max())]

	if scrap_edge_peaks:
		peaks_loc = peaks_loc[peaks_loc>start_loc.min()]
		throughs_loc = throughs_loc[throughs_loc<start_loc.max()]
		start_loc = start_loc[start_loc<end_loc.max()]
		end_loc = end_loc[end_loc>start_loc.min()]

	temp = []
	for value in start_loc:
		temp.append((end_loc[end_loc>value]).min())
	end_loc = np.array(temp)

	return peaks_loc,throughs_loc,frames_for_one_pulse,start_loc,end_loc



# 27/09/2023
# I add a bunch of butter filters from Yacopo

from scipy.signal import butter, filtfilt
def butter_lowpass(cutoff, fs, order):
    """
    Crea un filtro passa basso Butterworth.



    Parametri:
    -----------
    cutoff : float
        Frequenza di taglio del filtro, in Hz
    fs : float
        Frequenza di campionamento del segnale, in Hz
    order : int, default: 5
        Ordine del filtro Butterworth



    Returns:
    -----------
    b : ndarray
        Coefficienti del numeratore del filtro
    a : ndarray
        Coefficienti del denominatore del filtro
    """
    nyq = 0.5 * fs
    normal_cutoff = cutoff / nyq
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    return b, a


def butter_lowpass_filter(data, cutoff, fs, order):
    """
    Applica un filtro passa basso Butterworth al segnale.

    Parametri:
    -----------
    data : ndarray
        Segnale da filtrare
    cutoff : float
        Frequenza di taglio del filtro, in Hz
    fs : float
        Frequenza di campionamento del segnale, in Hz
    order : int, default: 5
        Ordine del filtro Butterworth



    Returns:
    -----------
    y : ndarray
        Segnale filtrato
    """
    b, a = butter_lowpass(cutoff, fs, order=order)
    y = filtfilt(b, a, data)
    return y



def butter_highpass(cutoff, fs, order=5):
    """
    Calcola i coefficienti dei polinomi per un filtro passa-alto di Butterworth.



    Parametri:
    -----------
    cutoff : float
        Frequenza di taglio del filtro, in Hz
    fs : float
        Frequenza di campionamento del segnale, in Hz
    order : int, default: 5
        Ordine del filtro Butterworth



    Returns:
    -----------
    b : ndarray
        Numeratore del filtro IIR
    a : ndarray
        Denominatore del filtro IIR
    """
    nyq = 0.5 * fs
    normal_cutoff = cutoff / nyq
    b, a = butter(order, normal_cutoff, btype='highpass', analog=False)
    return b, a



def butter_highpass_filter(data, cutoff, fs, order=1):
    """
    Applica un filtro passa alto Butterworth al segnale.



    Parametri:
    -----------
    data : ndarray
        Segnale da filtrare
    cutoff : float
        Frequenza di taglio del filtro, in Hz
    fs : float
        Frequenza di campionamento del segnale, in Hz
    order : int, default: 5
        Ordine del filtro Butterworth



    Returns:
    -----------
    y : ndarray
        Segnale filtrato
    """
    b, a = butter_highpass(cutoff, fs, order=order)
    y = filtfilt(b, a, data)
    return y



from scipy.signal import butter, filtfilt



def butter_bandpass_filter(data, lowcut, highcut, fs, order=1):
    """
    Applica un filtro passa banda Butterworth al segnale.



    Parametri:
    -----------
    data : ndarray
        Segnale da filtrare
    lowcut : float
        Frequenza di taglio inferiore del filtro, in Hz
    highcut : float
        Frequenza di taglio superiore del filtro, in Hz
    fs : float
        Frequenza di campionamento del segnale, in Hz
    order : int, default: 1
        Ordine del filtro Butterworth



    Returns:
    -----------
    y : ndarray
        Segnale filtrato
    """
    b, a = butter_bandpass(lowcut, highcut, fs, order=order)
    y = filtfilt(b, a, data)
    return y


def linear_interpolation_2D(x, y, num_points):
	# piecewise 2D interpolation
    """
    Performs linear interpolation between points defined by x and y coordinates.

    :param x: List of x-coordinates of the points.
    :param y: List of y-coordinates of the points.
    :param num_points: Number of points to interpolate between each pair of adjacent points.
    :return: Interpolated x and y coordinates as lists.
    """
    interpolated_x = []
    interpolated_y = []

    for i in range(len(x) - 1):
        x0, x1 = x[i], x[i + 1]
        y0, y1 = y[i], y[i + 1]

        for j in range(num_points):
            t = j / num_points
            interpolated_x.append(x0 + (x1 - x0) * t)
            interpolated_y.append(y0 + (y1 - y0) * t)

    # Add the last point
    interpolated_x.append(x[-1])
    interpolated_y.append(y[-1])

    return interpolated_x, interpolated_y

def return_full_fuelling_location_list(path = '/home/ffederic/work/irvb/MAST-U/'):
	from pyexcel_ods import save_data,get_data
	# path = '/home/ffederic/work/irvb/MAST-U/'

	shot_list = get_data(path+'shot_list2.ods')
	fuelling_location_list_orig = shot_list['Sheet1'][0]
	temp = []
	for value in fuelling_location_list_orig:
		if value.find('fuelling')!=-1 and value.find('_')!=-1:
			temp.append(value)
	fuelling_location_list = [string.replace('fuelling ','').lower() for string in temp[:-1]]
	return fuelling_location_list
